{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>MobileNet - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 1: Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MobileNet-Pytorch\n",
    "import argparse \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from mobilenets import mobilenet\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cudause_cud  = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Train, Validate, Test. Heavily inspired by Kevinzakka https://github.com/kevinzakka/DenseNet/blob/master/data_loader.py\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "valid_size=0.1\n",
    "\n",
    "# define transforms\n",
    "valid_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "train_dataset = datasets.CIFAR10(root=\"data\", train=True, \n",
    "            download=True, transform=train_transform)\n",
    "\n",
    "valid_dataset = datasets.CIFAR10(root=\"data\", train=True, \n",
    "            download=True, transform=valid_transform)\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train)) #5w张图片的10%用来当做验证集\n",
    "\n",
    "\n",
    "np.random.seed(42)# 42\n",
    "np.random.shuffle(indices) # 随机乱序[0,1,...,49999]\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx) # 这个很有意思\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "###################################################################################\n",
    "# ------------------------- 使用不同的批次大小 ------------------------------------\n",
    "###################################################################################\n",
    "\n",
    "show_step=2  # 批次大，show_step就小点\n",
    "max_epoch=250  # 训练最大epoch数目\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                batch_size=512, sampler=train_sampler)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                batch_size=512, sampler=valid_sampler)\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), normalize\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"data\", \n",
    "                                train=False, \n",
    "                                download=True,transform=test_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=512, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Model Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 32  缩放5次到 1x1@1024 \n",
    "# From https://github.com/kuangliu/pytorch-cifar \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        # 分组卷积数=输入通道数\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        \n",
    "        \n",
    "        #self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        one_conv_kernel_size = 3\n",
    "        self.conv1D= nn.Conv1d(1, out_planes, one_conv_kernel_size, stride=1,padding=1,groups=1,dilation=1,bias=False) # 在__init__初始化        \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # -------------------------- Attention -----------------------\n",
    "        w = F.avg_pool2d(x,x.shape[-1])  #最好在初始化层定义好\n",
    "        #print(w.shape)\n",
    "        # [bs,in_Channel,1,1]\n",
    "        w = w.view(w.shape[0],1,w.shape[1])\n",
    "        # [bs,1,in_Channel]\n",
    "        # one_conv_filter = nn.Conv1d(1, out_channel, one_conv_kernel_size, stride=1,padding=1,groups=1,dilation=1) # 在__init__初始化\n",
    "        # [bs,out_channel,in_Channel]\n",
    "        w = self.conv1D(w)\n",
    "        w = 0.5*F.tanh(w) # [-0.5,+0.5]\n",
    "        # -------------- softmax ---------------------------\n",
    "        #print(w.shape)\n",
    "        w = w.view(w.shape[0],w.shape[1],w.shape[2],1,1)\n",
    "        #print(w.shape)\n",
    "        \n",
    "        # ------------------------- fusion --------------------------\n",
    "        out=out.view(out.shape[0],1,out.shape[1],out.shape[2],out.shape[3])\n",
    "        #print(\"x size:\",out.shape)\n",
    "        \n",
    "        out=out*w\n",
    "        #print(\"after fusion x size:\",out.shape)\n",
    "        out=out.sum(dim=2)\n",
    "        \n",
    "        out = F.relu(self.bn2(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n",
    "    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32) # 自动化构建层\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for x in self.cfg:\n",
    "            out_planes = x if isinstance(x, int) else x[0]\n",
    "            stride = 1 if isinstance(x, int) else x[1]\n",
    "            layers.append(Block(in_planes, out_planes, stride))\n",
    "            in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 32  缩放5次到 1x1@1024 \n",
    "# From https://github.com/kuangliu/pytorch-cifar \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Block_Attention_HALF(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block_Attention_HALF, self).__init__()\n",
    "        \n",
    "        # 分组卷积数=输入通道数\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        \n",
    "        #------------------------ 一半 ------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_planes, int(out_planes*0.125), kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        #------------------------ 另一半 ----------------------------\n",
    "        self.scaleLayer= nn.Conv1d(1, 1, 1, stride=1,padding=0,groups=1,dilation=1,bias=True)\n",
    "        \n",
    "        one_conv_kernel_size = 9 # [3,7,9]\n",
    "        self.conv1D= nn.Conv1d(1, int(out_planes*0.875), one_conv_kernel_size, stride=1,padding=4,groups=1,dilation=1,bias=False) # 在__init__初始化        \n",
    "        \n",
    "        #------------------------------------------------------------\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu6(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # -------------------------- Attention -----------------------\n",
    "        w = F.avg_pool2d(x,x.shape[-1])  #最好在初始化层定义好\n",
    "        #print(w.shape)\n",
    "        # [bs,in_Channel,1,1]\n",
    "        in_channel=w.shape[1]\n",
    "        #w = w.view(w.shape[0],1,w.shape[1])\n",
    "        # [bs,1,in_Channel]\n",
    "        # 对这批数据取平均 且保留第0维\n",
    "        \n",
    "        #w= w.mean(dim=0,keepdim=True)\n",
    "        \n",
    "        \n",
    "#         MAX=w.shape[0]\n",
    "#         NUM=torch.floor(MAX*torch.rand(1)).long()\n",
    "#         if NUM>=0 and NUM<MAX:\n",
    "#             w=w[NUM]\n",
    "#         else:\n",
    "#             w=w[0]\n",
    "        #w=w[0]-torch.mean(w[0])\n",
    "    \n",
    "        a=torch.randn(1).cuda()*0.1\n",
    "        \n",
    "        # 策略1\n",
    "#         b=int(a*10)\n",
    "#         if b>0:\n",
    "#             w=w[b]\n",
    "#         else:\n",
    "#             w=w[0]\n",
    "        \n",
    "        #\n",
    "        w=w[0]\n",
    "        #w=torch.randn(w[0].shape).cuda()*1\n",
    "        \n",
    "        \n",
    "        if a>0.38:\n",
    "            print(w.shape)\n",
    "            print(w)\n",
    "        \n",
    "        w=w.view(1,1,in_channel)\n",
    "        \n",
    "        w=self.scaleLayer(w)  #自动缩放层\n",
    "        \n",
    "        if a>0.38:\n",
    "            print(self.scaleLayer.weight)\n",
    "        # [bs=1,1,in_Channel]\n",
    "        # one_conv_filter = nn.Conv1d(1, out_channel, one_conv_kernel_size, stride=1,padding=1,groups=1,dilation=1) # 在__init__初始化\n",
    "        # [bs=1,out_channel,in_Channel]\n",
    "        w = self.conv1D(w)\n",
    "        # [bs=1,out_channel,in_Channel]\n",
    "        \n",
    "        #-------------------------------------\n",
    "        w = 0.1*F.tanh(w) # [-0.5,+0.5]\n",
    "        #w=F.softmax(w,dim=2)\n",
    "        \n",
    "        if a>0.38:\n",
    "            print(self.conv1D.weight.shape)\n",
    "            print(self.conv1D.weight)\n",
    "            print(w.shape)\n",
    "            print(w)\n",
    "            \n",
    "        # [bs=1,out_channel//2,in_Channel]\n",
    "        w=w.view(w.shape[1],w.shape[2],1,1)\n",
    "        # [out_channel//2,in_Channel,1,1]\n",
    "        \n",
    "        # -------------- softmax ---------------------------\n",
    "        #print(w.shape)\n",
    "        \n",
    "        # ------------------------- fusion --------------------------\n",
    "       \n",
    "        # conv 1x1\n",
    "        out_1=self.conv2(out)\n",
    "        out_2=F.conv2d(out,w,bias=None,stride=1,groups=1,dilation=1)\n",
    "        out=torch.cat([out_1,out_2],1)\n",
    "        \n",
    "        # ----------------------- 试一试不要用relu -------------------------------\n",
    "        out = F.relu6(self.bn2(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Block_Attention(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block_Attention, self).__init__()\n",
    "        \n",
    "        # 分组卷积数=输入通道数\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        \n",
    "        \n",
    "        #self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        one_conv_kernel_size = 17 # [3,7,9]\n",
    "        self.conv1D= nn.Conv1d(1, out_planes, one_conv_kernel_size, stride=1,padding=8,groups=1,dilation=1,bias=False) # 在__init__初始化        \n",
    "        \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # -------------------------- Attention -----------------------\n",
    "        w = F.avg_pool2d(x,x.shape[-1])  #最好在初始化层定义好\n",
    "        #print(w.shape)\n",
    "        # [bs,in_Channel,1,1]\n",
    "        in_channel=w.shape[1]\n",
    "        #w = w.view(w.shape[0],1,w.shape[1])\n",
    "        # [bs,1,in_Channel]\n",
    "        # 对这批数据取平均 且保留第0维\n",
    "        \n",
    "        #w= w.mean(dim=0,keepdim=True)\n",
    "        \n",
    "        \n",
    "#         MAX=w.shape[0]\n",
    "#         NUM=torch.floor(MAX*torch.rand(1)).long()\n",
    "#         if NUM>=0 and NUM<MAX:\n",
    "#             w=w[NUM]\n",
    "#         else:\n",
    "#             w=w[0]\n",
    "        \n",
    "        w=w[0]\n",
    "        \n",
    "        w=w.view(1,1,in_channel)\n",
    "        # [bs=1,1,in_Channel]\n",
    "        # one_conv_filter = nn.Conv1d(1, out_channel, one_conv_kernel_size, stride=1,padding=1,groups=1,dilation=1) # 在__init__初始化\n",
    "        # [bs=1,out_channel,in_Channel]\n",
    "        w = self.conv1D(w)\n",
    "        # [bs=1,out_channel,in_Channel]\n",
    "        w = 0.5*F.tanh(w) # [-0.5,+0.5]\n",
    "         # [bs=1,out_channel,in_Channel]\n",
    "        w=w.view(w.shape[1],w.shape[2],1,1)\n",
    "        # [out_channel,in_Channel,1,1]\n",
    "        \n",
    "        # -------------- softmax ---------------------------\n",
    "        #print(w.shape)\n",
    "        \n",
    "        # ------------------------- fusion --------------------------\n",
    "       \n",
    "        # conv 1x1\n",
    "        out=F.conv2d(out,w,bias=None,stride=1,groups=1,dilation=1)\n",
    "\n",
    "        out = F.relu(self.bn2(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        # 分组卷积数=输入通道数\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n",
    "    #cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
    "    \n",
    "    #cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), [1024,1]]\n",
    "    cfg = [64, (128,2), 128, 256, 256, (512,2), 512, [512,1], [512,1],[512,1], [512,1], [1024,1], [1024,1]]\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32) # 自动化构建层\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for x in self.cfg:\n",
    "            if isinstance(x, int):\n",
    "                out_planes = x\n",
    "                stride = 1 \n",
    "                layers.append(Block(in_planes, out_planes, stride))\n",
    "            elif isinstance(x, tuple):\n",
    "                out_planes = x[0]\n",
    "                stride = x[1]\n",
    "                layers.append(Block(in_planes, out_planes, stride))\n",
    "            # AC层通过list存放设置参数\n",
    "            elif isinstance(x, list):\n",
    "                out_planes= x[0]\n",
    "                stride = x[1] if len(x)==2 else 1\n",
    "                layers.append(Block_Attention_HALF(in_planes, out_planes, stride))   \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            in_planes = out_planes\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From https://github.com/Z0m6ie/CIFAR-10_PyTorch\n",
    "#model = mobilenet(num_classes=10, large_img=False)\n",
    "\n",
    "# From https://github.com/kuangliu/pytorch-cifar \n",
    "if torch.cuda.is_available():\n",
    "    model=MobileNet(10).cuda()\n",
    "else:\n",
    "    model=MobileNet(10)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "#scheduler = StepLR(optimizer, step_size=70, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150,200,230,250], gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement validation\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    #writer = SummaryWriter()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        correct = 0\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        accuracy = 100. * (correct.cpu().numpy()/ len(output))\n",
    "        optimizer.step()\n",
    "        if batch_idx % 5*show_step == 0:\n",
    "#             if batch_idx % 2*show_step == 0:\n",
    "#                 print(model.layers[1].conv1D.weight.shape)\n",
    "#                 print(model.layers[1].conv1D.weight[0:2][0:2])\n",
    "            \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {:.2f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), accuracy))\n",
    "#             f1=open(\"Cifar10_INFO.txt\",\"a+\")\n",
    "#             f1.write(\"\\n\"+'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {:.2f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item(), accuracy))\n",
    "#             f1.close()\n",
    "            \n",
    "            #writer.add_scalar('Loss/Loss', loss.item(), epoch)\n",
    "            #writer.add_scalar('Accuracy/Accuracy', accuracy, epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    #writer = SummaryWriter()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        valid_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    valid_loss /= len(valid_idx)\n",
    "    accuracy = 100. * correct.cpu().numpy() / len(valid_idx)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        valid_loss, correct, len(valid_idx),\n",
    "        100. * correct / len(valid_idx)))\n",
    "    \n",
    "#     f1=open(\"Cifar10_INFO.txt\",\"a+\")\n",
    "#     f1.write('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "#         valid_loss, correct, len(valid_idx),\n",
    "#         100. * correct / len(valid_idx)))\n",
    "#     f1.close()\n",
    "    #writer.add_scalar('Loss/Validation_Loss', valid_loss, epoch)\n",
    "    #writer.add_scalar('Accuracy/Validation_Accuracy', accuracy, epoch)\n",
    "    return valid_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix best model\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct.cpu().numpy() / len(test_loader.dataset)))\n",
    "    \n",
    "#     f1=open(\"Cifar10_INFO.txt\",\"a+\")\n",
    "#     f1.write('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct.cpu().numpy() / len(test_loader.dataset)))\n",
    "#     f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_best(loss, accuracy, best_loss, best_acc):\n",
    "    if best_loss == None:\n",
    "        best_loss = loss\n",
    "        best_acc = accuracy\n",
    "        file = 'saved_models/best_save_model.p'\n",
    "        torch.save(model.state_dict(), file)\n",
    "        \n",
    "    elif loss < best_loss and accuracy > best_acc:\n",
    "        best_loss = loss\n",
    "        best_acc = accuracy\n",
    "        file = 'saved_models/best_save_model.p'\n",
    "        torch.save(model.state_dict(), file)\n",
    "    return best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.334667, Accuracy: 8.79\n",
      "Train Epoch: 0 [2560/50000 (6%)]\tLoss: 2.369067, Accuracy: 8.98\n",
      "Train Epoch: 0 [5120/50000 (11%)]\tLoss: 2.186809, Accuracy: 13.09\n",
      "Train Epoch: 0 [7680/50000 (17%)]\tLoss: 2.099201, Accuracy: 19.73\n",
      "Train Epoch: 0 [10240/50000 (23%)]\tLoss: 2.090390, Accuracy: 13.67\n",
      "Train Epoch: 0 [12800/50000 (28%)]\tLoss: 2.052088, Accuracy: 16.60\n",
      "Train Epoch: 0 [15360/50000 (34%)]\tLoss: 2.016495, Accuracy: 17.58\n",
      "Train Epoch: 0 [17920/50000 (40%)]\tLoss: 1.967940, Accuracy: 21.29\n",
      "Train Epoch: 0 [20480/50000 (45%)]\tLoss: 2.024180, Accuracy: 17.58\n",
      "Train Epoch: 0 [23040/50000 (51%)]\tLoss: 1.966370, Accuracy: 18.55\n",
      "Train Epoch: 0 [25600/50000 (57%)]\tLoss: 1.922051, Accuracy: 18.36\n",
      "Train Epoch: 0 [28160/50000 (62%)]\tLoss: 2.009023, Accuracy: 14.26\n",
      "Train Epoch: 0 [30720/50000 (68%)]\tLoss: 1.927094, Accuracy: 20.51\n",
      "Train Epoch: 0 [33280/50000 (74%)]\tLoss: 1.901771, Accuracy: 23.05\n",
      "Train Epoch: 0 [35840/50000 (80%)]\tLoss: 1.986556, Accuracy: 19.14\n",
      "Train Epoch: 0 [38400/50000 (85%)]\tLoss: 1.874880, Accuracy: 25.20\n",
      "Train Epoch: 0 [40960/50000 (91%)]\tLoss: 1.882394, Accuracy: 21.88\n",
      "Train Epoch: 0 [43520/50000 (97%)]\tLoss: 1.909975, Accuracy: 26.76\n",
      "\n",
      "Validation set: Average loss: 2.4333, Accuracy: 578/5000 (11.00%)\n",
      "\n",
      "the time of this epoch:[36.672646045684814 s]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.894974, Accuracy: 24.02\n",
      "Train Epoch: 1 [2560/50000 (6%)]\tLoss: 1.886398, Accuracy: 26.17\n",
      "Train Epoch: 1 [5120/50000 (11%)]\tLoss: 1.832086, Accuracy: 28.32\n",
      "Train Epoch: 1 [7680/50000 (17%)]\tLoss: 1.824412, Accuracy: 27.93\n",
      "Train Epoch: 1 [10240/50000 (23%)]\tLoss: 1.843674, Accuracy: 24.22\n",
      "Train Epoch: 1 [12800/50000 (28%)]\tLoss: 1.821602, Accuracy: 27.73\n",
      "Train Epoch: 1 [15360/50000 (34%)]\tLoss: 1.807116, Accuracy: 28.12\n",
      "Train Epoch: 1 [17920/50000 (40%)]\tLoss: 1.788781, Accuracy: 30.66\n",
      "Train Epoch: 1 [20480/50000 (45%)]\tLoss: 1.761356, Accuracy: 29.10\n",
      "Train Epoch: 1 [23040/50000 (51%)]\tLoss: 1.807827, Accuracy: 29.10\n",
      "Train Epoch: 1 [25600/50000 (57%)]\tLoss: 1.720080, Accuracy: 31.84\n",
      "Train Epoch: 1 [28160/50000 (62%)]\tLoss: 1.755004, Accuracy: 28.91\n",
      "Train Epoch: 1 [30720/50000 (68%)]\tLoss: 1.729593, Accuracy: 29.49\n",
      "Train Epoch: 1 [33280/50000 (74%)]\tLoss: 1.712140, Accuracy: 35.35\n",
      "Train Epoch: 1 [35840/50000 (80%)]\tLoss: 1.735225, Accuracy: 34.57\n",
      "Train Epoch: 1 [38400/50000 (85%)]\tLoss: 1.763711, Accuracy: 29.69\n",
      "Train Epoch: 1 [40960/50000 (91%)]\tLoss: 1.641902, Accuracy: 34.18\n",
      "Train Epoch: 1 [43520/50000 (97%)]\tLoss: 1.622468, Accuracy: 37.30\n",
      "\n",
      "Validation set: Average loss: 1.8393, Accuracy: 1444/5000 (28.00%)\n",
      "\n",
      "the time of this epoch:[36.391754388809204 s]\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.638507, Accuracy: 36.13\n",
      "Train Epoch: 2 [2560/50000 (6%)]\tLoss: 1.564013, Accuracy: 37.70\n",
      "Train Epoch: 2 [5120/50000 (11%)]\tLoss: 1.633782, Accuracy: 36.33\n",
      "Train Epoch: 2 [7680/50000 (17%)]\tLoss: 1.620165, Accuracy: 37.89\n",
      "Train Epoch: 2 [10240/50000 (23%)]\tLoss: 1.588892, Accuracy: 39.06\n",
      "Train Epoch: 2 [12800/50000 (28%)]\tLoss: 1.582873, Accuracy: 36.72\n",
      "Train Epoch: 2 [15360/50000 (34%)]\tLoss: 1.663406, Accuracy: 35.94\n",
      "Train Epoch: 2 [17920/50000 (40%)]\tLoss: 1.559187, Accuracy: 41.60\n",
      "Train Epoch: 2 [20480/50000 (45%)]\tLoss: 1.491619, Accuracy: 42.77\n",
      "Train Epoch: 2 [23040/50000 (51%)]\tLoss: 1.615648, Accuracy: 36.91\n",
      "Train Epoch: 2 [25600/50000 (57%)]\tLoss: 1.567428, Accuracy: 38.48\n",
      "Train Epoch: 2 [28160/50000 (62%)]\tLoss: 1.486005, Accuracy: 45.90\n",
      "Train Epoch: 2 [30720/50000 (68%)]\tLoss: 1.499529, Accuracy: 42.38\n",
      "Train Epoch: 2 [33280/50000 (74%)]\tLoss: 1.548751, Accuracy: 41.02\n",
      "Train Epoch: 2 [35840/50000 (80%)]\tLoss: 1.509275, Accuracy: 42.97\n",
      "Train Epoch: 2 [38400/50000 (85%)]\tLoss: 1.478327, Accuracy: 43.36\n",
      "Train Epoch: 2 [40960/50000 (91%)]\tLoss: 1.482294, Accuracy: 44.14\n",
      "Train Epoch: 2 [43520/50000 (97%)]\tLoss: 1.380470, Accuracy: 47.85\n",
      "\n",
      "Validation set: Average loss: 1.9447, Accuracy: 1804/5000 (36.00%)\n",
      "\n",
      "the time of this epoch:[36.37612962722778 s]\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.515020, Accuracy: 42.97\n",
      "Train Epoch: 3 [2560/50000 (6%)]\tLoss: 1.409552, Accuracy: 47.66\n",
      "Train Epoch: 3 [5120/50000 (11%)]\tLoss: 1.346770, Accuracy: 48.83\n",
      "Train Epoch: 3 [7680/50000 (17%)]\tLoss: 1.447350, Accuracy: 50.39\n",
      "Train Epoch: 3 [10240/50000 (23%)]\tLoss: 1.368247, Accuracy: 49.41\n",
      "Train Epoch: 3 [12800/50000 (28%)]\tLoss: 1.385709, Accuracy: 49.80\n",
      "Train Epoch: 3 [15360/50000 (34%)]\tLoss: 1.378626, Accuracy: 48.05\n",
      "Train Epoch: 3 [17920/50000 (40%)]\tLoss: 1.374161, Accuracy: 47.46\n",
      "Train Epoch: 3 [20480/50000 (45%)]\tLoss: 1.263729, Accuracy: 54.30\n",
      "Train Epoch: 3 [23040/50000 (51%)]\tLoss: 1.338898, Accuracy: 51.17\n",
      "Train Epoch: 3 [25600/50000 (57%)]\tLoss: 1.234361, Accuracy: 55.47\n",
      "Train Epoch: 3 [28160/50000 (62%)]\tLoss: 1.339382, Accuracy: 51.37\n",
      "Train Epoch: 3 [30720/50000 (68%)]\tLoss: 1.371702, Accuracy: 48.44\n",
      "Train Epoch: 3 [33280/50000 (74%)]\tLoss: 1.321473, Accuracy: 52.15\n",
      "Train Epoch: 3 [35840/50000 (80%)]\tLoss: 1.303014, Accuracy: 52.73\n",
      "Train Epoch: 3 [38400/50000 (85%)]\tLoss: 1.319045, Accuracy: 53.32\n",
      "Train Epoch: 3 [40960/50000 (91%)]\tLoss: 1.239750, Accuracy: 55.08\n",
      "Train Epoch: 3 [43520/50000 (97%)]\tLoss: 1.321301, Accuracy: 54.88\n",
      "\n",
      "Validation set: Average loss: 2.1408, Accuracy: 1762/5000 (35.00%)\n",
      "\n",
      "the time of this epoch:[36.43634605407715 s]\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.207844, Accuracy: 55.08\n",
      "Train Epoch: 4 [2560/50000 (6%)]\tLoss: 1.201050, Accuracy: 55.47\n",
      "Train Epoch: 4 [5120/50000 (11%)]\tLoss: 1.159577, Accuracy: 58.79\n",
      "Train Epoch: 4 [7680/50000 (17%)]\tLoss: 1.181242, Accuracy: 60.35\n",
      "Train Epoch: 4 [10240/50000 (23%)]\tLoss: 1.312608, Accuracy: 54.30\n",
      "Train Epoch: 4 [12800/50000 (28%)]\tLoss: 1.236810, Accuracy: 56.05\n",
      "Train Epoch: 4 [15360/50000 (34%)]\tLoss: 1.252145, Accuracy: 55.27\n",
      "Train Epoch: 4 [17920/50000 (40%)]\tLoss: 1.221336, Accuracy: 59.57\n",
      "Train Epoch: 4 [20480/50000 (45%)]\tLoss: 1.113188, Accuracy: 60.74\n",
      "Train Epoch: 4 [23040/50000 (51%)]\tLoss: 1.147898, Accuracy: 60.16\n",
      "Train Epoch: 4 [25600/50000 (57%)]\tLoss: 1.125964, Accuracy: 58.20\n",
      "Train Epoch: 4 [28160/50000 (62%)]\tLoss: 1.175925, Accuracy: 55.86\n",
      "Train Epoch: 4 [30720/50000 (68%)]\tLoss: 1.075390, Accuracy: 60.16\n",
      "Train Epoch: 4 [33280/50000 (74%)]\tLoss: 1.125003, Accuracy: 61.52\n",
      "Train Epoch: 4 [35840/50000 (80%)]\tLoss: 1.146560, Accuracy: 57.23\n",
      "Train Epoch: 4 [38400/50000 (85%)]\tLoss: 1.059957, Accuracy: 60.94\n",
      "Train Epoch: 4 [40960/50000 (91%)]\tLoss: 1.084921, Accuracy: 60.16\n",
      "Train Epoch: 4 [43520/50000 (97%)]\tLoss: 1.097754, Accuracy: 60.35\n",
      "\n",
      "Validation set: Average loss: 1.2706, Accuracy: 2771/5000 (55.00%)\n",
      "\n",
      "the time of this epoch:[36.40307331085205 s]\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.015484, Accuracy: 66.80\n",
      "Train Epoch: 5 [2560/50000 (6%)]\tLoss: 1.000960, Accuracy: 62.11\n",
      "Train Epoch: 5 [5120/50000 (11%)]\tLoss: 1.034297, Accuracy: 59.38\n",
      "Train Epoch: 5 [7680/50000 (17%)]\tLoss: 0.982504, Accuracy: 64.84\n",
      "Train Epoch: 5 [10240/50000 (23%)]\tLoss: 1.085633, Accuracy: 58.98\n",
      "Train Epoch: 5 [12800/50000 (28%)]\tLoss: 1.077270, Accuracy: 61.91\n",
      "Train Epoch: 5 [15360/50000 (34%)]\tLoss: 1.086951, Accuracy: 60.94\n",
      "Train Epoch: 5 [17920/50000 (40%)]\tLoss: 1.012473, Accuracy: 65.23\n",
      "Train Epoch: 5 [20480/50000 (45%)]\tLoss: 1.022327, Accuracy: 65.43\n",
      "Train Epoch: 5 [23040/50000 (51%)]\tLoss: 1.043508, Accuracy: 65.43\n",
      "Train Epoch: 5 [25600/50000 (57%)]\tLoss: 1.075062, Accuracy: 60.94\n",
      "Train Epoch: 5 [28160/50000 (62%)]\tLoss: 1.019541, Accuracy: 64.26\n",
      "Train Epoch: 5 [30720/50000 (68%)]\tLoss: 0.954960, Accuracy: 65.43\n",
      "Train Epoch: 5 [33280/50000 (74%)]\tLoss: 0.980300, Accuracy: 64.06\n",
      "Train Epoch: 5 [35840/50000 (80%)]\tLoss: 0.894076, Accuracy: 69.53\n",
      "Train Epoch: 5 [38400/50000 (85%)]\tLoss: 0.982611, Accuracy: 66.02\n",
      "Train Epoch: 5 [40960/50000 (91%)]\tLoss: 0.989413, Accuracy: 65.23\n",
      "Train Epoch: 5 [43520/50000 (97%)]\tLoss: 0.952485, Accuracy: 65.62\n",
      "\n",
      "Validation set: Average loss: 1.1644, Accuracy: 3039/5000 (60.00%)\n",
      "\n",
      "the time of this epoch:[36.34291481971741 s]\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.020376, Accuracy: 64.84\n",
      "Train Epoch: 6 [2560/50000 (6%)]\tLoss: 0.904296, Accuracy: 66.21\n",
      "Train Epoch: 6 [5120/50000 (11%)]\tLoss: 0.883520, Accuracy: 68.16\n",
      "Train Epoch: 6 [7680/50000 (17%)]\tLoss: 0.964353, Accuracy: 65.23\n",
      "Train Epoch: 6 [10240/50000 (23%)]\tLoss: 0.930933, Accuracy: 67.97\n",
      "Train Epoch: 6 [12800/50000 (28%)]\tLoss: 0.900838, Accuracy: 69.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [15360/50000 (34%)]\tLoss: 1.024864, Accuracy: 64.65\n",
      "Train Epoch: 6 [17920/50000 (40%)]\tLoss: 0.995186, Accuracy: 67.19\n",
      "Train Epoch: 6 [20480/50000 (45%)]\tLoss: 0.909036, Accuracy: 67.19\n",
      "Train Epoch: 6 [23040/50000 (51%)]\tLoss: 0.903440, Accuracy: 67.77\n",
      "Train Epoch: 6 [25600/50000 (57%)]\tLoss: 0.907810, Accuracy: 68.55\n",
      "Train Epoch: 6 [28160/50000 (62%)]\tLoss: 0.830573, Accuracy: 71.88\n",
      "Train Epoch: 6 [30720/50000 (68%)]\tLoss: 0.818853, Accuracy: 69.34\n",
      "Train Epoch: 6 [33280/50000 (74%)]\tLoss: 0.935746, Accuracy: 67.38\n",
      "Train Epoch: 6 [35840/50000 (80%)]\tLoss: 0.876056, Accuracy: 70.51\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0832]],\n",
      "\n",
      "        [[ 0.1443]],\n",
      "\n",
      "        [[ 0.2583]],\n",
      "\n",
      "        [[ 0.1040]],\n",
      "\n",
      "        [[ 0.1928]],\n",
      "\n",
      "        [[ 0.1656]],\n",
      "\n",
      "        [[ 0.2219]],\n",
      "\n",
      "        [[ 0.1063]],\n",
      "\n",
      "        [[ 0.6474]],\n",
      "\n",
      "        [[ 0.0325]],\n",
      "\n",
      "        [[ 0.2633]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.2672]],\n",
      "\n",
      "        [[ 0.1702]],\n",
      "\n",
      "        [[ 0.0652]],\n",
      "\n",
      "        [[ 0.0935]],\n",
      "\n",
      "        [[ 0.2436]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0336]],\n",
      "\n",
      "        [[ 0.1061]],\n",
      "\n",
      "        [[ 0.1517]],\n",
      "\n",
      "        [[ 0.1573]],\n",
      "\n",
      "        [[ 0.0204]],\n",
      "\n",
      "        [[ 0.0678]],\n",
      "\n",
      "        [[ 0.0247]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.1058]],\n",
      "\n",
      "        [[ 0.1260]],\n",
      "\n",
      "        [[ 0.2110]],\n",
      "\n",
      "        [[ 0.0931]],\n",
      "\n",
      "        [[ 0.2995]],\n",
      "\n",
      "        [[ 0.1572]],\n",
      "\n",
      "        [[ 0.1957]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0735]],\n",
      "\n",
      "        [[ 0.2371]],\n",
      "\n",
      "        [[ 0.1787]],\n",
      "\n",
      "        [[ 0.2247]],\n",
      "\n",
      "        [[ 0.1740]],\n",
      "\n",
      "        [[ 0.0957]],\n",
      "\n",
      "        [[ 0.3452]],\n",
      "\n",
      "        [[ 0.0106]],\n",
      "\n",
      "        [[ 0.1591]],\n",
      "\n",
      "        [[ 0.0377]],\n",
      "\n",
      "        [[ 0.2339]],\n",
      "\n",
      "        [[ 0.0947]],\n",
      "\n",
      "        [[ 0.1710]],\n",
      "\n",
      "        [[ 0.1703]],\n",
      "\n",
      "        [[ 0.0888]],\n",
      "\n",
      "        [[ 0.2973]],\n",
      "\n",
      "        [[ 0.2585]],\n",
      "\n",
      "        [[ 0.0526]],\n",
      "\n",
      "        [[ 0.1631]],\n",
      "\n",
      "        [[ 0.3723]],\n",
      "\n",
      "        [[ 0.0364]],\n",
      "\n",
      "        [[ 0.1435]],\n",
      "\n",
      "        [[ 0.0255]],\n",
      "\n",
      "        [[ 0.1134]],\n",
      "\n",
      "        [[ 0.0914]],\n",
      "\n",
      "        [[ 0.1035]],\n",
      "\n",
      "        [[ 0.0617]],\n",
      "\n",
      "        [[ 0.0382]],\n",
      "\n",
      "        [[ 0.2649]],\n",
      "\n",
      "        [[ 0.1254]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0898]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.2554]],\n",
      "\n",
      "        [[ 0.0285]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0497]],\n",
      "\n",
      "        [[ 0.1093]],\n",
      "\n",
      "        [[ 0.1130]],\n",
      "\n",
      "        [[ 0.1887]],\n",
      "\n",
      "        [[ 0.1299]],\n",
      "\n",
      "        [[ 0.1235]],\n",
      "\n",
      "        [[ 0.0153]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.2939]],\n",
      "\n",
      "        [[ 0.1278]],\n",
      "\n",
      "        [[ 0.0588]],\n",
      "\n",
      "        [[ 0.2957]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0997]],\n",
      "\n",
      "        [[ 0.0633]],\n",
      "\n",
      "        [[ 0.1199]],\n",
      "\n",
      "        [[ 0.2128]],\n",
      "\n",
      "        [[ 0.2206]],\n",
      "\n",
      "        [[ 0.1488]],\n",
      "\n",
      "        [[ 0.1308]],\n",
      "\n",
      "        [[ 0.2092]],\n",
      "\n",
      "        [[ 0.1036]],\n",
      "\n",
      "        [[ 0.0584]],\n",
      "\n",
      "        [[ 0.0387]],\n",
      "\n",
      "        [[ 0.1755]],\n",
      "\n",
      "        [[ 0.0859]],\n",
      "\n",
      "        [[ 0.1301]],\n",
      "\n",
      "        [[ 0.1282]],\n",
      "\n",
      "        [[ 0.1585]],\n",
      "\n",
      "        [[ 0.0796]],\n",
      "\n",
      "        [[ 0.1034]],\n",
      "\n",
      "        [[ 0.1137]],\n",
      "\n",
      "        [[ 0.0671]],\n",
      "\n",
      "        [[ 0.0374]],\n",
      "\n",
      "        [[ 0.2784]],\n",
      "\n",
      "        [[ 0.0889]],\n",
      "\n",
      "        [[ 0.2514]],\n",
      "\n",
      "        [[ 0.0744]],\n",
      "\n",
      "        [[ 0.1156]],\n",
      "\n",
      "        [[ 0.0298]],\n",
      "\n",
      "        [[ 0.0931]],\n",
      "\n",
      "        [[ 0.1008]],\n",
      "\n",
      "        [[ 0.0855]],\n",
      "\n",
      "        [[ 0.1648]],\n",
      "\n",
      "        [[ 0.0742]],\n",
      "\n",
      "        [[ 0.0119]],\n",
      "\n",
      "        [[ 0.0434]],\n",
      "\n",
      "        [[ 0.0529]],\n",
      "\n",
      "        [[ 0.1759]],\n",
      "\n",
      "        [[ 0.2580]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.2725]],\n",
      "\n",
      "        [[ 0.0461]],\n",
      "\n",
      "        [[ 0.2045]],\n",
      "\n",
      "        [[ 0.0596]],\n",
      "\n",
      "        [[ 0.1633]],\n",
      "\n",
      "        [[ 0.1767]],\n",
      "\n",
      "        [[ 0.0933]],\n",
      "\n",
      "        [[ 0.0554]],\n",
      "\n",
      "        [[ 0.1000]],\n",
      "\n",
      "        [[ 0.1520]],\n",
      "\n",
      "        [[ 0.2962]],\n",
      "\n",
      "        [[ 0.0195]],\n",
      "\n",
      "        [[ 0.2397]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.2030]],\n",
      "\n",
      "        [[ 0.1237]],\n",
      "\n",
      "        [[ 0.1504]],\n",
      "\n",
      "        [[ 0.1516]],\n",
      "\n",
      "        [[ 0.1412]],\n",
      "\n",
      "        [[ 0.1921]],\n",
      "\n",
      "        [[ 0.1180]],\n",
      "\n",
      "        [[ 0.0382]],\n",
      "\n",
      "        [[ 0.0456]],\n",
      "\n",
      "        [[ 0.1145]],\n",
      "\n",
      "        [[ 0.2027]],\n",
      "\n",
      "        [[ 0.2862]],\n",
      "\n",
      "        [[ 0.1391]],\n",
      "\n",
      "        [[ 0.2165]],\n",
      "\n",
      "        [[ 0.1229]],\n",
      "\n",
      "        [[ 0.1429]],\n",
      "\n",
      "        [[ 0.1385]],\n",
      "\n",
      "        [[ 0.0162]],\n",
      "\n",
      "        [[ 0.1302]],\n",
      "\n",
      "        [[ 0.1100]],\n",
      "\n",
      "        [[ 0.0338]],\n",
      "\n",
      "        [[ 0.2082]],\n",
      "\n",
      "        [[ 0.0498]],\n",
      "\n",
      "        [[ 0.1531]],\n",
      "\n",
      "        [[ 0.2745]],\n",
      "\n",
      "        [[ 0.0492]],\n",
      "\n",
      "        [[ 0.0297]],\n",
      "\n",
      "        [[ 0.1179]],\n",
      "\n",
      "        [[ 0.2379]],\n",
      "\n",
      "        [[ 0.0191]],\n",
      "\n",
      "        [[ 0.0545]],\n",
      "\n",
      "        [[ 0.1409]],\n",
      "\n",
      "        [[ 0.0588]],\n",
      "\n",
      "        [[ 0.1170]],\n",
      "\n",
      "        [[ 0.2469]],\n",
      "\n",
      "        [[ 0.1090]],\n",
      "\n",
      "        [[ 0.2130]],\n",
      "\n",
      "        [[ 0.0718]],\n",
      "\n",
      "        [[ 0.2145]],\n",
      "\n",
      "        [[ 0.0628]],\n",
      "\n",
      "        [[ 0.1364]],\n",
      "\n",
      "        [[ 0.0854]],\n",
      "\n",
      "        [[ 0.2110]],\n",
      "\n",
      "        [[ 0.0284]],\n",
      "\n",
      "        [[ 0.1713]],\n",
      "\n",
      "        [[ 0.0537]],\n",
      "\n",
      "        [[ 0.1069]],\n",
      "\n",
      "        [[ 0.0300]],\n",
      "\n",
      "        [[ 0.1577]],\n",
      "\n",
      "        [[ 0.1182]],\n",
      "\n",
      "        [[ 0.0910]],\n",
      "\n",
      "        [[ 0.0324]],\n",
      "\n",
      "        [[ 0.1622]],\n",
      "\n",
      "        [[ 0.1318]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0813]],\n",
      "\n",
      "        [[ 0.1721]],\n",
      "\n",
      "        [[ 0.0095]],\n",
      "\n",
      "        [[ 0.1462]],\n",
      "\n",
      "        [[ 0.2253]],\n",
      "\n",
      "        [[ 0.1963]],\n",
      "\n",
      "        [[ 0.2747]],\n",
      "\n",
      "        [[ 0.0399]],\n",
      "\n",
      "        [[ 0.1798]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.2936]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.2858]],\n",
      "\n",
      "        [[ 0.2004]],\n",
      "\n",
      "        [[ 0.1026]],\n",
      "\n",
      "        [[ 0.0415]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0953]],\n",
      "\n",
      "        [[ 0.0506]],\n",
      "\n",
      "        [[ 0.1230]],\n",
      "\n",
      "        [[ 0.1718]],\n",
      "\n",
      "        [[ 0.0826]],\n",
      "\n",
      "        [[ 0.2980]],\n",
      "\n",
      "        [[ 0.0322]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0982]],\n",
      "\n",
      "        [[ 0.0720]],\n",
      "\n",
      "        [[ 0.0726]],\n",
      "\n",
      "        [[ 0.0805]],\n",
      "\n",
      "        [[ 0.1315]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.1329]],\n",
      "\n",
      "        [[ 0.2035]],\n",
      "\n",
      "        [[ 0.0598]],\n",
      "\n",
      "        [[ 0.0639]],\n",
      "\n",
      "        [[ 0.1855]],\n",
      "\n",
      "        [[ 0.2954]],\n",
      "\n",
      "        [[ 0.1388]],\n",
      "\n",
      "        [[ 0.0882]],\n",
      "\n",
      "        [[ 0.1951]],\n",
      "\n",
      "        [[ 0.1796]],\n",
      "\n",
      "        [[ 0.0447]],\n",
      "\n",
      "        [[ 0.1716]],\n",
      "\n",
      "        [[ 0.1124]],\n",
      "\n",
      "        [[ 0.2731]],\n",
      "\n",
      "        [[ 0.1755]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2582]],\n",
      "\n",
      "        [[ 0.1319]],\n",
      "\n",
      "        [[ 0.0299]],\n",
      "\n",
      "        [[ 0.2026]],\n",
      "\n",
      "        [[ 0.0517]],\n",
      "\n",
      "        [[ 0.1750]],\n",
      "\n",
      "        [[ 0.0894]],\n",
      "\n",
      "        [[ 0.1950]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.2844]],\n",
      "\n",
      "        [[ 0.2968]],\n",
      "\n",
      "        [[ 0.1242]],\n",
      "\n",
      "        [[ 0.1774]],\n",
      "\n",
      "        [[ 0.2861]],\n",
      "\n",
      "        [[ 0.2430]],\n",
      "\n",
      "        [[ 0.1945]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0601]],\n",
      "\n",
      "        [[ 0.1290]],\n",
      "\n",
      "        [[ 0.1534]],\n",
      "\n",
      "        [[ 0.0722]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0103]],\n",
      "\n",
      "        [[ 0.2517]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1130]],\n",
      "\n",
      "        [[ 0.1357]],\n",
      "\n",
      "        [[ 0.0632]],\n",
      "\n",
      "        [[ 0.2101]],\n",
      "\n",
      "        [[ 0.0221]],\n",
      "\n",
      "        [[ 0.1442]],\n",
      "\n",
      "        [[ 0.0585]],\n",
      "\n",
      "        [[ 0.0589]],\n",
      "\n",
      "        [[ 0.1643]],\n",
      "\n",
      "        [[ 0.0848]],\n",
      "\n",
      "        [[ 0.1460]],\n",
      "\n",
      "        [[ 0.0425]],\n",
      "\n",
      "        [[ 0.2498]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.1816]],\n",
      "\n",
      "        [[ 0.1919]],\n",
      "\n",
      "        [[ 0.1753]],\n",
      "\n",
      "        [[ 0.1257]],\n",
      "\n",
      "        [[ 0.0245]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1799]],\n",
      "\n",
      "        [[ 0.1433]],\n",
      "\n",
      "        [[ 0.1526]],\n",
      "\n",
      "        [[ 0.1770]],\n",
      "\n",
      "        [[ 0.2457]],\n",
      "\n",
      "        [[ 0.1173]],\n",
      "\n",
      "        [[ 0.1860]],\n",
      "\n",
      "        [[ 0.1525]],\n",
      "\n",
      "        [[ 0.0726]],\n",
      "\n",
      "        [[ 0.1040]],\n",
      "\n",
      "        [[ 0.2901]],\n",
      "\n",
      "        [[ 0.2042]],\n",
      "\n",
      "        [[ 0.1783]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.1798]],\n",
      "\n",
      "        [[ 0.0875]],\n",
      "\n",
      "        [[ 0.2106]],\n",
      "\n",
      "        [[ 0.1538]],\n",
      "\n",
      "        [[ 0.0298]],\n",
      "\n",
      "        [[ 0.1271]],\n",
      "\n",
      "        [[ 0.2050]],\n",
      "\n",
      "        [[ 0.2421]],\n",
      "\n",
      "        [[ 0.0858]],\n",
      "\n",
      "        [[ 0.1269]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1933]],\n",
      "\n",
      "        [[ 0.2780]],\n",
      "\n",
      "        [[ 0.1925]],\n",
      "\n",
      "        [[ 0.1408]],\n",
      "\n",
      "        [[ 0.1844]],\n",
      "\n",
      "        [[ 0.0900]],\n",
      "\n",
      "        [[ 0.0928]],\n",
      "\n",
      "        [[ 0.1330]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0841]],\n",
      "\n",
      "        [[ 0.1517]],\n",
      "\n",
      "        [[ 0.0436]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.1305]],\n",
      "\n",
      "        [[ 0.1848]],\n",
      "\n",
      "        [[ 0.1844]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.1159]],\n",
      "\n",
      "        [[ 0.0689]],\n",
      "\n",
      "        [[ 0.0860]],\n",
      "\n",
      "        [[ 0.0252]],\n",
      "\n",
      "        [[ 0.0974]],\n",
      "\n",
      "        [[ 0.2187]],\n",
      "\n",
      "        [[ 0.0423]],\n",
      "\n",
      "        [[ 0.1187]],\n",
      "\n",
      "        [[ 0.1416]],\n",
      "\n",
      "        [[ 0.0234]],\n",
      "\n",
      "        [[ 0.0824]],\n",
      "\n",
      "        [[ 0.1022]],\n",
      "\n",
      "        [[ 0.1121]],\n",
      "\n",
      "        [[ 0.1043]],\n",
      "\n",
      "        [[ 0.1404]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.1243]],\n",
      "\n",
      "        [[ 0.1517]],\n",
      "\n",
      "        [[ 0.1310]],\n",
      "\n",
      "        [[ 0.1110]],\n",
      "\n",
      "        [[ 0.2638]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1607]],\n",
      "\n",
      "        [[ 0.0928]],\n",
      "\n",
      "        [[ 0.1890]],\n",
      "\n",
      "        [[ 0.1636]],\n",
      "\n",
      "        [[ 0.1240]],\n",
      "\n",
      "        [[ 0.0833]],\n",
      "\n",
      "        [[ 0.0823]],\n",
      "\n",
      "        [[ 0.0862]],\n",
      "\n",
      "        [[ 0.1038]],\n",
      "\n",
      "        [[ 0.0420]],\n",
      "\n",
      "        [[ 0.0328]],\n",
      "\n",
      "        [[ 0.0825]],\n",
      "\n",
      "        [[ 0.1567]],\n",
      "\n",
      "        [[ 0.2756]],\n",
      "\n",
      "        [[ 0.2917]],\n",
      "\n",
      "        [[ 0.1501]],\n",
      "\n",
      "        [[ 0.1731]],\n",
      "\n",
      "        [[ 0.1331]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0179]],\n",
      "\n",
      "        [[ 0.1628]],\n",
      "\n",
      "        [[ 0.2018]],\n",
      "\n",
      "        [[ 0.1428]],\n",
      "\n",
      "        [[ 0.1504]],\n",
      "\n",
      "        [[ 0.1402]],\n",
      "\n",
      "        [[ 0.0960]],\n",
      "\n",
      "        [[ 0.0604]],\n",
      "\n",
      "        [[ 0.1603]],\n",
      "\n",
      "        [[ 0.0718]],\n",
      "\n",
      "        [[ 0.0755]],\n",
      "\n",
      "        [[ 0.1155]],\n",
      "\n",
      "        [[ 0.1899]],\n",
      "\n",
      "        [[ 0.2787]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.2805]],\n",
      "\n",
      "        [[ 0.2818]],\n",
      "\n",
      "        [[ 0.0227]],\n",
      "\n",
      "        [[ 0.1213]],\n",
      "\n",
      "        [[ 0.2530]],\n",
      "\n",
      "        [[ 0.1321]],\n",
      "\n",
      "        [[ 0.1603]],\n",
      "\n",
      "        [[ 0.2358]],\n",
      "\n",
      "        [[ 0.1198]],\n",
      "\n",
      "        [[ 0.1816]],\n",
      "\n",
      "        [[ 0.2694]],\n",
      "\n",
      "        [[ 0.2873]],\n",
      "\n",
      "        [[ 0.0862]],\n",
      "\n",
      "        [[ 0.0495]],\n",
      "\n",
      "        [[ 0.0346]],\n",
      "\n",
      "        [[ 0.1192]],\n",
      "\n",
      "        [[ 0.0393]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2849]],\n",
      "\n",
      "        [[ 0.0902]],\n",
      "\n",
      "        [[ 0.1955]],\n",
      "\n",
      "        [[ 0.0874]],\n",
      "\n",
      "        [[ 0.0913]],\n",
      "\n",
      "        [[ 0.1755]],\n",
      "\n",
      "        [[ 0.1950]],\n",
      "\n",
      "        [[ 0.1609]],\n",
      "\n",
      "        [[ 0.1331]],\n",
      "\n",
      "        [[ 0.0967]],\n",
      "\n",
      "        [[ 0.1628]],\n",
      "\n",
      "        [[ 0.1864]],\n",
      "\n",
      "        [[ 0.0876]],\n",
      "\n",
      "        [[ 0.0417]],\n",
      "\n",
      "        [[ 0.1178]],\n",
      "\n",
      "        [[ 0.1236]],\n",
      "\n",
      "        [[ 0.1766]],\n",
      "\n",
      "        [[ 0.1868]],\n",
      "\n",
      "        [[ 0.2175]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.1781]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0816]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1941]],\n",
      "\n",
      "        [[ 0.2596]],\n",
      "\n",
      "        [[ 0.1697]],\n",
      "\n",
      "        [[ 0.0654]],\n",
      "\n",
      "        [[ 0.2001]],\n",
      "\n",
      "        [[ 0.2094]],\n",
      "\n",
      "        [[ 0.1061]],\n",
      "\n",
      "        [[ 0.1691]],\n",
      "\n",
      "        [[ 0.1379]],\n",
      "\n",
      "        [[ 0.0833]],\n",
      "\n",
      "        [[ 0.1193]],\n",
      "\n",
      "        [[ 0.2612]],\n",
      "\n",
      "        [[ 0.0710]],\n",
      "\n",
      "        [[ 0.2078]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0366]],\n",
      "\n",
      "        [[ 0.2081]],\n",
      "\n",
      "        [[ 0.0904]],\n",
      "\n",
      "        [[ 0.1044]],\n",
      "\n",
      "        [[ 0.0317]],\n",
      "\n",
      "        [[ 0.0055]],\n",
      "\n",
      "        [[ 0.0504]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.0437]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1438]],\n",
      "\n",
      "        [[ 0.0828]],\n",
      "\n",
      "        [[ 0.0350]],\n",
      "\n",
      "        [[ 0.2456]],\n",
      "\n",
      "        [[ 0.0865]],\n",
      "\n",
      "        [[ 0.2902]],\n",
      "\n",
      "        [[ 0.0624]],\n",
      "\n",
      "        [[ 0.2092]],\n",
      "\n",
      "        [[ 0.0932]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0975]],\n",
      "\n",
      "        [[ 0.1943]],\n",
      "\n",
      "        [[ 0.1912]],\n",
      "\n",
      "        [[ 0.0763]],\n",
      "\n",
      "        [[ 0.1993]],\n",
      "\n",
      "        [[ 0.0823]],\n",
      "\n",
      "        [[ 0.2918]],\n",
      "\n",
      "        [[ 0.1140]],\n",
      "\n",
      "        [[ 0.1336]],\n",
      "\n",
      "        [[ 0.1865]],\n",
      "\n",
      "        [[ 0.2287]],\n",
      "\n",
      "        [[ 0.2115]],\n",
      "\n",
      "        [[ 0.2887]],\n",
      "\n",
      "        [[ 0.1458]],\n",
      "\n",
      "        [[ 0.0285]],\n",
      "\n",
      "        [[ 0.0052]],\n",
      "\n",
      "        [[ 0.1808]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2067]],\n",
      "\n",
      "        [[ 0.1184]],\n",
      "\n",
      "        [[ 0.0168]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0831]],\n",
      "\n",
      "        [[ 0.0760]],\n",
      "\n",
      "        [[ 0.1991]],\n",
      "\n",
      "        [[ 0.1824]],\n",
      "\n",
      "        [[ 0.2976]],\n",
      "\n",
      "        [[ 0.1664]],\n",
      "\n",
      "        [[ 0.1710]],\n",
      "\n",
      "        [[ 0.0811]],\n",
      "\n",
      "        [[ 0.0408]],\n",
      "\n",
      "        [[ 0.1197]],\n",
      "\n",
      "        [[ 0.2707]],\n",
      "\n",
      "        [[ 0.0633]],\n",
      "\n",
      "        [[ 0.1479]],\n",
      "\n",
      "        [[ 0.0691]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1638]],\n",
      "\n",
      "        [[ 0.1848]],\n",
      "\n",
      "        [[ 0.0124]],\n",
      "\n",
      "        [[ 0.1575]],\n",
      "\n",
      "        [[ 0.2237]],\n",
      "\n",
      "        [[ 0.2576]],\n",
      "\n",
      "        [[ 0.2114]],\n",
      "\n",
      "        [[ 0.2272]],\n",
      "\n",
      "        [[ 0.1376]],\n",
      "\n",
      "        [[ 0.3029]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.1428]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[[-0.2047]]], device='cuda:0')\n",
      "torch.Size([896, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1413, -0.0128,  0.1818,  ..., -0.2251, -0.2507,  0.0533]],\n",
      "\n",
      "        [[ 0.2380, -0.1916,  0.0650,  ..., -0.1052, -0.1967,  0.2300]],\n",
      "\n",
      "        [[ 0.0268, -0.1396, -0.1535,  ..., -0.0762,  0.0567,  0.2275]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0688, -0.0872, -0.2092,  ...,  0.1715, -0.0244,  0.0408]],\n",
      "\n",
      "        [[ 0.1351, -0.0948, -0.0489,  ...,  0.0781,  0.0496,  0.0757]],\n",
      "\n",
      "        [[ 0.1600, -0.2280,  0.0068,  ...,  0.1403,  0.1958, -0.0735]]], device='cuda:0')\n",
      "torch.Size([1, 896, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 7.6202e-03,  5.0777e-03,  1.7323e-03,  ..., -5.7814e-03,\n",
      "          -9.9371e-03, -9.7982e-03],\n",
      "         [ 3.6724e-03,  5.2813e-03,  4.2177e-03,  ...,  3.7322e-03,\n",
      "           1.6959e-03,  6.6506e-04],\n",
      "         [ 3.0979e-03,  4.8890e-04,  3.0533e-03,  ...,  1.1714e-02,\n",
      "           8.5930e-03,  7.2040e-03],\n",
      "         ...,\n",
      "         [ 1.6130e-03,  6.3276e-03,  9.8501e-03,  ...,  1.4178e-02,\n",
      "           1.7289e-02,  1.4018e-02],\n",
      "         [-6.9946e-03, -9.9165e-03, -1.0190e-02,  ..., -8.6392e-03,\n",
      "          -6.1712e-03, -7.2871e-03],\n",
      "         [-5.5104e-03, -7.9650e-03, -8.6058e-03,  ..., -4.9420e-03,\n",
      "          -1.9730e-03, -2.7346e-03]]], device='cuda:0')\n",
      "Train Epoch: 6 [38400/50000 (85%)]\tLoss: 0.924262, Accuracy: 65.43\n",
      "Train Epoch: 6 [40960/50000 (91%)]\tLoss: 0.887030, Accuracy: 67.97\n",
      "Train Epoch: 6 [43520/50000 (97%)]\tLoss: 0.824545, Accuracy: 70.12\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.3183]],\n",
      "\n",
      "        [[ 0.1160]],\n",
      "\n",
      "        [[ 0.1529]],\n",
      "\n",
      "        [[ 0.1730]],\n",
      "\n",
      "        [[ 0.2405]],\n",
      "\n",
      "        [[ 0.1895]],\n",
      "\n",
      "        [[ 0.1990]],\n",
      "\n",
      "        [[ 0.1383]],\n",
      "\n",
      "        [[ 0.1950]],\n",
      "\n",
      "        [[ 0.0773]],\n",
      "\n",
      "        [[ 0.3282]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.3174]],\n",
      "\n",
      "        [[ 0.1045]],\n",
      "\n",
      "        [[ 0.1010]],\n",
      "\n",
      "        [[ 0.2476]],\n",
      "\n",
      "        [[ 0.2173]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1297]],\n",
      "\n",
      "        [[ 0.1707]],\n",
      "\n",
      "        [[ 0.1978]],\n",
      "\n",
      "        [[ 0.0788]],\n",
      "\n",
      "        [[ 0.0300]],\n",
      "\n",
      "        [[ 0.0553]],\n",
      "\n",
      "        [[ 0.1126]],\n",
      "\n",
      "        [[ 0.0969]],\n",
      "\n",
      "        [[ 0.1786]],\n",
      "\n",
      "        [[ 0.0805]],\n",
      "\n",
      "        [[ 0.2817]],\n",
      "\n",
      "        [[ 0.2518]],\n",
      "\n",
      "        [[ 0.3155]],\n",
      "\n",
      "        [[ 0.0916]],\n",
      "\n",
      "        [[ 0.0827]],\n",
      "\n",
      "        [[ 0.0156]],\n",
      "\n",
      "        [[ 0.0701]],\n",
      "\n",
      "        [[ 0.0696]],\n",
      "\n",
      "        [[ 0.2522]],\n",
      "\n",
      "        [[ 0.1324]],\n",
      "\n",
      "        [[ 0.1832]],\n",
      "\n",
      "        [[ 0.0972]],\n",
      "\n",
      "        [[ 0.2983]],\n",
      "\n",
      "        [[ 0.0469]],\n",
      "\n",
      "        [[ 0.2784]],\n",
      "\n",
      "        [[ 0.0218]],\n",
      "\n",
      "        [[ 0.1161]],\n",
      "\n",
      "        [[ 0.0822]],\n",
      "\n",
      "        [[ 0.0206]],\n",
      "\n",
      "        [[ 0.0923]],\n",
      "\n",
      "        [[ 0.0721]],\n",
      "\n",
      "        [[ 0.0779]],\n",
      "\n",
      "        [[ 0.1760]],\n",
      "\n",
      "        [[ 0.0445]],\n",
      "\n",
      "        [[ 0.2140]],\n",
      "\n",
      "        [[ 0.3582]],\n",
      "\n",
      "        [[ 0.1927]],\n",
      "\n",
      "        [[ 0.1125]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.2044]],\n",
      "\n",
      "        [[ 0.0169]],\n",
      "\n",
      "        [[ 0.1496]],\n",
      "\n",
      "        [[ 0.1024]],\n",
      "\n",
      "        [[ 0.0319]],\n",
      "\n",
      "        [[ 0.2654]],\n",
      "\n",
      "        [[ 0.0751]],\n",
      "\n",
      "        [[ 0.0022]],\n",
      "\n",
      "        [[ 0.0694]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.2414]],\n",
      "\n",
      "        [[ 0.0221]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0527]],\n",
      "\n",
      "        [[ 0.1049]],\n",
      "\n",
      "        [[ 0.0869]],\n",
      "\n",
      "        [[ 0.1446]],\n",
      "\n",
      "        [[ 0.1238]],\n",
      "\n",
      "        [[ 0.1275]],\n",
      "\n",
      "        [[ 0.0097]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.2820]],\n",
      "\n",
      "        [[ 0.0923]],\n",
      "\n",
      "        [[ 0.0661]],\n",
      "\n",
      "        [[ 0.2869]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0952]],\n",
      "\n",
      "        [[ 0.0505]],\n",
      "\n",
      "        [[ 0.1141]],\n",
      "\n",
      "        [[ 0.2082]],\n",
      "\n",
      "        [[ 0.1689]],\n",
      "\n",
      "        [[ 0.1130]],\n",
      "\n",
      "        [[ 0.1265]],\n",
      "\n",
      "        [[ 0.1626]],\n",
      "\n",
      "        [[ 0.0816]],\n",
      "\n",
      "        [[ 0.0466]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.1684]],\n",
      "\n",
      "        [[ 0.0878]],\n",
      "\n",
      "        [[ 0.0985]],\n",
      "\n",
      "        [[ 0.1239]],\n",
      "\n",
      "        [[ 0.1234]],\n",
      "\n",
      "        [[ 0.0608]],\n",
      "\n",
      "        [[ 0.0807]],\n",
      "\n",
      "        [[ 0.0853]],\n",
      "\n",
      "        [[ 0.0572]],\n",
      "\n",
      "        [[ 0.0295]],\n",
      "\n",
      "        [[ 0.1629]],\n",
      "\n",
      "        [[ 0.0697]],\n",
      "\n",
      "        [[ 0.2433]],\n",
      "\n",
      "        [[ 0.0579]],\n",
      "\n",
      "        [[ 0.0885]],\n",
      "\n",
      "        [[ 0.0285]],\n",
      "\n",
      "        [[ 0.0713]],\n",
      "\n",
      "        [[ 0.1006]],\n",
      "\n",
      "        [[ 0.0819]],\n",
      "\n",
      "        [[ 0.1282]],\n",
      "\n",
      "        [[ 0.0626]],\n",
      "\n",
      "        [[ 0.0109]],\n",
      "\n",
      "        [[ 0.0410]],\n",
      "\n",
      "        [[ 0.0445]],\n",
      "\n",
      "        [[ 0.1365]],\n",
      "\n",
      "        [[ 0.2488]],\n",
      "\n",
      "        [[ 0.0211]],\n",
      "\n",
      "        [[ 0.2612]],\n",
      "\n",
      "        [[ 0.0408]],\n",
      "\n",
      "        [[ 0.1959]],\n",
      "\n",
      "        [[ 0.0561]],\n",
      "\n",
      "        [[ 0.1498]],\n",
      "\n",
      "        [[ 0.1753]],\n",
      "\n",
      "        [[ 0.0912]],\n",
      "\n",
      "        [[ 0.0556]],\n",
      "\n",
      "        [[ 0.0770]],\n",
      "\n",
      "        [[ 0.1173]],\n",
      "\n",
      "        [[ 0.2833]],\n",
      "\n",
      "        [[ 0.0161]],\n",
      "\n",
      "        [[ 0.2315]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.1973]],\n",
      "\n",
      "        [[ 0.1195]],\n",
      "\n",
      "        [[ 0.1185]],\n",
      "\n",
      "        [[ 0.1161]],\n",
      "\n",
      "        [[ 0.1365]],\n",
      "\n",
      "        [[ 0.1485]],\n",
      "\n",
      "        [[ 0.0910]],\n",
      "\n",
      "        [[ 0.0298]],\n",
      "\n",
      "        [[ 0.0543]],\n",
      "\n",
      "        [[ 0.1140]],\n",
      "\n",
      "        [[ 0.1910]],\n",
      "\n",
      "        [[ 0.2753]],\n",
      "\n",
      "        [[ 0.1077]],\n",
      "\n",
      "        [[ 0.2068]],\n",
      "\n",
      "        [[ 0.0952]],\n",
      "\n",
      "        [[ 0.1079]],\n",
      "\n",
      "        [[ 0.1347]],\n",
      "\n",
      "        [[ 0.0129]],\n",
      "\n",
      "        [[ 0.1238]],\n",
      "\n",
      "        [[ 0.1029]],\n",
      "\n",
      "        [[ 0.0307]],\n",
      "\n",
      "        [[ 0.1907]],\n",
      "\n",
      "        [[ 0.0554]],\n",
      "\n",
      "        [[ 0.1494]],\n",
      "\n",
      "        [[ 0.2645]],\n",
      "\n",
      "        [[ 0.0443]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0890]],\n",
      "\n",
      "        [[ 0.2270]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0425]],\n",
      "\n",
      "        [[ 0.1072]],\n",
      "\n",
      "        [[ 0.0564]],\n",
      "\n",
      "        [[ 0.0886]],\n",
      "\n",
      "        [[ 0.2450]],\n",
      "\n",
      "        [[ 0.1082]],\n",
      "\n",
      "        [[ 0.2057]],\n",
      "\n",
      "        [[ 0.0683]],\n",
      "\n",
      "        [[ 0.2054]],\n",
      "\n",
      "        [[ 0.0510]],\n",
      "\n",
      "        [[ 0.1038]],\n",
      "\n",
      "        [[ 0.0662]],\n",
      "\n",
      "        [[ 0.2036]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.1658]],\n",
      "\n",
      "        [[ 0.0512]],\n",
      "\n",
      "        [[ 0.1021]],\n",
      "\n",
      "        [[ 0.0286]],\n",
      "\n",
      "        [[ 0.1234]],\n",
      "\n",
      "        [[ 0.1135]],\n",
      "\n",
      "        [[ 0.0861]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.1265]],\n",
      "\n",
      "        [[ 0.1322]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0768]],\n",
      "\n",
      "        [[ 0.1328]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.1128]],\n",
      "\n",
      "        [[ 0.2167]],\n",
      "\n",
      "        [[ 0.1888]],\n",
      "\n",
      "        [[ 0.2660]],\n",
      "\n",
      "        [[ 0.0320]],\n",
      "\n",
      "        [[ 0.1384]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.2784]],\n",
      "\n",
      "        [[ 0.0101]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0254]],\n",
      "\n",
      "        [[ 0.2762]],\n",
      "\n",
      "        [[ 0.1519]],\n",
      "\n",
      "        [[ 0.0970]],\n",
      "\n",
      "        [[ 0.0358]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0855]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.1216]],\n",
      "\n",
      "        [[ 0.1295]],\n",
      "\n",
      "        [[ 0.0790]],\n",
      "\n",
      "        [[ 0.2881]],\n",
      "\n",
      "        [[ 0.0247]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0763]],\n",
      "\n",
      "        [[ 0.0561]],\n",
      "\n",
      "        [[ 0.0606]],\n",
      "\n",
      "        [[ 0.0590]],\n",
      "\n",
      "        [[ 0.1011]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.1336]],\n",
      "\n",
      "        [[ 0.1573]],\n",
      "\n",
      "        [[ 0.0582]],\n",
      "\n",
      "        [[ 0.0614]],\n",
      "\n",
      "        [[ 0.1770]],\n",
      "\n",
      "        [[ 0.2852]],\n",
      "\n",
      "        [[ 0.1083]],\n",
      "\n",
      "        [[ 0.0686]],\n",
      "\n",
      "        [[ 0.1880]],\n",
      "\n",
      "        [[ 0.1496]],\n",
      "\n",
      "        [[ 0.0471]],\n",
      "\n",
      "        [[ 0.1631]],\n",
      "\n",
      "        [[ 0.1126]],\n",
      "\n",
      "        [[ 0.2535]],\n",
      "\n",
      "        [[ 0.1363]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2527]],\n",
      "\n",
      "        [[ 0.1019]],\n",
      "\n",
      "        [[ 0.0307]],\n",
      "\n",
      "        [[ 0.1656]],\n",
      "\n",
      "        [[ 0.0412]],\n",
      "\n",
      "        [[ 0.1351]],\n",
      "\n",
      "        [[ 0.0679]],\n",
      "\n",
      "        [[ 0.1522]],\n",
      "\n",
      "        [[ 0.0204]],\n",
      "\n",
      "        [[ 0.2751]],\n",
      "\n",
      "        [[ 0.2853]],\n",
      "\n",
      "        [[ 0.1249]],\n",
      "\n",
      "        [[ 0.1724]],\n",
      "\n",
      "        [[ 0.2727]],\n",
      "\n",
      "        [[ 0.2393]],\n",
      "\n",
      "        [[ 0.1497]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0471]],\n",
      "\n",
      "        [[ 0.1230]],\n",
      "\n",
      "        [[ 0.1188]],\n",
      "\n",
      "        [[ 0.0748]],\n",
      "\n",
      "        [[ 0.0176]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.2413]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1162]],\n",
      "\n",
      "        [[ 0.1045]],\n",
      "\n",
      "        [[ 0.0488]],\n",
      "\n",
      "        [[ 0.2000]],\n",
      "\n",
      "        [[ 0.0165]],\n",
      "\n",
      "        [[ 0.1103]],\n",
      "\n",
      "        [[ 0.0588]],\n",
      "\n",
      "        [[ 0.0461]],\n",
      "\n",
      "        [[ 0.1605]],\n",
      "\n",
      "        [[ 0.0647]],\n",
      "\n",
      "        [[ 0.1439]],\n",
      "\n",
      "        [[ 0.0446]],\n",
      "\n",
      "        [[ 0.2399]],\n",
      "\n",
      "        [[ 0.0443]],\n",
      "\n",
      "        [[ 0.1430]],\n",
      "\n",
      "        [[ 0.1473]],\n",
      "\n",
      "        [[ 0.1359]],\n",
      "\n",
      "        [[ 0.0981]],\n",
      "\n",
      "        [[ 0.0268]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1393]],\n",
      "\n",
      "        [[ 0.1081]],\n",
      "\n",
      "        [[ 0.1445]],\n",
      "\n",
      "        [[ 0.1392]],\n",
      "\n",
      "        [[ 0.2359]],\n",
      "\n",
      "        [[ 0.0927]],\n",
      "\n",
      "        [[ 0.1718]],\n",
      "\n",
      "        [[ 0.1459]],\n",
      "\n",
      "        [[ 0.0693]],\n",
      "\n",
      "        [[ 0.0826]],\n",
      "\n",
      "        [[ 0.2802]],\n",
      "\n",
      "        [[ 0.1585]],\n",
      "\n",
      "        [[ 0.1724]],\n",
      "\n",
      "        [[ 0.0226]],\n",
      "\n",
      "        [[ 0.1451]],\n",
      "\n",
      "        [[ 0.0738]],\n",
      "\n",
      "        [[ 0.2029]],\n",
      "\n",
      "        [[ 0.1470]],\n",
      "\n",
      "        [[ 0.0329]],\n",
      "\n",
      "        [[ 0.1227]],\n",
      "\n",
      "        [[ 0.1975]],\n",
      "\n",
      "        [[ 0.2348]],\n",
      "\n",
      "        [[ 0.0820]],\n",
      "\n",
      "        [[ 0.0366]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1855]],\n",
      "\n",
      "        [[ 0.2687]],\n",
      "\n",
      "        [[ 0.1512]],\n",
      "\n",
      "        [[ 0.1105]],\n",
      "\n",
      "        [[ 0.1781]],\n",
      "\n",
      "        [[ 0.0699]],\n",
      "\n",
      "        [[ 0.0718]],\n",
      "\n",
      "        [[ 0.1024]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0805]],\n",
      "\n",
      "        [[ 0.1441]],\n",
      "\n",
      "        [[ 0.0339]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.1126]],\n",
      "\n",
      "        [[ 0.1422]],\n",
      "\n",
      "        [[ 0.1427]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0900]],\n",
      "\n",
      "        [[ 0.0650]],\n",
      "\n",
      "        [[ 0.0689]],\n",
      "\n",
      "        [[ 0.0197]],\n",
      "\n",
      "        [[ 0.0751]],\n",
      "\n",
      "        [[ 0.2104]],\n",
      "\n",
      "        [[ 0.0324]],\n",
      "\n",
      "        [[ 0.0925]],\n",
      "\n",
      "        [[ 0.1103]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0847]],\n",
      "\n",
      "        [[ 0.0772]],\n",
      "\n",
      "        [[ 0.1118]],\n",
      "\n",
      "        [[ 0.0988]],\n",
      "\n",
      "        [[ 0.1086]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0957]],\n",
      "\n",
      "        [[ 0.1189]],\n",
      "\n",
      "        [[ 0.1010]],\n",
      "\n",
      "        [[ 0.1081]],\n",
      "\n",
      "        [[ 0.2509]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1253]],\n",
      "\n",
      "        [[ 0.0897]],\n",
      "\n",
      "        [[ 0.1466]],\n",
      "\n",
      "        [[ 0.1576]],\n",
      "\n",
      "        [[ 0.0948]],\n",
      "\n",
      "        [[ 0.0645]],\n",
      "\n",
      "        [[ 0.0801]],\n",
      "\n",
      "        [[ 0.0661]],\n",
      "\n",
      "        [[ 0.0796]],\n",
      "\n",
      "        [[ 0.0335]],\n",
      "\n",
      "        [[ 0.0295]],\n",
      "\n",
      "        [[ 0.0640]],\n",
      "\n",
      "        [[ 0.1520]],\n",
      "\n",
      "        [[ 0.2668]],\n",
      "\n",
      "        [[ 0.2809]],\n",
      "\n",
      "        [[ 0.1453]],\n",
      "\n",
      "        [[ 0.1691]],\n",
      "\n",
      "        [[ 0.0992]],\n",
      "\n",
      "        [[ 0.0035]],\n",
      "\n",
      "        [[ 0.0152]],\n",
      "\n",
      "        [[ 0.1236]],\n",
      "\n",
      "        [[ 0.1543]],\n",
      "\n",
      "        [[ 0.1096]],\n",
      "\n",
      "        [[ 0.1163]],\n",
      "\n",
      "        [[ 0.1080]],\n",
      "\n",
      "        [[ 0.0892]],\n",
      "\n",
      "        [[ 0.0485]],\n",
      "\n",
      "        [[ 0.1241]],\n",
      "\n",
      "        [[ 0.0760]],\n",
      "\n",
      "        [[ 0.0581]],\n",
      "\n",
      "        [[ 0.0904]],\n",
      "\n",
      "        [[ 0.1850]],\n",
      "\n",
      "        [[ 0.2917]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.2682]],\n",
      "\n",
      "        [[ 0.2714]],\n",
      "\n",
      "        [[ 0.0188]],\n",
      "\n",
      "        [[ 0.0942]],\n",
      "\n",
      "        [[ 0.2444]],\n",
      "\n",
      "        [[ 0.1021]],\n",
      "\n",
      "        [[ 0.1533]],\n",
      "\n",
      "        [[ 0.2249]],\n",
      "\n",
      "        [[ 0.0917]],\n",
      "\n",
      "        [[ 0.1413]],\n",
      "\n",
      "        [[ 0.2599]],\n",
      "\n",
      "        [[ 0.2758]],\n",
      "\n",
      "        [[ 0.0733]],\n",
      "\n",
      "        [[ 0.0431]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.0925]],\n",
      "\n",
      "        [[ 0.0374]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2721]],\n",
      "\n",
      "        [[ 0.0855]],\n",
      "\n",
      "        [[ 0.1348]],\n",
      "\n",
      "        [[ 0.0631]],\n",
      "\n",
      "        [[ 0.0707]],\n",
      "\n",
      "        [[ 0.1685]],\n",
      "\n",
      "        [[ 0.1508]],\n",
      "\n",
      "        [[ 0.1557]],\n",
      "\n",
      "        [[ 0.1027]],\n",
      "\n",
      "        [[ 0.0996]],\n",
      "\n",
      "        [[ 0.1255]],\n",
      "\n",
      "        [[ 0.1443]],\n",
      "\n",
      "        [[ 0.0683]],\n",
      "\n",
      "        [[ 0.0348]],\n",
      "\n",
      "        [[ 0.0905]],\n",
      "\n",
      "        [[ 0.0959]],\n",
      "\n",
      "        [[ 0.1350]],\n",
      "\n",
      "        [[ 0.1805]],\n",
      "\n",
      "        [[ 0.2087]],\n",
      "\n",
      "        [[ 0.0175]],\n",
      "\n",
      "        [[ 0.1711]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0772]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1485]],\n",
      "\n",
      "        [[ 0.2467]],\n",
      "\n",
      "        [[ 0.1192]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.2608]],\n",
      "\n",
      "        [[ 0.1984]],\n",
      "\n",
      "        [[ 0.1035]],\n",
      "\n",
      "        [[ 0.1609]],\n",
      "\n",
      "        [[ 0.1066]],\n",
      "\n",
      "        [[ 0.0628]],\n",
      "\n",
      "        [[ 0.0931]],\n",
      "\n",
      "        [[ 0.2524]],\n",
      "\n",
      "        [[ 0.0770]],\n",
      "\n",
      "        [[ 0.2019]],\n",
      "\n",
      "        [[ 0.0091]],\n",
      "\n",
      "        [[ 0.0304]],\n",
      "\n",
      "        [[ 0.1982]],\n",
      "\n",
      "        [[ 0.0717]],\n",
      "\n",
      "        [[ 0.0800]],\n",
      "\n",
      "        [[ 0.0286]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0375]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.0472]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1355]],\n",
      "\n",
      "        [[ 0.0651]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.2574]],\n",
      "\n",
      "        [[ 0.0805]],\n",
      "\n",
      "        [[ 0.2837]],\n",
      "\n",
      "        [[ 0.0554]],\n",
      "\n",
      "        [[ 0.2019]],\n",
      "\n",
      "        [[ 0.0882]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0747]],\n",
      "\n",
      "        [[ 0.1468]],\n",
      "\n",
      "        [[ 0.1490]],\n",
      "\n",
      "        [[ 0.0700]],\n",
      "\n",
      "        [[ 0.1544]],\n",
      "\n",
      "        [[ 0.0812]],\n",
      "\n",
      "        [[ 0.2759]],\n",
      "\n",
      "        [[ 0.1065]],\n",
      "\n",
      "        [[ 0.1025]],\n",
      "\n",
      "        [[ 0.1451]],\n",
      "\n",
      "        [[ 0.2208]],\n",
      "\n",
      "        [[ 0.1994]],\n",
      "\n",
      "        [[ 0.2773]],\n",
      "\n",
      "        [[ 0.1105]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.1416]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1992]],\n",
      "\n",
      "        [[ 0.0987]],\n",
      "\n",
      "        [[ 0.0115]],\n",
      "\n",
      "        [[ 0.0117]],\n",
      "\n",
      "        [[ 0.0642]],\n",
      "\n",
      "        [[ 0.0644]],\n",
      "\n",
      "        [[ 0.1925]],\n",
      "\n",
      "        [[ 0.1737]],\n",
      "\n",
      "        [[ 0.2883]],\n",
      "\n",
      "        [[ 0.1288]],\n",
      "\n",
      "        [[ 0.1675]],\n",
      "\n",
      "        [[ 0.0618]],\n",
      "\n",
      "        [[ 0.0346]],\n",
      "\n",
      "        [[ 0.1148]],\n",
      "\n",
      "        [[ 0.2635]],\n",
      "\n",
      "        [[ 0.0562]],\n",
      "\n",
      "        [[ 0.1146]],\n",
      "\n",
      "        [[ 0.0496]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1587]],\n",
      "\n",
      "        [[ 0.1444]],\n",
      "\n",
      "        [[ 0.0116]],\n",
      "\n",
      "        [[ 0.1508]],\n",
      "\n",
      "        [[ 0.2148]],\n",
      "\n",
      "        [[ 0.2490]],\n",
      "\n",
      "        [[ 0.2025]],\n",
      "\n",
      "        [[ 0.1763]],\n",
      "\n",
      "        [[ 0.1354]],\n",
      "\n",
      "        [[ 0.2930]],\n",
      "\n",
      "        [[ 0.0252]],\n",
      "\n",
      "        [[ 0.1108]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[[-0.1988]]], device='cuda:0')\n",
      "torch.Size([896, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1412, -0.0122,  0.1809,  ..., -0.2223, -0.2477,  0.0536]],\n",
      "\n",
      "        [[ 0.2359, -0.1900,  0.0644,  ..., -0.1044, -0.1950,  0.2280]],\n",
      "\n",
      "        [[ 0.0266, -0.1385, -0.1522,  ..., -0.0755,  0.0562,  0.2256]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0682, -0.0865, -0.2075,  ...,  0.1701, -0.0242,  0.0405]],\n",
      "\n",
      "        [[ 0.1339, -0.0940, -0.0485,  ...,  0.0775,  0.0492,  0.0751]],\n",
      "\n",
      "        [[ 0.1585, -0.2263,  0.0065,  ...,  0.1392,  0.1942, -0.0728]]], device='cuda:0')\n",
      "torch.Size([1, 896, 512])\n",
      "tensor([[[ 7.2228e-03,  4.9989e-03,  1.2463e-03,  ..., -5.8449e-03,\n",
      "          -1.0041e-02, -9.7217e-03],\n",
      "         [ 4.0790e-03,  5.7755e-03,  3.9788e-03,  ...,  3.7458e-03,\n",
      "           1.5240e-03,  8.6613e-04],\n",
      "         [ 3.6372e-03, -7.9206e-04,  3.7780e-03,  ...,  1.1491e-02,\n",
      "           8.4554e-03,  7.1051e-03],\n",
      "         ...,\n",
      "         [ 2.5679e-03,  6.6162e-03,  1.0594e-02,  ...,  1.4071e-02,\n",
      "           1.7139e-02,  1.3884e-02],\n",
      "         [-8.1329e-03, -1.0979e-02, -9.4088e-03,  ..., -8.6224e-03,\n",
      "          -6.2706e-03, -7.0681e-03],\n",
      "         [-5.7822e-03, -9.0739e-03, -8.6851e-03,  ..., -4.8648e-03,\n",
      "          -2.1628e-03, -2.5321e-03]]], device='cuda:0')\n",
      "\n",
      "Validation set: Average loss: 1.2061, Accuracy: 2964/5000 (59.00%)\n",
      "\n",
      "the time of this epoch:[36.39824676513672 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.825463, Accuracy: 69.34\n",
      "Train Epoch: 7 [2560/50000 (6%)]\tLoss: 0.805559, Accuracy: 75.00\n",
      "Train Epoch: 7 [5120/50000 (11%)]\tLoss: 0.767601, Accuracy: 72.07\n",
      "Train Epoch: 7 [7680/50000 (17%)]\tLoss: 0.790504, Accuracy: 72.46\n",
      "Train Epoch: 7 [10240/50000 (23%)]\tLoss: 0.837906, Accuracy: 67.77\n",
      "Train Epoch: 7 [12800/50000 (28%)]\tLoss: 0.800927, Accuracy: 72.85\n",
      "Train Epoch: 7 [15360/50000 (34%)]\tLoss: 0.849508, Accuracy: 71.68\n",
      "Train Epoch: 7 [17920/50000 (40%)]\tLoss: 0.808032, Accuracy: 71.09\n",
      "Train Epoch: 7 [20480/50000 (45%)]\tLoss: 0.834743, Accuracy: 68.16\n",
      "Train Epoch: 7 [23040/50000 (51%)]\tLoss: 0.815036, Accuracy: 72.66\n",
      "Train Epoch: 7 [25600/50000 (57%)]\tLoss: 0.914729, Accuracy: 69.53\n",
      "Train Epoch: 7 [28160/50000 (62%)]\tLoss: 0.805486, Accuracy: 69.92\n",
      "Train Epoch: 7 [30720/50000 (68%)]\tLoss: 0.726323, Accuracy: 75.20\n",
      "Train Epoch: 7 [33280/50000 (74%)]\tLoss: 0.758338, Accuracy: 74.41\n",
      "Train Epoch: 7 [35840/50000 (80%)]\tLoss: 0.828130, Accuracy: 70.90\n",
      "Train Epoch: 7 [38400/50000 (85%)]\tLoss: 0.869447, Accuracy: 70.70\n",
      "Train Epoch: 7 [40960/50000 (91%)]\tLoss: 0.716115, Accuracy: 73.63\n",
      "Train Epoch: 7 [43520/50000 (97%)]\tLoss: 0.795580, Accuracy: 71.68\n",
      "\n",
      "Validation set: Average loss: 1.2729, Accuracy: 2883/5000 (57.00%)\n",
      "\n",
      "the time of this epoch:[36.408326387405396 s]\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.721305, Accuracy: 73.44\n",
      "Train Epoch: 8 [2560/50000 (6%)]\tLoss: 0.736543, Accuracy: 75.78\n",
      "Train Epoch: 8 [5120/50000 (11%)]\tLoss: 0.724206, Accuracy: 73.83\n",
      "Train Epoch: 8 [7680/50000 (17%)]\tLoss: 0.699557, Accuracy: 74.02\n",
      "Train Epoch: 8 [10240/50000 (23%)]\tLoss: 0.800143, Accuracy: 71.09\n",
      "Train Epoch: 8 [12800/50000 (28%)]\tLoss: 0.715457, Accuracy: 73.24\n",
      "Train Epoch: 8 [15360/50000 (34%)]\tLoss: 0.800405, Accuracy: 72.27\n",
      "Train Epoch: 8 [17920/50000 (40%)]\tLoss: 0.761427, Accuracy: 73.05\n",
      "Train Epoch: 8 [20480/50000 (45%)]\tLoss: 0.753718, Accuracy: 74.41\n",
      "Train Epoch: 8 [23040/50000 (51%)]\tLoss: 0.772330, Accuracy: 74.61\n",
      "Train Epoch: 8 [25600/50000 (57%)]\tLoss: 0.744579, Accuracy: 73.83\n",
      "Train Epoch: 8 [28160/50000 (62%)]\tLoss: 0.784133, Accuracy: 74.61\n",
      "Train Epoch: 8 [30720/50000 (68%)]\tLoss: 0.782709, Accuracy: 75.20\n",
      "Train Epoch: 8 [33280/50000 (74%)]\tLoss: 0.615068, Accuracy: 78.71\n",
      "Train Epoch: 8 [35840/50000 (80%)]\tLoss: 0.725770, Accuracy: 74.02\n",
      "Train Epoch: 8 [38400/50000 (85%)]\tLoss: 0.674393, Accuracy: 78.91\n",
      "Train Epoch: 8 [40960/50000 (91%)]\tLoss: 0.740358, Accuracy: 75.00\n",
      "Train Epoch: 8 [43520/50000 (97%)]\tLoss: 0.663004, Accuracy: 75.78\n",
      "\n",
      "Validation set: Average loss: 0.8079, Accuracy: 3643/5000 (72.00%)\n",
      "\n",
      "the time of this epoch:[36.44592237472534 s]\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.654954, Accuracy: 76.95\n",
      "Train Epoch: 9 [2560/50000 (6%)]\tLoss: 0.651014, Accuracy: 79.10\n",
      "Train Epoch: 9 [5120/50000 (11%)]\tLoss: 0.626789, Accuracy: 78.71\n",
      "Train Epoch: 9 [7680/50000 (17%)]\tLoss: 0.684412, Accuracy: 75.20\n",
      "Train Epoch: 9 [10240/50000 (23%)]\tLoss: 0.663829, Accuracy: 76.17\n",
      "Train Epoch: 9 [12800/50000 (28%)]\tLoss: 0.652871, Accuracy: 81.05\n",
      "Train Epoch: 9 [15360/50000 (34%)]\tLoss: 0.719915, Accuracy: 74.41\n",
      "Train Epoch: 9 [17920/50000 (40%)]\tLoss: 0.640647, Accuracy: 78.12\n",
      "Train Epoch: 9 [20480/50000 (45%)]\tLoss: 0.637023, Accuracy: 78.71\n",
      "Train Epoch: 9 [23040/50000 (51%)]\tLoss: 0.658776, Accuracy: 75.98\n",
      "Train Epoch: 9 [25600/50000 (57%)]\tLoss: 0.669198, Accuracy: 77.93\n",
      "Train Epoch: 9 [28160/50000 (62%)]\tLoss: 0.614277, Accuracy: 79.30\n",
      "Train Epoch: 9 [30720/50000 (68%)]\tLoss: 0.658616, Accuracy: 78.91\n",
      "Train Epoch: 9 [33280/50000 (74%)]\tLoss: 0.747963, Accuracy: 74.61\n",
      "Train Epoch: 9 [35840/50000 (80%)]\tLoss: 0.701398, Accuracy: 76.56\n",
      "Train Epoch: 9 [38400/50000 (85%)]\tLoss: 0.666419, Accuracy: 76.17\n",
      "Train Epoch: 9 [40960/50000 (91%)]\tLoss: 0.720309, Accuracy: 74.41\n",
      "Train Epoch: 9 [43520/50000 (97%)]\tLoss: 0.609649, Accuracy: 79.30\n",
      "\n",
      "Validation set: Average loss: 0.9064, Accuracy: 3468/5000 (69.00%)\n",
      "\n",
      "the time of this epoch:[36.38363242149353 s]\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.652329, Accuracy: 75.98\n",
      "Train Epoch: 10 [2560/50000 (6%)]\tLoss: 0.637076, Accuracy: 75.20\n",
      "Train Epoch: 10 [5120/50000 (11%)]\tLoss: 0.623585, Accuracy: 78.12\n",
      "Train Epoch: 10 [7680/50000 (17%)]\tLoss: 0.616071, Accuracy: 78.52\n",
      "Train Epoch: 10 [10240/50000 (23%)]\tLoss: 0.638930, Accuracy: 77.93\n",
      "Train Epoch: 10 [12800/50000 (28%)]\tLoss: 0.559457, Accuracy: 81.05\n",
      "Train Epoch: 10 [15360/50000 (34%)]\tLoss: 0.713220, Accuracy: 76.95\n",
      "Train Epoch: 10 [17920/50000 (40%)]\tLoss: 0.726063, Accuracy: 77.15\n",
      "Train Epoch: 10 [20480/50000 (45%)]\tLoss: 0.587527, Accuracy: 79.69\n",
      "Train Epoch: 10 [23040/50000 (51%)]\tLoss: 0.623381, Accuracy: 77.73\n",
      "Train Epoch: 10 [25600/50000 (57%)]\tLoss: 0.612914, Accuracy: 80.47\n",
      "Train Epoch: 10 [28160/50000 (62%)]\tLoss: 0.673103, Accuracy: 76.95\n",
      "Train Epoch: 10 [30720/50000 (68%)]\tLoss: 0.731858, Accuracy: 76.17\n",
      "Train Epoch: 10 [33280/50000 (74%)]\tLoss: 0.667822, Accuracy: 76.76\n",
      "Train Epoch: 10 [35840/50000 (80%)]\tLoss: 0.566019, Accuracy: 81.45\n",
      "Train Epoch: 10 [38400/50000 (85%)]\tLoss: 0.628547, Accuracy: 80.08\n",
      "Train Epoch: 10 [40960/50000 (91%)]\tLoss: 0.612260, Accuracy: 80.08\n",
      "Train Epoch: 10 [43520/50000 (97%)]\tLoss: 0.621238, Accuracy: 80.27\n",
      "\n",
      "Validation set: Average loss: 0.8758, Accuracy: 3626/5000 (72.00%)\n",
      "\n",
      "the time of this epoch:[36.415005683898926 s]\n",
      "\n",
      "Test set: Average loss: 0.8828, Accuracy: 7263/10000 (72.63%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.525013, Accuracy: 83.59\n",
      "Train Epoch: 11 [2560/50000 (6%)]\tLoss: 0.514204, Accuracy: 82.23\n",
      "Train Epoch: 11 [5120/50000 (11%)]\tLoss: 0.539602, Accuracy: 81.25\n",
      "Train Epoch: 11 [7680/50000 (17%)]\tLoss: 0.601569, Accuracy: 80.47\n",
      "Train Epoch: 11 [10240/50000 (23%)]\tLoss: 0.530745, Accuracy: 81.84\n",
      "Train Epoch: 11 [12800/50000 (28%)]\tLoss: 0.595623, Accuracy: 79.30\n",
      "Train Epoch: 11 [15360/50000 (34%)]\tLoss: 0.564361, Accuracy: 79.10\n",
      "Train Epoch: 11 [17920/50000 (40%)]\tLoss: 0.565001, Accuracy: 81.25\n",
      "Train Epoch: 11 [20480/50000 (45%)]\tLoss: 0.504207, Accuracy: 83.98\n",
      "Train Epoch: 11 [23040/50000 (51%)]\tLoss: 0.638399, Accuracy: 80.27\n",
      "Train Epoch: 11 [25600/50000 (57%)]\tLoss: 0.526725, Accuracy: 82.23\n",
      "Train Epoch: 11 [28160/50000 (62%)]\tLoss: 0.618901, Accuracy: 79.10\n",
      "Train Epoch: 11 [30720/50000 (68%)]\tLoss: 0.532779, Accuracy: 81.05\n",
      "Train Epoch: 11 [33280/50000 (74%)]\tLoss: 0.604974, Accuracy: 77.34\n",
      "Train Epoch: 11 [35840/50000 (80%)]\tLoss: 0.666004, Accuracy: 76.17\n",
      "Train Epoch: 11 [38400/50000 (85%)]\tLoss: 0.652513, Accuracy: 78.71\n",
      "Train Epoch: 11 [40960/50000 (91%)]\tLoss: 0.578124, Accuracy: 78.71\n",
      "Train Epoch: 11 [43520/50000 (97%)]\tLoss: 0.586868, Accuracy: 80.08\n",
      "\n",
      "Validation set: Average loss: 0.9133, Accuracy: 3602/5000 (72.00%)\n",
      "\n",
      "the time of this epoch:[39.52678108215332 s]\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.496667, Accuracy: 82.42\n",
      "Train Epoch: 12 [2560/50000 (6%)]\tLoss: 0.551114, Accuracy: 82.42\n",
      "Train Epoch: 12 [5120/50000 (11%)]\tLoss: 0.534828, Accuracy: 80.86\n",
      "Train Epoch: 12 [7680/50000 (17%)]\tLoss: 0.550924, Accuracy: 79.69\n",
      "Train Epoch: 12 [10240/50000 (23%)]\tLoss: 0.542163, Accuracy: 83.20\n",
      "Train Epoch: 12 [12800/50000 (28%)]\tLoss: 0.535193, Accuracy: 80.47\n",
      "Train Epoch: 12 [15360/50000 (34%)]\tLoss: 0.519490, Accuracy: 83.98\n",
      "Train Epoch: 12 [17920/50000 (40%)]\tLoss: 0.520836, Accuracy: 81.84\n",
      "Train Epoch: 12 [20480/50000 (45%)]\tLoss: 0.581345, Accuracy: 81.05\n",
      "Train Epoch: 12 [23040/50000 (51%)]\tLoss: 0.431577, Accuracy: 84.96\n",
      "Train Epoch: 12 [25600/50000 (57%)]\tLoss: 0.521949, Accuracy: 82.62\n",
      "Train Epoch: 12 [28160/50000 (62%)]\tLoss: 0.527075, Accuracy: 81.05\n",
      "Train Epoch: 12 [30720/50000 (68%)]\tLoss: 0.593463, Accuracy: 79.10\n",
      "Train Epoch: 12 [33280/50000 (74%)]\tLoss: 0.518310, Accuracy: 81.25\n",
      "Train Epoch: 12 [35840/50000 (80%)]\tLoss: 0.534017, Accuracy: 80.66\n",
      "Train Epoch: 12 [38400/50000 (85%)]\tLoss: 0.588813, Accuracy: 79.49\n",
      "Train Epoch: 12 [40960/50000 (91%)]\tLoss: 0.531813, Accuracy: 83.59\n",
      "Train Epoch: 12 [43520/50000 (97%)]\tLoss: 0.534224, Accuracy: 82.42\n",
      "\n",
      "Validation set: Average loss: 0.8789, Accuracy: 3604/5000 (72.00%)\n",
      "\n",
      "the time of this epoch:[36.42159652709961 s]\n",
      "\n",
      "Test set: Average loss: 0.8860, Accuracy: 7143/10000 (71.43%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.516194, Accuracy: 82.81\n",
      "Train Epoch: 13 [2560/50000 (6%)]\tLoss: 0.452188, Accuracy: 85.55\n",
      "Train Epoch: 13 [5120/50000 (11%)]\tLoss: 0.525313, Accuracy: 81.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [7680/50000 (17%)]\tLoss: 0.482936, Accuracy: 82.62\n",
      "Train Epoch: 13 [10240/50000 (23%)]\tLoss: 0.418178, Accuracy: 85.94\n",
      "Train Epoch: 13 [12800/50000 (28%)]\tLoss: 0.495072, Accuracy: 83.40\n",
      "Train Epoch: 13 [15360/50000 (34%)]\tLoss: 0.535469, Accuracy: 80.86\n",
      "Train Epoch: 13 [17920/50000 (40%)]\tLoss: 0.399869, Accuracy: 85.74\n",
      "Train Epoch: 13 [20480/50000 (45%)]\tLoss: 0.508209, Accuracy: 83.20\n",
      "Train Epoch: 13 [23040/50000 (51%)]\tLoss: 0.493418, Accuracy: 81.25\n",
      "Train Epoch: 13 [25600/50000 (57%)]\tLoss: 0.516093, Accuracy: 82.62\n",
      "Train Epoch: 13 [28160/50000 (62%)]\tLoss: 0.503919, Accuracy: 82.23\n",
      "Train Epoch: 13 [30720/50000 (68%)]\tLoss: 0.485510, Accuracy: 82.23\n",
      "Train Epoch: 13 [33280/50000 (74%)]\tLoss: 0.450114, Accuracy: 83.98\n",
      "Train Epoch: 13 [35840/50000 (80%)]\tLoss: 0.493425, Accuracy: 83.98\n",
      "Train Epoch: 13 [38400/50000 (85%)]\tLoss: 0.522821, Accuracy: 80.86\n",
      "Train Epoch: 13 [40960/50000 (91%)]\tLoss: 0.570168, Accuracy: 80.66\n",
      "Train Epoch: 13 [43520/50000 (97%)]\tLoss: 0.509623, Accuracy: 82.62\n",
      "\n",
      "Validation set: Average loss: 0.6755, Accuracy: 3888/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[39.51806592941284 s]\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.458820, Accuracy: 85.74\n",
      "Train Epoch: 14 [2560/50000 (6%)]\tLoss: 0.524578, Accuracy: 79.88\n",
      "Train Epoch: 14 [5120/50000 (11%)]\tLoss: 0.458419, Accuracy: 85.16\n",
      "Train Epoch: 14 [7680/50000 (17%)]\tLoss: 0.463251, Accuracy: 84.57\n",
      "Train Epoch: 14 [10240/50000 (23%)]\tLoss: 0.453202, Accuracy: 83.40\n",
      "Train Epoch: 14 [12800/50000 (28%)]\tLoss: 0.445352, Accuracy: 83.20\n",
      "Train Epoch: 14 [15360/50000 (34%)]\tLoss: 0.503894, Accuracy: 81.84\n",
      "Train Epoch: 14 [17920/50000 (40%)]\tLoss: 0.469137, Accuracy: 83.01\n",
      "Train Epoch: 14 [20480/50000 (45%)]\tLoss: 0.516789, Accuracy: 83.01\n",
      "Train Epoch: 14 [23040/50000 (51%)]\tLoss: 0.499507, Accuracy: 84.96\n",
      "Train Epoch: 14 [25600/50000 (57%)]\tLoss: 0.545896, Accuracy: 81.64\n",
      "Train Epoch: 14 [28160/50000 (62%)]\tLoss: 0.455609, Accuracy: 84.77\n",
      "Train Epoch: 14 [30720/50000 (68%)]\tLoss: 0.445839, Accuracy: 83.20\n",
      "Train Epoch: 14 [33280/50000 (74%)]\tLoss: 0.466205, Accuracy: 83.20\n",
      "Train Epoch: 14 [35840/50000 (80%)]\tLoss: 0.445440, Accuracy: 84.96\n",
      "Train Epoch: 14 [38400/50000 (85%)]\tLoss: 0.540353, Accuracy: 80.27\n",
      "Train Epoch: 14 [40960/50000 (91%)]\tLoss: 0.591460, Accuracy: 79.88\n",
      "Train Epoch: 14 [43520/50000 (97%)]\tLoss: 0.468592, Accuracy: 84.57\n",
      "\n",
      "Validation set: Average loss: 0.7626, Accuracy: 3788/5000 (75.00%)\n",
      "\n",
      "the time of this epoch:[36.38608980178833 s]\n",
      "\n",
      "Test set: Average loss: 0.7665, Accuracy: 7526/10000 (75.26%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.453411, Accuracy: 83.59\n",
      "Train Epoch: 15 [2560/50000 (6%)]\tLoss: 0.439451, Accuracy: 86.13\n",
      "Train Epoch: 15 [5120/50000 (11%)]\tLoss: 0.405970, Accuracy: 87.11\n",
      "Train Epoch: 15 [7680/50000 (17%)]\tLoss: 0.397369, Accuracy: 85.74\n",
      "Train Epoch: 15 [10240/50000 (23%)]\tLoss: 0.503074, Accuracy: 81.45\n",
      "Train Epoch: 15 [12800/50000 (28%)]\tLoss: 0.467513, Accuracy: 85.16\n",
      "Train Epoch: 15 [15360/50000 (34%)]\tLoss: 0.479038, Accuracy: 82.23\n",
      "Train Epoch: 15 [17920/50000 (40%)]\tLoss: 0.459229, Accuracy: 83.40\n",
      "Train Epoch: 15 [20480/50000 (45%)]\tLoss: 0.483262, Accuracy: 84.38\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0891]],\n",
      "\n",
      "        [[ 0.0727]],\n",
      "\n",
      "        [[ 0.0775]],\n",
      "\n",
      "        [[ 0.0673]],\n",
      "\n",
      "        [[ 0.0620]],\n",
      "\n",
      "        [[ 0.2188]],\n",
      "\n",
      "        [[ 0.0830]],\n",
      "\n",
      "        [[ 0.0355]],\n",
      "\n",
      "        [[ 0.0590]],\n",
      "\n",
      "        [[ 0.1854]],\n",
      "\n",
      "        [[ 0.0984]],\n",
      "\n",
      "        [[ 0.0441]],\n",
      "\n",
      "        [[ 0.0898]],\n",
      "\n",
      "        [[ 0.0578]],\n",
      "\n",
      "        [[ 0.0886]],\n",
      "\n",
      "        [[ 0.0614]],\n",
      "\n",
      "        [[ 0.1040]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0548]],\n",
      "\n",
      "        [[ 0.0400]],\n",
      "\n",
      "        [[ 0.0779]],\n",
      "\n",
      "        [[ 0.1411]],\n",
      "\n",
      "        [[ 0.1181]],\n",
      "\n",
      "        [[ 0.0986]],\n",
      "\n",
      "        [[ 0.0541]],\n",
      "\n",
      "        [[ 0.0604]],\n",
      "\n",
      "        [[ 0.1291]],\n",
      "\n",
      "        [[ 0.2773]],\n",
      "\n",
      "        [[ 0.1978]],\n",
      "\n",
      "        [[ 0.0182]],\n",
      "\n",
      "        [[ 0.0561]],\n",
      "\n",
      "        [[ 0.3021]],\n",
      "\n",
      "        [[ 0.0892]],\n",
      "\n",
      "        [[ 0.2666]],\n",
      "\n",
      "        [[ 0.1291]],\n",
      "\n",
      "        [[ 0.2970]],\n",
      "\n",
      "        [[ 0.1928]],\n",
      "\n",
      "        [[ 0.1301]],\n",
      "\n",
      "        [[ 0.1171]],\n",
      "\n",
      "        [[ 0.1433]],\n",
      "\n",
      "        [[ 0.0618]],\n",
      "\n",
      "        [[ 0.0312]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0601]],\n",
      "\n",
      "        [[ 0.1361]],\n",
      "\n",
      "        [[ 0.0585]],\n",
      "\n",
      "        [[ 0.0561]],\n",
      "\n",
      "        [[ 0.0876]],\n",
      "\n",
      "        [[ 0.2775]],\n",
      "\n",
      "        [[ 0.3235]],\n",
      "\n",
      "        [[ 0.1685]],\n",
      "\n",
      "        [[ 0.2594]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.1002]],\n",
      "\n",
      "        [[ 0.3069]],\n",
      "\n",
      "        [[ 0.1096]],\n",
      "\n",
      "        [[ 0.1267]],\n",
      "\n",
      "        [[ 0.2845]],\n",
      "\n",
      "        [[ 0.4190]],\n",
      "\n",
      "        [[ 0.1411]],\n",
      "\n",
      "        [[ 0.0607]],\n",
      "\n",
      "        [[ 0.1296]],\n",
      "\n",
      "        [[ 0.0461]],\n",
      "\n",
      "        [[ 0.0914]],\n",
      "\n",
      "        [[ 0.0966]],\n",
      "\n",
      "        [[ 0.0283]],\n",
      "\n",
      "        [[ 0.0102]],\n",
      "\n",
      "        [[ 0.3442]],\n",
      "\n",
      "        [[ 0.2002]],\n",
      "\n",
      "        [[ 0.2680]],\n",
      "\n",
      "        [[ 0.1068]],\n",
      "\n",
      "        [[ 0.1033]],\n",
      "\n",
      "        [[ 0.1684]],\n",
      "\n",
      "        [[ 0.2117]],\n",
      "\n",
      "        [[ 0.0585]],\n",
      "\n",
      "        [[ 0.1432]],\n",
      "\n",
      "        [[ 0.0525]],\n",
      "\n",
      "        [[ 0.1468]],\n",
      "\n",
      "        [[ 0.1430]],\n",
      "\n",
      "        [[ 0.0794]],\n",
      "\n",
      "        [[ 0.0765]],\n",
      "\n",
      "        [[ 0.1027]],\n",
      "\n",
      "        [[ 0.1258]],\n",
      "\n",
      "        [[ 0.0574]],\n",
      "\n",
      "        [[ 0.1055]],\n",
      "\n",
      "        [[ 0.0761]],\n",
      "\n",
      "        [[ 0.1271]],\n",
      "\n",
      "        [[ 0.1120]],\n",
      "\n",
      "        [[ 0.2615]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1451]],\n",
      "\n",
      "        [[ 0.1303]],\n",
      "\n",
      "        [[ 0.1186]],\n",
      "\n",
      "        [[ 0.2695]],\n",
      "\n",
      "        [[ 0.3850]],\n",
      "\n",
      "        [[ 0.2042]],\n",
      "\n",
      "        [[ 0.1156]],\n",
      "\n",
      "        [[ 0.1186]],\n",
      "\n",
      "        [[ 0.0499]],\n",
      "\n",
      "        [[ 0.0541]],\n",
      "\n",
      "        [[ 0.2303]],\n",
      "\n",
      "        [[ 0.0500]],\n",
      "\n",
      "        [[ 0.1333]],\n",
      "\n",
      "        [[ 0.1507]],\n",
      "\n",
      "        [[ 0.2028]],\n",
      "\n",
      "        [[ 0.0680]],\n",
      "\n",
      "        [[ 0.2262]],\n",
      "\n",
      "        [[ 0.1316]],\n",
      "\n",
      "        [[ 0.0528]],\n",
      "\n",
      "        [[ 0.0368]],\n",
      "\n",
      "        [[ 0.1622]],\n",
      "\n",
      "        [[ 0.1028]],\n",
      "\n",
      "        [[ 0.2714]],\n",
      "\n",
      "        [[ 0.1969]],\n",
      "\n",
      "        [[ 0.0695]],\n",
      "\n",
      "        [[ 0.2940]],\n",
      "\n",
      "        [[ 0.0102]],\n",
      "\n",
      "        [[ 0.1283]],\n",
      "\n",
      "        [[ 0.0517]],\n",
      "\n",
      "        [[ 0.0176]],\n",
      "\n",
      "        [[ 0.1894]],\n",
      "\n",
      "        [[ 0.2055]],\n",
      "\n",
      "        [[ 0.4037]],\n",
      "\n",
      "        [[ 0.3257]],\n",
      "\n",
      "        [[ 0.0284]],\n",
      "\n",
      "        [[ 0.2010]],\n",
      "\n",
      "        [[ 0.1094]],\n",
      "\n",
      "        [[ 0.3788]],\n",
      "\n",
      "        [[ 0.0608]],\n",
      "\n",
      "        [[ 0.1111]],\n",
      "\n",
      "        [[ 0.3438]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0972]],\n",
      "\n",
      "        [[ 0.3854]],\n",
      "\n",
      "        [[ 0.2478]],\n",
      "\n",
      "        [[ 0.2953]],\n",
      "\n",
      "        [[ 0.2724]],\n",
      "\n",
      "        [[ 0.1116]],\n",
      "\n",
      "        [[ 0.2305]],\n",
      "\n",
      "        [[ 0.1146]],\n",
      "\n",
      "        [[ 0.1776]],\n",
      "\n",
      "        [[ 0.0201]],\n",
      "\n",
      "        [[ 0.1316]],\n",
      "\n",
      "        [[ 0.1407]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.1054]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0447]],\n",
      "\n",
      "        [[ 0.0331]],\n",
      "\n",
      "        [[ 0.0743]],\n",
      "\n",
      "        [[ 0.3248]],\n",
      "\n",
      "        [[ 0.0945]],\n",
      "\n",
      "        [[ 0.1199]],\n",
      "\n",
      "        [[ 0.3244]],\n",
      "\n",
      "        [[ 0.1206]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0822]],\n",
      "\n",
      "        [[ 0.1062]],\n",
      "\n",
      "        [[ 0.3753]],\n",
      "\n",
      "        [[ 0.1238]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0734]],\n",
      "\n",
      "        [[ 0.0504]],\n",
      "\n",
      "        [[ 0.1135]],\n",
      "\n",
      "        [[ 0.1182]],\n",
      "\n",
      "        [[ 0.1503]],\n",
      "\n",
      "        [[ 0.1733]],\n",
      "\n",
      "        [[ 0.0557]],\n",
      "\n",
      "        [[ 0.1146]],\n",
      "\n",
      "        [[ 0.1237]],\n",
      "\n",
      "        [[ 0.0667]],\n",
      "\n",
      "        [[ 0.3298]],\n",
      "\n",
      "        [[ 0.0348]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.1257]],\n",
      "\n",
      "        [[ 0.0567]],\n",
      "\n",
      "        [[ 0.0511]],\n",
      "\n",
      "        [[ 0.0997]],\n",
      "\n",
      "        [[ 0.1457]],\n",
      "\n",
      "        [[ 0.0553]],\n",
      "\n",
      "        [[ 0.0647]],\n",
      "\n",
      "        [[ 0.1849]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1402]],\n",
      "\n",
      "        [[ 0.1354]],\n",
      "\n",
      "        [[ 0.0860]],\n",
      "\n",
      "        [[ 0.3334]],\n",
      "\n",
      "        [[ 0.0677]],\n",
      "\n",
      "        [[ 0.0479]],\n",
      "\n",
      "        [[ 0.1251]],\n",
      "\n",
      "        [[ 0.1353]],\n",
      "\n",
      "        [[ 0.0685]],\n",
      "\n",
      "        [[ 0.0808]],\n",
      "\n",
      "        [[ 0.1558]],\n",
      "\n",
      "        [[ 0.1122]],\n",
      "\n",
      "        [[ 0.2108]],\n",
      "\n",
      "        [[ 0.1452]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.2550]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0421]],\n",
      "\n",
      "        [[ 0.0012]],\n",
      "\n",
      "        [[ 0.2823]],\n",
      "\n",
      "        [[ 0.2238]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.3353]],\n",
      "\n",
      "        [[ 0.0812]],\n",
      "\n",
      "        [[ 0.0107]],\n",
      "\n",
      "        [[ 0.0227]],\n",
      "\n",
      "        [[ 0.2063]],\n",
      "\n",
      "        [[ 0.0065]],\n",
      "\n",
      "        [[ 0.2877]],\n",
      "\n",
      "        [[ 0.2046]],\n",
      "\n",
      "        [[ 0.0337]],\n",
      "\n",
      "        [[ 0.0959]],\n",
      "\n",
      "        [[ 0.0788]],\n",
      "\n",
      "        [[ 0.1311]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1402]],\n",
      "\n",
      "        [[ 0.3162]],\n",
      "\n",
      "        [[ 0.1537]],\n",
      "\n",
      "        [[ 0.0845]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.1196]],\n",
      "\n",
      "        [[ 0.2689]],\n",
      "\n",
      "        [[ 0.0797]],\n",
      "\n",
      "        [[ 0.1345]],\n",
      "\n",
      "        [[ 0.0952]],\n",
      "\n",
      "        [[ 0.1353]],\n",
      "\n",
      "        [[ 0.0597]],\n",
      "\n",
      "        [[ 0.4023]],\n",
      "\n",
      "        [[ 0.3294]],\n",
      "\n",
      "        [[ 0.0274]],\n",
      "\n",
      "        [[ 0.0170]],\n",
      "\n",
      "        [[ 0.4035]],\n",
      "\n",
      "        [[ 0.1631]],\n",
      "\n",
      "        [[ 0.0436]],\n",
      "\n",
      "        [[ 0.1891]],\n",
      "\n",
      "        [[ 0.1510]],\n",
      "\n",
      "        [[ 0.3118]],\n",
      "\n",
      "        [[ 0.1058]],\n",
      "\n",
      "        [[ 0.3643]],\n",
      "\n",
      "        [[ 0.1899]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.1111]],\n",
      "\n",
      "        [[ 0.0720]],\n",
      "\n",
      "        [[ 0.3387]],\n",
      "\n",
      "        [[ 0.3011]],\n",
      "\n",
      "        [[ 0.3529]],\n",
      "\n",
      "        [[ 0.3266]],\n",
      "\n",
      "        [[ 0.3434]],\n",
      "\n",
      "        [[ 0.0795]],\n",
      "\n",
      "        [[ 0.3693]],\n",
      "\n",
      "        [[ 0.1631]],\n",
      "\n",
      "        [[ 0.3722]],\n",
      "\n",
      "        [[ 0.2129]],\n",
      "\n",
      "        [[ 0.1430]],\n",
      "\n",
      "        [[ 0.0431]],\n",
      "\n",
      "        [[ 0.2696]],\n",
      "\n",
      "        [[ 0.1613]],\n",
      "\n",
      "        [[ 0.2608]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.2187]],\n",
      "\n",
      "        [[ 0.0817]],\n",
      "\n",
      "        [[ 0.3028]],\n",
      "\n",
      "        [[ 0.0858]],\n",
      "\n",
      "        [[ 0.1417]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.1455]],\n",
      "\n",
      "        [[ 0.1210]],\n",
      "\n",
      "        [[ 0.3494]],\n",
      "\n",
      "        [[ 0.1595]],\n",
      "\n",
      "        [[ 0.0465]],\n",
      "\n",
      "        [[ 0.0423]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0575]],\n",
      "\n",
      "        [[ 0.1728]],\n",
      "\n",
      "        [[ 0.1384]],\n",
      "\n",
      "        [[ 0.1134]],\n",
      "\n",
      "        [[ 0.1183]],\n",
      "\n",
      "        [[ 0.1206]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0457]],\n",
      "\n",
      "        [[ 0.0581]],\n",
      "\n",
      "        [[ 0.0738]],\n",
      "\n",
      "        [[ 0.1295]],\n",
      "\n",
      "        [[ 0.0763]],\n",
      "\n",
      "        [[ 0.0443]],\n",
      "\n",
      "        [[ 0.3158]],\n",
      "\n",
      "        [[ 0.0821]],\n",
      "\n",
      "        [[ 0.0649]],\n",
      "\n",
      "        [[ 0.2778]],\n",
      "\n",
      "        [[ 0.0047]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1464]],\n",
      "\n",
      "        [[ 0.3079]],\n",
      "\n",
      "        [[ 0.0516]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.2883]],\n",
      "\n",
      "        [[ 0.0660]],\n",
      "\n",
      "        [[ 0.1215]],\n",
      "\n",
      "        [[ 0.1060]],\n",
      "\n",
      "        [[ 0.2592]],\n",
      "\n",
      "        [[ 0.1267]],\n",
      "\n",
      "        [[ 0.3720]],\n",
      "\n",
      "        [[ 0.0632]],\n",
      "\n",
      "        [[ 0.0523]],\n",
      "\n",
      "        [[ 0.0848]],\n",
      "\n",
      "        [[ 0.3066]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0352]],\n",
      "\n",
      "        [[ 0.2659]],\n",
      "\n",
      "        [[ 0.0433]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.1175]],\n",
      "\n",
      "        [[ 0.3447]],\n",
      "\n",
      "        [[ 0.0934]],\n",
      "\n",
      "        [[ 0.1258]],\n",
      "\n",
      "        [[ 0.3664]],\n",
      "\n",
      "        [[ 0.0792]],\n",
      "\n",
      "        [[ 0.0900]],\n",
      "\n",
      "        [[ 0.0401]],\n",
      "\n",
      "        [[ 0.0832]],\n",
      "\n",
      "        [[ 0.0563]],\n",
      "\n",
      "        [[ 0.0891]],\n",
      "\n",
      "        [[ 0.3101]],\n",
      "\n",
      "        [[ 0.0525]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.1565]],\n",
      "\n",
      "        [[ 0.3499]],\n",
      "\n",
      "        [[ 0.1538]],\n",
      "\n",
      "        [[ 0.1316]],\n",
      "\n",
      "        [[ 0.2495]],\n",
      "\n",
      "        [[ 0.3185]],\n",
      "\n",
      "        [[ 0.3432]],\n",
      "\n",
      "        [[ 0.1703]],\n",
      "\n",
      "        [[ 0.2681]],\n",
      "\n",
      "        [[ 0.2538]],\n",
      "\n",
      "        [[ 0.2412]],\n",
      "\n",
      "        [[ 0.0409]],\n",
      "\n",
      "        [[ 0.1129]],\n",
      "\n",
      "        [[ 0.3077]],\n",
      "\n",
      "        [[ 0.0920]],\n",
      "\n",
      "        [[ 0.3433]],\n",
      "\n",
      "        [[ 0.1686]],\n",
      "\n",
      "        [[ 0.0897]],\n",
      "\n",
      "        [[ 0.3165]],\n",
      "\n",
      "        [[ 0.1414]],\n",
      "\n",
      "        [[ 0.1234]],\n",
      "\n",
      "        [[ 0.3045]],\n",
      "\n",
      "        [[ 0.1473]],\n",
      "\n",
      "        [[ 0.0408]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.2303]],\n",
      "\n",
      "        [[ 0.3889]],\n",
      "\n",
      "        [[ 0.0723]],\n",
      "\n",
      "        [[ 0.4008]],\n",
      "\n",
      "        [[ 0.1859]],\n",
      "\n",
      "        [[ 0.3246]],\n",
      "\n",
      "        [[ 0.0836]],\n",
      "\n",
      "        [[ 0.0512]],\n",
      "\n",
      "        [[ 0.3290]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.1982]],\n",
      "\n",
      "        [[ 0.1521]],\n",
      "\n",
      "        [[ 0.3742]],\n",
      "\n",
      "        [[ 0.2916]],\n",
      "\n",
      "        [[ 0.1151]],\n",
      "\n",
      "        [[ 0.0507]],\n",
      "\n",
      "        [[ 0.2708]],\n",
      "\n",
      "        [[ 0.1151]],\n",
      "\n",
      "        [[ 0.0438]],\n",
      "\n",
      "        [[ 0.1071]],\n",
      "\n",
      "        [[ 0.1249]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.3616]],\n",
      "\n",
      "        [[ 0.1046]],\n",
      "\n",
      "        [[ 0.1122]],\n",
      "\n",
      "        [[ 0.1168]],\n",
      "\n",
      "        [[ 0.1440]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.3076]],\n",
      "\n",
      "        [[ 0.1908]],\n",
      "\n",
      "        [[ 0.1425]],\n",
      "\n",
      "        [[ 0.0746]],\n",
      "\n",
      "        [[ 0.3700]],\n",
      "\n",
      "        [[ 0.3912]],\n",
      "\n",
      "        [[ 0.0682]],\n",
      "\n",
      "        [[ 0.0752]],\n",
      "\n",
      "        [[ 0.0578]],\n",
      "\n",
      "        [[ 0.0394]],\n",
      "\n",
      "        [[ 0.3667]],\n",
      "\n",
      "        [[ 0.0801]],\n",
      "\n",
      "        [[ 0.1067]],\n",
      "\n",
      "        [[ 0.1298]],\n",
      "\n",
      "        [[ 0.1010]],\n",
      "\n",
      "        [[ 0.0578]],\n",
      "\n",
      "        [[ 0.1055]],\n",
      "\n",
      "        [[ 0.0677]],\n",
      "\n",
      "        [[ 0.3682]],\n",
      "\n",
      "        [[ 0.0690]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2393]],\n",
      "\n",
      "        [[ 0.0093]],\n",
      "\n",
      "        [[ 0.0687]],\n",
      "\n",
      "        [[ 0.1077]],\n",
      "\n",
      "        [[ 0.1228]],\n",
      "\n",
      "        [[ 0.3929]],\n",
      "\n",
      "        [[ 0.0844]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.0426]],\n",
      "\n",
      "        [[ 0.3617]],\n",
      "\n",
      "        [[ 0.0825]],\n",
      "\n",
      "        [[ 0.0071]],\n",
      "\n",
      "        [[ 0.0892]],\n",
      "\n",
      "        [[ 0.1567]],\n",
      "\n",
      "        [[ 0.1535]],\n",
      "\n",
      "        [[ 0.2202]],\n",
      "\n",
      "        [[ 0.1747]],\n",
      "\n",
      "        [[ 0.0864]],\n",
      "\n",
      "        [[ 0.0526]],\n",
      "\n",
      "        [[ 0.2457]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1241]],\n",
      "\n",
      "        [[ 0.1909]],\n",
      "\n",
      "        [[ 0.0465]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.3070]],\n",
      "\n",
      "        [[ 0.1413]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0915]],\n",
      "\n",
      "        [[ 0.2291]],\n",
      "\n",
      "        [[ 0.0283]],\n",
      "\n",
      "        [[ 0.1070]],\n",
      "\n",
      "        [[ 0.2929]],\n",
      "\n",
      "        [[ 0.3408]],\n",
      "\n",
      "        [[ 0.0517]],\n",
      "\n",
      "        [[ 0.3379]],\n",
      "\n",
      "        [[ 0.3477]],\n",
      "\n",
      "        [[ 0.1533]],\n",
      "\n",
      "        [[ 0.2362]],\n",
      "\n",
      "        [[ 0.1263]],\n",
      "\n",
      "        [[ 0.0691]],\n",
      "\n",
      "        [[ 0.3619]],\n",
      "\n",
      "        [[ 0.1177]],\n",
      "\n",
      "        [[ 0.2060]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.1536]],\n",
      "\n",
      "        [[ 0.1295]],\n",
      "\n",
      "        [[ 0.3003]],\n",
      "\n",
      "        [[ 0.0461]],\n",
      "\n",
      "        [[ 0.2385]],\n",
      "\n",
      "        [[ 0.0897]],\n",
      "\n",
      "        [[ 0.1505]],\n",
      "\n",
      "        [[ 0.0855]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.1410]],\n",
      "\n",
      "        [[ 0.0510]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0407]],\n",
      "\n",
      "        [[ 0.0839]],\n",
      "\n",
      "        [[ 0.1190]],\n",
      "\n",
      "        [[ 0.3523]],\n",
      "\n",
      "        [[ 0.0603]],\n",
      "\n",
      "        [[ 0.1280]],\n",
      "\n",
      "        [[ 0.1428]],\n",
      "\n",
      "        [[ 0.0731]],\n",
      "\n",
      "        [[ 0.2247]],\n",
      "\n",
      "        [[ 0.1546]],\n",
      "\n",
      "        [[ 0.1380]],\n",
      "\n",
      "        [[ 0.0556]],\n",
      "\n",
      "        [[ 0.1910]],\n",
      "\n",
      "        [[ 0.0393]],\n",
      "\n",
      "        [[ 0.0445]],\n",
      "\n",
      "        [[ 0.3264]],\n",
      "\n",
      "        [[ 0.3172]],\n",
      "\n",
      "        [[ 0.0826]],\n",
      "\n",
      "        [[ 0.0795]],\n",
      "\n",
      "        [[ 0.3484]],\n",
      "\n",
      "        [[ 0.3770]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.3993]],\n",
      "\n",
      "        [[ 0.3946]],\n",
      "\n",
      "        [[ 0.0418]],\n",
      "\n",
      "        [[ 0.0796]],\n",
      "\n",
      "        [[ 0.2632]],\n",
      "\n",
      "        [[ 0.2883]],\n",
      "\n",
      "        [[ 0.1020]],\n",
      "\n",
      "        [[ 0.0619]],\n",
      "\n",
      "        [[ 0.2361]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.1698]],\n",
      "\n",
      "        [[ 0.2931]],\n",
      "\n",
      "        [[ 0.0457]],\n",
      "\n",
      "        [[ 0.2489]],\n",
      "\n",
      "        [[ 0.1103]],\n",
      "\n",
      "        [[ 0.1467]],\n",
      "\n",
      "        [[ 0.0887]],\n",
      "\n",
      "        [[ 0.0342]],\n",
      "\n",
      "        [[ 0.0013]],\n",
      "\n",
      "        [[ 0.2652]],\n",
      "\n",
      "        [[ 0.0530]],\n",
      "\n",
      "        [[ 0.0388]],\n",
      "\n",
      "        [[ 0.1805]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0241]],\n",
      "\n",
      "        [[ 0.1442]],\n",
      "\n",
      "        [[ 0.0945]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[[-2.4581]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[-1.3181e-01,  1.6300e-01, -5.6855e-02,  ..., -4.7393e-02,\n",
      "          -1.2897e-01, -1.3582e-01]],\n",
      "\n",
      "        [[ 9.3068e-02, -1.5030e-01, -9.3082e-02,  ..., -1.3275e-01,\n",
      "           1.4846e-01,  9.3643e-02]],\n",
      "\n",
      "        [[ 1.1750e-01, -1.6402e-01, -8.9085e-02,  ..., -8.9091e-03,\n",
      "           7.1889e-02, -6.7907e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.9461e-02,  2.4554e-02, -5.9215e-02,  ..., -7.4328e-02,\n",
      "           3.6444e-02,  1.6317e-01]],\n",
      "\n",
      "        [[-3.8235e-02,  8.0904e-02,  1.2024e-01,  ...,  1.4822e-01,\n",
      "          -1.2999e-02, -5.9017e-02]],\n",
      "\n",
      "        [[-9.6453e-02,  3.9129e-02, -9.2594e-03,  ..., -2.5017e-02,\n",
      "          -1.0057e-01, -7.6844e-02]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor(1.00000e-02 *\n",
      "       [[[-1.1462, -1.4066, -1.6351,  ..., -0.4328, -0.2488, -0.0992],\n",
      "         [-0.2521, -0.0702, -0.4553,  ..., -1.6658, -1.1247, -1.0711],\n",
      "         [ 0.0613, -0.1816, -0.5533,  ..., -0.7531, -0.7202, -0.1765],\n",
      "         ...,\n",
      "         [ 0.9670,  1.0523,  0.8147,  ...,  0.3464,  0.6447,  0.6647],\n",
      "         [ 0.7322,  0.4400,  0.9306,  ...,  1.3926,  0.7898,  0.7909],\n",
      "         [-0.8215, -0.9790, -1.0155,  ..., -0.5294, -0.4270, -0.5973]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [23040/50000 (51%)]\tLoss: 0.474906, Accuracy: 84.96\n",
      "Train Epoch: 15 [25600/50000 (57%)]\tLoss: 0.505667, Accuracy: 82.23\n",
      "Train Epoch: 15 [28160/50000 (62%)]\tLoss: 0.432501, Accuracy: 84.96\n",
      "Train Epoch: 15 [30720/50000 (68%)]\tLoss: 0.521083, Accuracy: 81.84\n",
      "Train Epoch: 15 [33280/50000 (74%)]\tLoss: 0.475459, Accuracy: 83.40\n",
      "Train Epoch: 15 [35840/50000 (80%)]\tLoss: 0.407460, Accuracy: 85.94\n",
      "Train Epoch: 15 [38400/50000 (85%)]\tLoss: 0.442435, Accuracy: 83.79\n",
      "Train Epoch: 15 [40960/50000 (91%)]\tLoss: 0.438196, Accuracy: 84.57\n",
      "Train Epoch: 15 [43520/50000 (97%)]\tLoss: 0.368263, Accuracy: 87.11\n",
      "\n",
      "Validation set: Average loss: 0.8317, Accuracy: 3679/5000 (73.00%)\n",
      "\n",
      "the time of this epoch:[39.467621088027954 s]\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.354518, Accuracy: 88.87\n",
      "Train Epoch: 16 [2560/50000 (6%)]\tLoss: 0.425779, Accuracy: 84.77\n",
      "Train Epoch: 16 [5120/50000 (11%)]\tLoss: 0.410249, Accuracy: 86.13\n",
      "Train Epoch: 16 [7680/50000 (17%)]\tLoss: 0.432018, Accuracy: 84.38\n",
      "Train Epoch: 16 [10240/50000 (23%)]\tLoss: 0.375992, Accuracy: 86.33\n",
      "Train Epoch: 16 [12800/50000 (28%)]\tLoss: 0.393764, Accuracy: 85.55\n",
      "Train Epoch: 16 [15360/50000 (34%)]\tLoss: 0.471359, Accuracy: 82.03\n",
      "Train Epoch: 16 [17920/50000 (40%)]\tLoss: 0.412778, Accuracy: 86.33\n",
      "Train Epoch: 16 [20480/50000 (45%)]\tLoss: 0.461274, Accuracy: 83.01\n",
      "Train Epoch: 16 [23040/50000 (51%)]\tLoss: 0.478192, Accuracy: 83.20\n",
      "Train Epoch: 16 [25600/50000 (57%)]\tLoss: 0.437795, Accuracy: 84.57\n",
      "Train Epoch: 16 [28160/50000 (62%)]\tLoss: 0.404437, Accuracy: 86.13\n",
      "Train Epoch: 16 [30720/50000 (68%)]\tLoss: 0.395733, Accuracy: 86.91\n",
      "Train Epoch: 16 [33280/50000 (74%)]\tLoss: 0.386183, Accuracy: 86.72\n",
      "Train Epoch: 16 [35840/50000 (80%)]\tLoss: 0.373085, Accuracy: 87.70\n",
      "Train Epoch: 16 [38400/50000 (85%)]\tLoss: 0.401455, Accuracy: 86.52\n",
      "Train Epoch: 16 [40960/50000 (91%)]\tLoss: 0.432045, Accuracy: 84.57\n",
      "Train Epoch: 16 [43520/50000 (97%)]\tLoss: 0.481716, Accuracy: 83.20\n",
      "\n",
      "Validation set: Average loss: 0.7285, Accuracy: 3828/5000 (76.00%)\n",
      "\n",
      "the time of this epoch:[57.15758419036865 s]\n",
      "\n",
      "Test set: Average loss: 0.7581, Accuracy: 7613/10000 (76.13%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.381856, Accuracy: 87.30\n",
      "Train Epoch: 17 [2560/50000 (6%)]\tLoss: 0.432856, Accuracy: 85.74\n",
      "Train Epoch: 17 [5120/50000 (11%)]\tLoss: 0.397650, Accuracy: 87.30\n",
      "Train Epoch: 17 [7680/50000 (17%)]\tLoss: 0.382365, Accuracy: 86.13\n",
      "Train Epoch: 17 [10240/50000 (23%)]\tLoss: 0.462799, Accuracy: 84.57\n",
      "Train Epoch: 17 [12800/50000 (28%)]\tLoss: 0.358963, Accuracy: 87.50\n",
      "Train Epoch: 17 [15360/50000 (34%)]\tLoss: 0.382584, Accuracy: 88.28\n",
      "Train Epoch: 17 [17920/50000 (40%)]\tLoss: 0.429010, Accuracy: 85.16\n",
      "Train Epoch: 17 [20480/50000 (45%)]\tLoss: 0.445627, Accuracy: 85.94\n",
      "Train Epoch: 17 [23040/50000 (51%)]\tLoss: 0.372280, Accuracy: 88.09\n",
      "Train Epoch: 17 [25600/50000 (57%)]\tLoss: 0.385964, Accuracy: 85.74\n",
      "Train Epoch: 17 [28160/50000 (62%)]\tLoss: 0.415957, Accuracy: 86.33\n",
      "Train Epoch: 17 [30720/50000 (68%)]\tLoss: 0.445751, Accuracy: 84.57\n",
      "Train Epoch: 17 [33280/50000 (74%)]\tLoss: 0.362677, Accuracy: 86.72\n",
      "Train Epoch: 17 [35840/50000 (80%)]\tLoss: 0.416303, Accuracy: 85.16\n",
      "Train Epoch: 17 [38400/50000 (85%)]\tLoss: 0.410720, Accuracy: 86.91\n",
      "Train Epoch: 17 [40960/50000 (91%)]\tLoss: 0.401785, Accuracy: 86.33\n",
      "Train Epoch: 17 [43520/50000 (97%)]\tLoss: 0.504925, Accuracy: 84.38\n",
      "\n",
      "Validation set: Average loss: 0.7865, Accuracy: 3794/5000 (75.00%)\n",
      "\n",
      "the time of this epoch:[39.46433997154236 s]\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.494544, Accuracy: 81.05\n",
      "Train Epoch: 18 [2560/50000 (6%)]\tLoss: 0.387054, Accuracy: 88.48\n",
      "Train Epoch: 18 [5120/50000 (11%)]\tLoss: 0.395418, Accuracy: 85.74\n",
      "Train Epoch: 18 [7680/50000 (17%)]\tLoss: 0.396644, Accuracy: 85.16\n",
      "Train Epoch: 18 [10240/50000 (23%)]\tLoss: 0.314275, Accuracy: 88.87\n",
      "Train Epoch: 18 [12800/50000 (28%)]\tLoss: 0.357946, Accuracy: 87.70\n",
      "Train Epoch: 18 [15360/50000 (34%)]\tLoss: 0.350507, Accuracy: 85.74\n",
      "Train Epoch: 18 [17920/50000 (40%)]\tLoss: 0.380278, Accuracy: 86.52\n",
      "Train Epoch: 18 [20480/50000 (45%)]\tLoss: 0.410566, Accuracy: 85.16\n",
      "Train Epoch: 18 [23040/50000 (51%)]\tLoss: 0.467550, Accuracy: 85.94\n",
      "Train Epoch: 18 [25600/50000 (57%)]\tLoss: 0.379151, Accuracy: 87.70\n",
      "Train Epoch: 18 [28160/50000 (62%)]\tLoss: 0.451526, Accuracy: 85.16\n",
      "Train Epoch: 18 [30720/50000 (68%)]\tLoss: 0.404434, Accuracy: 86.72\n",
      "Train Epoch: 18 [33280/50000 (74%)]\tLoss: 0.462491, Accuracy: 85.35\n",
      "Train Epoch: 18 [35840/50000 (80%)]\tLoss: 0.380164, Accuracy: 88.09\n",
      "Train Epoch: 18 [38400/50000 (85%)]\tLoss: 0.444879, Accuracy: 86.33\n",
      "Train Epoch: 18 [40960/50000 (91%)]\tLoss: 0.474881, Accuracy: 84.57\n",
      "Train Epoch: 18 [43520/50000 (97%)]\tLoss: 0.467807, Accuracy: 83.79\n",
      "\n",
      "Validation set: Average loss: 0.5671, Accuracy: 4037/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.599464893341064 s]\n",
      "\n",
      "Test set: Average loss: 0.5818, Accuracy: 8081/10000 (80.81%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.370976, Accuracy: 87.11\n",
      "Train Epoch: 19 [2560/50000 (6%)]\tLoss: 0.418748, Accuracy: 86.52\n",
      "Train Epoch: 19 [5120/50000 (11%)]\tLoss: 0.369785, Accuracy: 87.89\n",
      "Train Epoch: 19 [7680/50000 (17%)]\tLoss: 0.402772, Accuracy: 85.74\n",
      "Train Epoch: 19 [10240/50000 (23%)]\tLoss: 0.332007, Accuracy: 86.13\n",
      "Train Epoch: 19 [12800/50000 (28%)]\tLoss: 0.411282, Accuracy: 86.72\n",
      "Train Epoch: 19 [15360/50000 (34%)]\tLoss: 0.445762, Accuracy: 84.77\n",
      "Train Epoch: 19 [17920/50000 (40%)]\tLoss: 0.356499, Accuracy: 87.30\n",
      "Train Epoch: 19 [20480/50000 (45%)]\tLoss: 0.399301, Accuracy: 86.13\n",
      "Train Epoch: 19 [23040/50000 (51%)]\tLoss: 0.449608, Accuracy: 85.35\n",
      "Train Epoch: 19 [25600/50000 (57%)]\tLoss: 0.372763, Accuracy: 88.28\n",
      "Train Epoch: 19 [28160/50000 (62%)]\tLoss: 0.450027, Accuracy: 84.96\n",
      "Train Epoch: 19 [30720/50000 (68%)]\tLoss: 0.440985, Accuracy: 85.55\n",
      "Train Epoch: 19 [33280/50000 (74%)]\tLoss: 0.352239, Accuracy: 86.91\n",
      "Train Epoch: 19 [35840/50000 (80%)]\tLoss: 0.367715, Accuracy: 88.09\n",
      "Train Epoch: 19 [38400/50000 (85%)]\tLoss: 0.330650, Accuracy: 87.30\n",
      "Train Epoch: 19 [40960/50000 (91%)]\tLoss: 0.460519, Accuracy: 84.38\n",
      "Train Epoch: 19 [43520/50000 (97%)]\tLoss: 0.405419, Accuracy: 85.55\n",
      "\n",
      "Validation set: Average loss: 0.8904, Accuracy: 3611/5000 (72.00%)\n",
      "\n",
      "the time of this epoch:[39.5287299156189 s]\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.424161, Accuracy: 85.74\n",
      "Train Epoch: 20 [2560/50000 (6%)]\tLoss: 0.389289, Accuracy: 86.91\n",
      "Train Epoch: 20 [5120/50000 (11%)]\tLoss: 0.404486, Accuracy: 88.28\n",
      "Train Epoch: 20 [7680/50000 (17%)]\tLoss: 0.366008, Accuracy: 86.91\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.2413]],\n",
      "\n",
      "        [[ 0.0074]],\n",
      "\n",
      "        [[ 0.0577]],\n",
      "\n",
      "        [[ 0.0431]],\n",
      "\n",
      "        [[ 0.0436]],\n",
      "\n",
      "        [[ 0.0142]],\n",
      "\n",
      "        [[ 0.0451]],\n",
      "\n",
      "        [[ 0.5212]],\n",
      "\n",
      "        [[ 0.1007]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.1783]],\n",
      "\n",
      "        [[ 0.2048]],\n",
      "\n",
      "        [[ 0.2746]],\n",
      "\n",
      "        [[ 0.0744]],\n",
      "\n",
      "        [[ 0.0623]],\n",
      "\n",
      "        [[ 0.0714]],\n",
      "\n",
      "        [[ 0.0410]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.1049]],\n",
      "\n",
      "        [[ 0.0081]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0892]],\n",
      "\n",
      "        [[ 0.0960]],\n",
      "\n",
      "        [[ 0.1609]],\n",
      "\n",
      "        [[ 0.0013]],\n",
      "\n",
      "        [[ 0.0102]],\n",
      "\n",
      "        [[ 0.0457]],\n",
      "\n",
      "        [[ 0.1344]],\n",
      "\n",
      "        [[ 0.0097]],\n",
      "\n",
      "        [[ 0.0685]],\n",
      "\n",
      "        [[ 0.0775]],\n",
      "\n",
      "        [[ 0.0951]],\n",
      "\n",
      "        [[ 0.0710]],\n",
      "\n",
      "        [[ 0.0561]],\n",
      "\n",
      "        [[ 0.0400]],\n",
      "\n",
      "        [[ 0.0251]],\n",
      "\n",
      "        [[ 0.0856]],\n",
      "\n",
      "        [[ 0.2183]],\n",
      "\n",
      "        [[ 0.0874]],\n",
      "\n",
      "        [[ 0.2531]],\n",
      "\n",
      "        [[ 0.0604]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.0171]],\n",
      "\n",
      "        [[ 0.0065]],\n",
      "\n",
      "        [[ 0.1793]],\n",
      "\n",
      "        [[ 0.0130]],\n",
      "\n",
      "        [[ 0.1094]],\n",
      "\n",
      "        [[ 0.6978]],\n",
      "\n",
      "        [[ 0.1572]],\n",
      "\n",
      "        [[ 0.1319]],\n",
      "\n",
      "        [[ 0.0715]],\n",
      "\n",
      "        [[ 0.1040]],\n",
      "\n",
      "        [[ 0.1125]],\n",
      "\n",
      "        [[ 0.0211]],\n",
      "\n",
      "        [[ 0.0952]],\n",
      "\n",
      "        [[ 0.3542]],\n",
      "\n",
      "        [[ 0.6909]],\n",
      "\n",
      "        [[ 0.7629]],\n",
      "\n",
      "        [[ 0.0478]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0158]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0574]],\n",
      "\n",
      "        [[ 0.0275]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2605]],\n",
      "\n",
      "        [[ 0.2590]],\n",
      "\n",
      "        [[ 0.0231]],\n",
      "\n",
      "        [[ 0.2658]],\n",
      "\n",
      "        [[ 0.0877]],\n",
      "\n",
      "        [[ 0.0493]],\n",
      "\n",
      "        [[ 0.1246]],\n",
      "\n",
      "        [[ 0.0727]],\n",
      "\n",
      "        [[ 0.0446]],\n",
      "\n",
      "        [[ 0.1688]],\n",
      "\n",
      "        [[ 0.0741]],\n",
      "\n",
      "        [[ 0.0512]],\n",
      "\n",
      "        [[ 0.0771]],\n",
      "\n",
      "        [[ 0.0603]],\n",
      "\n",
      "        [[ 0.0957]],\n",
      "\n",
      "        [[ 0.0940]],\n",
      "\n",
      "        [[ 0.0808]],\n",
      "\n",
      "        [[ 0.0529]],\n",
      "\n",
      "        [[ 0.1716]],\n",
      "\n",
      "        [[ 0.2014]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.2172]],\n",
      "\n",
      "        [[ 0.1396]],\n",
      "\n",
      "        [[ 0.0557]],\n",
      "\n",
      "        [[ 0.1730]],\n",
      "\n",
      "        [[ 0.1031]],\n",
      "\n",
      "        [[ 0.1019]],\n",
      "\n",
      "        [[ 0.0808]],\n",
      "\n",
      "        [[ 0.2338]],\n",
      "\n",
      "        [[ 0.2177]],\n",
      "\n",
      "        [[ 0.0565]],\n",
      "\n",
      "        [[ 0.1131]],\n",
      "\n",
      "        [[ 0.1920]],\n",
      "\n",
      "        [[ 0.1531]],\n",
      "\n",
      "        [[ 0.2150]],\n",
      "\n",
      "        [[ 0.3789]],\n",
      "\n",
      "        [[ 0.2330]],\n",
      "\n",
      "        [[ 0.1305]],\n",
      "\n",
      "        [[ 0.0778]],\n",
      "\n",
      "        [[ 0.0641]],\n",
      "\n",
      "        [[ 0.1506]],\n",
      "\n",
      "        [[ 0.2298]],\n",
      "\n",
      "        [[ 0.0620]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.1845]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.3102]],\n",
      "\n",
      "        [[ 0.0924]],\n",
      "\n",
      "        [[ 0.2799]],\n",
      "\n",
      "        [[ 0.0668]],\n",
      "\n",
      "        [[ 0.1516]],\n",
      "\n",
      "        [[ 0.0834]],\n",
      "\n",
      "        [[ 0.0149]],\n",
      "\n",
      "        [[ 0.0465]],\n",
      "\n",
      "        [[ 0.0331]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0614]],\n",
      "\n",
      "        [[ 0.0609]],\n",
      "\n",
      "        [[ 0.2158]],\n",
      "\n",
      "        [[ 0.0320]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0487]],\n",
      "\n",
      "        [[ 0.0529]],\n",
      "\n",
      "        [[ 0.0778]],\n",
      "\n",
      "        [[ 0.0765]],\n",
      "\n",
      "        [[ 0.0685]],\n",
      "\n",
      "        [[ 0.3133]],\n",
      "\n",
      "        [[ 0.0310]],\n",
      "\n",
      "        [[ 0.2365]],\n",
      "\n",
      "        [[ 0.0733]],\n",
      "\n",
      "        [[ 0.0871]],\n",
      "\n",
      "        [[ 0.0827]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.0489]],\n",
      "\n",
      "        [[ 0.0356]],\n",
      "\n",
      "        [[ 0.1052]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.2965]],\n",
      "\n",
      "        [[ 0.3502]],\n",
      "\n",
      "        [[ 0.2469]],\n",
      "\n",
      "        [[ 0.2287]],\n",
      "\n",
      "        [[ 0.0713]],\n",
      "\n",
      "        [[ 0.0286]],\n",
      "\n",
      "        [[ 0.0453]],\n",
      "\n",
      "        [[ 0.1240]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0622]],\n",
      "\n",
      "        [[ 0.3012]],\n",
      "\n",
      "        [[ 0.0345]],\n",
      "\n",
      "        [[ 0.0457]],\n",
      "\n",
      "        [[ 0.3014]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.0591]],\n",
      "\n",
      "        [[ 0.0902]],\n",
      "\n",
      "        [[ 0.2779]],\n",
      "\n",
      "        [[ 0.2018]],\n",
      "\n",
      "        [[ 0.1513]],\n",
      "\n",
      "        [[ 0.0572]],\n",
      "\n",
      "        [[ 0.0525]],\n",
      "\n",
      "        [[ 0.0639]],\n",
      "\n",
      "        [[ 0.0022]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0475]],\n",
      "\n",
      "        [[ 0.0424]],\n",
      "\n",
      "        [[ 0.0722]],\n",
      "\n",
      "        [[ 0.0856]],\n",
      "\n",
      "        [[ 0.0602]],\n",
      "\n",
      "        [[ 0.2067]],\n",
      "\n",
      "        [[ 0.3678]],\n",
      "\n",
      "        [[ 0.3487]],\n",
      "\n",
      "        [[ 0.1043]],\n",
      "\n",
      "        [[ 0.0629]],\n",
      "\n",
      "        [[ 0.2427]],\n",
      "\n",
      "        [[ 0.2854]],\n",
      "\n",
      "        [[ 0.1264]],\n",
      "\n",
      "        [[ 0.3522]],\n",
      "\n",
      "        [[ 0.0436]],\n",
      "\n",
      "        [[ 0.0567]],\n",
      "\n",
      "        [[ 0.0749]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.0321]],\n",
      "\n",
      "        [[ 0.2292]],\n",
      "\n",
      "        [[ 0.1910]],\n",
      "\n",
      "        [[ 0.0904]],\n",
      "\n",
      "        [[ 0.0355]],\n",
      "\n",
      "        [[ 0.2307]],\n",
      "\n",
      "        [[ 0.0902]],\n",
      "\n",
      "        [[ 0.0423]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.3530]],\n",
      "\n",
      "        [[ 0.0583]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0599]],\n",
      "\n",
      "        [[ 0.1249]],\n",
      "\n",
      "        [[ 0.3245]],\n",
      "\n",
      "        [[ 0.0539]],\n",
      "\n",
      "        [[ 0.0700]],\n",
      "\n",
      "        [[ 0.0721]],\n",
      "\n",
      "        [[ 0.0415]],\n",
      "\n",
      "        [[ 0.0697]],\n",
      "\n",
      "        [[ 0.1230]],\n",
      "\n",
      "        [[ 0.0744]],\n",
      "\n",
      "        [[ 0.0797]],\n",
      "\n",
      "        [[ 0.0819]],\n",
      "\n",
      "        [[ 0.0976]],\n",
      "\n",
      "        [[ 0.0816]],\n",
      "\n",
      "        [[ 0.2780]],\n",
      "\n",
      "        [[ 0.0198]],\n",
      "\n",
      "        [[ 0.2320]],\n",
      "\n",
      "        [[ 0.0865]],\n",
      "\n",
      "        [[ 0.2057]],\n",
      "\n",
      "        [[ 0.0829]],\n",
      "\n",
      "        [[ 0.2856]],\n",
      "\n",
      "        [[ 0.0180]],\n",
      "\n",
      "        [[ 0.0517]],\n",
      "\n",
      "        [[ 0.0838]],\n",
      "\n",
      "        [[ 0.0254]],\n",
      "\n",
      "        [[ 0.0023]],\n",
      "\n",
      "        [[ 0.0796]],\n",
      "\n",
      "        [[ 0.1167]],\n",
      "\n",
      "        [[ 0.1343]],\n",
      "\n",
      "        [[ 0.0558]],\n",
      "\n",
      "        [[ 0.0248]],\n",
      "\n",
      "        [[ 0.2563]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2134]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0321]],\n",
      "\n",
      "        [[ 0.0513]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.2062]],\n",
      "\n",
      "        [[ 0.0430]],\n",
      "\n",
      "        [[ 0.0438]],\n",
      "\n",
      "        [[ 0.0978]],\n",
      "\n",
      "        [[ 0.0814]],\n",
      "\n",
      "        [[ 0.1902]],\n",
      "\n",
      "        [[ 0.1607]],\n",
      "\n",
      "        [[ 0.0499]],\n",
      "\n",
      "        [[ 0.3522]],\n",
      "\n",
      "        [[ 0.0066]],\n",
      "\n",
      "        [[ 0.1826]],\n",
      "\n",
      "        [[ 0.2696]],\n",
      "\n",
      "        [[ 0.2438]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.3095]],\n",
      "\n",
      "        [[ 0.0561]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.0459]],\n",
      "\n",
      "        [[ 0.0421]],\n",
      "\n",
      "        [[ 0.3350]],\n",
      "\n",
      "        [[ 0.0484]],\n",
      "\n",
      "        [[ 0.0687]],\n",
      "\n",
      "        [[ 0.0502]],\n",
      "\n",
      "        [[ 0.0596]],\n",
      "\n",
      "        [[ 0.0401]],\n",
      "\n",
      "        [[ 0.2753]],\n",
      "\n",
      "        [[ 0.2012]],\n",
      "\n",
      "        [[ 0.2333]],\n",
      "\n",
      "        [[ 0.1554]],\n",
      "\n",
      "        [[ 0.1189]],\n",
      "\n",
      "        [[ 0.0602]],\n",
      "\n",
      "        [[ 0.0334]],\n",
      "\n",
      "        [[ 0.0454]],\n",
      "\n",
      "        [[ 0.0798]],\n",
      "\n",
      "        [[ 0.0621]],\n",
      "\n",
      "        [[ 0.0745]],\n",
      "\n",
      "        [[ 0.0961]],\n",
      "\n",
      "        [[ 0.1716]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.2972]],\n",
      "\n",
      "        [[ 0.0302]],\n",
      "\n",
      "        [[ 0.0546]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0364]],\n",
      "\n",
      "        [[ 0.0827]],\n",
      "\n",
      "        [[ 0.3005]],\n",
      "\n",
      "        [[ 0.1877]],\n",
      "\n",
      "        [[ 0.0790]],\n",
      "\n",
      "        [[ 0.0739]],\n",
      "\n",
      "        [[ 0.1706]],\n",
      "\n",
      "        [[ 0.0502]],\n",
      "\n",
      "        [[ 0.1851]],\n",
      "\n",
      "        [[ 0.3748]],\n",
      "\n",
      "        [[ 0.0709]],\n",
      "\n",
      "        [[ 0.0073]],\n",
      "\n",
      "        [[ 0.2650]],\n",
      "\n",
      "        [[ 0.1615]],\n",
      "\n",
      "        [[ 0.0590]],\n",
      "\n",
      "        [[ 0.1623]],\n",
      "\n",
      "        [[ 0.1894]],\n",
      "\n",
      "        [[ 0.1837]],\n",
      "\n",
      "        [[ 0.3404]],\n",
      "\n",
      "        [[ 0.3584]],\n",
      "\n",
      "        [[ 0.2143]],\n",
      "\n",
      "        [[ 0.0581]],\n",
      "\n",
      "        [[ 0.0163]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0287]],\n",
      "\n",
      "        [[ 0.1554]],\n",
      "\n",
      "        [[ 0.0706]],\n",
      "\n",
      "        [[ 0.2694]],\n",
      "\n",
      "        [[ 0.0427]],\n",
      "\n",
      "        [[ 0.0606]],\n",
      "\n",
      "        [[ 0.1298]],\n",
      "\n",
      "        [[ 0.0419]],\n",
      "\n",
      "        [[ 0.1984]],\n",
      "\n",
      "        [[ 0.0707]],\n",
      "\n",
      "        [[ 0.2449]],\n",
      "\n",
      "        [[ 0.3324]],\n",
      "\n",
      "        [[ 0.3139]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0324]],\n",
      "\n",
      "        [[ 0.2840]],\n",
      "\n",
      "        [[ 0.0502]],\n",
      "\n",
      "        [[ 0.0400]],\n",
      "\n",
      "        [[ 0.0582]],\n",
      "\n",
      "        [[ 0.0571]],\n",
      "\n",
      "        [[ 0.0711]],\n",
      "\n",
      "        [[ 0.1904]],\n",
      "\n",
      "        [[ 0.2365]],\n",
      "\n",
      "        [[ 0.0512]],\n",
      "\n",
      "        [[ 0.1620]],\n",
      "\n",
      "        [[ 0.2262]],\n",
      "\n",
      "        [[ 0.3488]],\n",
      "\n",
      "        [[ 0.0903]],\n",
      "\n",
      "        [[ 0.0714]],\n",
      "\n",
      "        [[ 0.0537]],\n",
      "\n",
      "        [[ 0.1926]],\n",
      "\n",
      "        [[ 0.2142]],\n",
      "\n",
      "        [[ 0.0948]],\n",
      "\n",
      "        [[ 0.0758]],\n",
      "\n",
      "        [[ 0.0318]],\n",
      "\n",
      "        [[ 0.0375]],\n",
      "\n",
      "        [[ 0.1496]],\n",
      "\n",
      "        [[ 0.1106]],\n",
      "\n",
      "        [[ 0.2741]],\n",
      "\n",
      "        [[ 0.3275]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2939]],\n",
      "\n",
      "        [[ 0.3563]],\n",
      "\n",
      "        [[ 0.1997]],\n",
      "\n",
      "        [[ 0.2118]],\n",
      "\n",
      "        [[ 0.0422]],\n",
      "\n",
      "        [[ 0.3738]],\n",
      "\n",
      "        [[ 0.0787]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2904]],\n",
      "\n",
      "        [[ 0.1509]],\n",
      "\n",
      "        [[ 0.3640]],\n",
      "\n",
      "        [[ 0.0235]],\n",
      "\n",
      "        [[ 0.2388]],\n",
      "\n",
      "        [[ 0.0788]],\n",
      "\n",
      "        [[ 0.0423]],\n",
      "\n",
      "        [[ 0.0740]],\n",
      "\n",
      "        [[ 0.0405]],\n",
      "\n",
      "        [[ 0.3406]],\n",
      "\n",
      "        [[ 0.0610]],\n",
      "\n",
      "        [[ 0.0384]],\n",
      "\n",
      "        [[ 0.0897]],\n",
      "\n",
      "        [[ 0.0653]],\n",
      "\n",
      "        [[ 0.3339]],\n",
      "\n",
      "        [[ 0.2924]],\n",
      "\n",
      "        [[ 0.1000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1516]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.1859]],\n",
      "\n",
      "        [[ 0.2069]],\n",
      "\n",
      "        [[ 0.0084]],\n",
      "\n",
      "        [[ 0.0873]],\n",
      "\n",
      "        [[ 0.0250]],\n",
      "\n",
      "        [[ 0.0858]],\n",
      "\n",
      "        [[ 0.0615]],\n",
      "\n",
      "        [[ 0.0022]],\n",
      "\n",
      "        [[ 0.0730]],\n",
      "\n",
      "        [[ 0.2853]],\n",
      "\n",
      "        [[ 0.0525]],\n",
      "\n",
      "        [[ 0.0417]],\n",
      "\n",
      "        [[ 0.3440]],\n",
      "\n",
      "        [[ 0.0700]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0476]],\n",
      "\n",
      "        [[ 0.0375]],\n",
      "\n",
      "        [[ 0.3731]],\n",
      "\n",
      "        [[ 0.3143]],\n",
      "\n",
      "        [[ 0.3471]],\n",
      "\n",
      "        [[ 0.0338]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0642]],\n",
      "\n",
      "        [[ 0.0347]],\n",
      "\n",
      "        [[ 0.0583]],\n",
      "\n",
      "        [[ 0.1922]],\n",
      "\n",
      "        [[ 0.2275]],\n",
      "\n",
      "        [[ 0.0371]],\n",
      "\n",
      "        [[ 0.2344]],\n",
      "\n",
      "        [[ 0.0499]],\n",
      "\n",
      "        [[ 0.0552]],\n",
      "\n",
      "        [[ 0.0854]],\n",
      "\n",
      "        [[ 0.0243]],\n",
      "\n",
      "        [[ 0.0365]],\n",
      "\n",
      "        [[ 0.1918]],\n",
      "\n",
      "        [[ 0.0725]],\n",
      "\n",
      "        [[ 0.0377]],\n",
      "\n",
      "        [[ 0.3570]],\n",
      "\n",
      "        [[ 0.0558]],\n",
      "\n",
      "        [[ 0.0816]],\n",
      "\n",
      "        [[ 0.2547]],\n",
      "\n",
      "        [[ 0.1715]],\n",
      "\n",
      "        [[ 0.0400]],\n",
      "\n",
      "        [[ 0.0918]],\n",
      "\n",
      "        [[ 0.0231]],\n",
      "\n",
      "        [[ 0.2941]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.1380]],\n",
      "\n",
      "        [[ 0.1106]],\n",
      "\n",
      "        [[ 0.0718]],\n",
      "\n",
      "        [[ 0.0857]],\n",
      "\n",
      "        [[ 0.0543]],\n",
      "\n",
      "        [[ 0.2508]],\n",
      "\n",
      "        [[ 0.0132]],\n",
      "\n",
      "        [[ 0.1593]],\n",
      "\n",
      "        [[ 0.0588]],\n",
      "\n",
      "        [[ 0.3758]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0408]],\n",
      "\n",
      "        [[ 0.1421]],\n",
      "\n",
      "        [[ 0.0587]],\n",
      "\n",
      "        [[ 0.0711]],\n",
      "\n",
      "        [[ 0.0737]],\n",
      "\n",
      "        [[ 0.0497]],\n",
      "\n",
      "        [[ 0.0817]],\n",
      "\n",
      "        [[ 0.0852]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0797]],\n",
      "\n",
      "        [[ 0.0623]],\n",
      "\n",
      "        [[ 0.0888]],\n",
      "\n",
      "        [[ 0.0498]],\n",
      "\n",
      "        [[ 0.2106]],\n",
      "\n",
      "        [[ 0.3590]],\n",
      "\n",
      "        [[ 0.2020]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0518]],\n",
      "\n",
      "        [[ 0.0405]],\n",
      "\n",
      "        [[ 0.0746]],\n",
      "\n",
      "        [[ 0.0123]],\n",
      "\n",
      "        [[ 0.2558]],\n",
      "\n",
      "        [[ 0.0873]],\n",
      "\n",
      "        [[ 0.0144]],\n",
      "\n",
      "        [[ 0.0610]],\n",
      "\n",
      "        [[ 0.0322]],\n",
      "\n",
      "        [[ 0.0909]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0705]],\n",
      "\n",
      "        [[ 0.1038]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0905]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0371]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0495]],\n",
      "\n",
      "        [[ 0.0715]],\n",
      "\n",
      "        [[ 0.0753]],\n",
      "\n",
      "        [[ 0.1466]],\n",
      "\n",
      "        [[ 0.3403]],\n",
      "\n",
      "        [[ 0.2804]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0499]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.0773]],\n",
      "\n",
      "        [[ 0.0339]],\n",
      "\n",
      "        [[ 0.0260]],\n",
      "\n",
      "        [[ 0.1380]],\n",
      "\n",
      "        [[ 0.1870]],\n",
      "\n",
      "        [[ 0.0616]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0275]],\n",
      "\n",
      "        [[ 0.0456]],\n",
      "\n",
      "        [[ 0.0743]],\n",
      "\n",
      "        [[ 0.2140]],\n",
      "\n",
      "        [[ 0.3628]],\n",
      "\n",
      "        [[ 0.0405]],\n",
      "\n",
      "        [[ 0.0757]],\n",
      "\n",
      "        [[ 0.0544]],\n",
      "\n",
      "        [[ 0.1866]],\n",
      "\n",
      "        [[ 0.1314]],\n",
      "\n",
      "        [[ 0.2187]],\n",
      "\n",
      "        [[ 0.1632]],\n",
      "\n",
      "        [[ 0.0872]],\n",
      "\n",
      "        [[ 0.0619]],\n",
      "\n",
      "        [[ 0.0867]],\n",
      "\n",
      "        [[ 0.0350]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0756]],\n",
      "\n",
      "        [[ 0.2857]],\n",
      "\n",
      "        [[ 0.0257]],\n",
      "\n",
      "        [[ 0.0522]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0560]],\n",
      "\n",
      "        [[ 0.0881]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1534]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0294,  0.0190,  0.1018,  ...,  0.1071, -0.1047,  0.0421]],\n",
      "\n",
      "        [[ 0.0943, -0.0616, -0.0588,  ..., -0.0188, -0.1067,  0.0557]],\n",
      "\n",
      "        [[ 0.0106,  0.1118, -0.0635,  ...,  0.0947,  0.0077,  0.1102]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0030, -0.0590,  0.0220,  ..., -0.0172, -0.1246,  0.0980]],\n",
      "\n",
      "        [[ 0.0696,  0.0518,  0.0792,  ...,  0.0157,  0.0753,  0.0482]],\n",
      "\n",
      "        [[-0.1016,  0.0786,  0.0776,  ...,  0.0235, -0.0200,  0.1350]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 0.4235,  0.9951,  1.4335,  ...,  1.8835,  1.4214,  1.6561],\n",
      "         [-0.1190, -0.2460, -0.4989,  ..., -0.1480, -0.0054,  0.1968],\n",
      "         [ 0.9521,  0.4696,  0.2215,  ...,  0.2921, -0.1616,  0.3611],\n",
      "         ...,\n",
      "         [-0.4178, -0.2374, -0.1363,  ..., -0.3275, -0.2332,  0.0709],\n",
      "         [ 1.0803,  0.8690,  1.2571,  ...,  1.2359,  1.1840,  1.2384],\n",
      "         [ 0.8328,  0.9852,  1.3327,  ...,  0.7454,  0.5786,  0.0065]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [10240/50000 (23%)]\tLoss: 0.412360, Accuracy: 84.96\n",
      "Train Epoch: 20 [12800/50000 (28%)]\tLoss: 0.396519, Accuracy: 87.70\n",
      "Train Epoch: 20 [15360/50000 (34%)]\tLoss: 0.363767, Accuracy: 88.28\n",
      "Train Epoch: 20 [17920/50000 (40%)]\tLoss: 0.381658, Accuracy: 85.55\n",
      "Train Epoch: 20 [20480/50000 (45%)]\tLoss: 0.295400, Accuracy: 88.67\n",
      "Train Epoch: 20 [23040/50000 (51%)]\tLoss: 0.388103, Accuracy: 86.13\n",
      "Train Epoch: 20 [25600/50000 (57%)]\tLoss: 0.353903, Accuracy: 88.67\n",
      "Train Epoch: 20 [28160/50000 (62%)]\tLoss: 0.418056, Accuracy: 85.16\n",
      "Train Epoch: 20 [30720/50000 (68%)]\tLoss: 0.389657, Accuracy: 85.55\n",
      "Train Epoch: 20 [33280/50000 (74%)]\tLoss: 0.373299, Accuracy: 87.30\n",
      "Train Epoch: 20 [35840/50000 (80%)]\tLoss: 0.381924, Accuracy: 86.91\n",
      "Train Epoch: 20 [38400/50000 (85%)]\tLoss: 0.369697, Accuracy: 87.50\n",
      "Train Epoch: 20 [40960/50000 (91%)]\tLoss: 0.451274, Accuracy: 83.20\n",
      "Train Epoch: 20 [43520/50000 (97%)]\tLoss: 0.381285, Accuracy: 87.70\n",
      "\n",
      "Validation set: Average loss: 0.5701, Accuracy: 4144/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[36.44502019882202 s]\n",
      "\n",
      "Test set: Average loss: 0.5926, Accuracy: 8117/10000 (81.17%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.355472, Accuracy: 88.28\n",
      "Train Epoch: 21 [2560/50000 (6%)]\tLoss: 0.358132, Accuracy: 86.91\n",
      "Train Epoch: 21 [5120/50000 (11%)]\tLoss: 0.377459, Accuracy: 87.30\n",
      "Train Epoch: 21 [7680/50000 (17%)]\tLoss: 0.351360, Accuracy: 89.26\n",
      "Train Epoch: 21 [10240/50000 (23%)]\tLoss: 0.353819, Accuracy: 87.89\n",
      "Train Epoch: 21 [12800/50000 (28%)]\tLoss: 0.326117, Accuracy: 88.28\n",
      "Train Epoch: 21 [15360/50000 (34%)]\tLoss: 0.340886, Accuracy: 88.28\n",
      "Train Epoch: 21 [17920/50000 (40%)]\tLoss: 0.376808, Accuracy: 87.50\n",
      "Train Epoch: 21 [20480/50000 (45%)]\tLoss: 0.388037, Accuracy: 86.72\n",
      "Train Epoch: 21 [23040/50000 (51%)]\tLoss: 0.371971, Accuracy: 87.30\n",
      "Train Epoch: 21 [25600/50000 (57%)]\tLoss: 0.327847, Accuracy: 88.87\n",
      "Train Epoch: 21 [28160/50000 (62%)]\tLoss: 0.347785, Accuracy: 88.09\n",
      "Train Epoch: 21 [30720/50000 (68%)]\tLoss: 0.428646, Accuracy: 85.94\n",
      "Train Epoch: 21 [33280/50000 (74%)]\tLoss: 0.375373, Accuracy: 85.94\n",
      "Train Epoch: 21 [35840/50000 (80%)]\tLoss: 0.363630, Accuracy: 88.09\n",
      "Train Epoch: 21 [38400/50000 (85%)]\tLoss: 0.395874, Accuracy: 85.55\n",
      "Train Epoch: 21 [40960/50000 (91%)]\tLoss: 0.437279, Accuracy: 83.98\n",
      "Train Epoch: 21 [43520/50000 (97%)]\tLoss: 0.417672, Accuracy: 85.74\n",
      "\n",
      "Validation set: Average loss: 0.6933, Accuracy: 3946/5000 (78.00%)\n",
      "\n",
      "the time of this epoch:[39.5306830406189 s]\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.325449, Accuracy: 89.45\n",
      "Train Epoch: 22 [2560/50000 (6%)]\tLoss: 0.363567, Accuracy: 88.28\n",
      "Train Epoch: 22 [5120/50000 (11%)]\tLoss: 0.303869, Accuracy: 91.02\n",
      "Train Epoch: 22 [7680/50000 (17%)]\tLoss: 0.359375, Accuracy: 87.11\n",
      "Train Epoch: 22 [10240/50000 (23%)]\tLoss: 0.375955, Accuracy: 87.70\n",
      "Train Epoch: 22 [12800/50000 (28%)]\tLoss: 0.342482, Accuracy: 88.09\n",
      "Train Epoch: 22 [15360/50000 (34%)]\tLoss: 0.351403, Accuracy: 88.09\n",
      "Train Epoch: 22 [17920/50000 (40%)]\tLoss: 0.378144, Accuracy: 87.50\n",
      "Train Epoch: 22 [20480/50000 (45%)]\tLoss: 0.341455, Accuracy: 88.87\n",
      "Train Epoch: 22 [23040/50000 (51%)]\tLoss: 0.372072, Accuracy: 88.48\n",
      "Train Epoch: 22 [25600/50000 (57%)]\tLoss: 0.329227, Accuracy: 88.09\n",
      "Train Epoch: 22 [28160/50000 (62%)]\tLoss: 0.325317, Accuracy: 88.09\n",
      "Train Epoch: 22 [30720/50000 (68%)]\tLoss: 0.346650, Accuracy: 89.65\n",
      "Train Epoch: 22 [33280/50000 (74%)]\tLoss: 0.340983, Accuracy: 89.06\n",
      "Train Epoch: 22 [35840/50000 (80%)]\tLoss: 0.338771, Accuracy: 87.30\n",
      "Train Epoch: 22 [38400/50000 (85%)]\tLoss: 0.354586, Accuracy: 87.89\n",
      "Train Epoch: 22 [40960/50000 (91%)]\tLoss: 0.369625, Accuracy: 87.30\n",
      "Train Epoch: 22 [43520/50000 (97%)]\tLoss: 0.389008, Accuracy: 85.94\n",
      "\n",
      "Validation set: Average loss: 1.1849, Accuracy: 3344/5000 (66.00%)\n",
      "\n",
      "the time of this epoch:[36.377686738967896 s]\n",
      "\n",
      "Test set: Average loss: 1.2236, Accuracy: 6596/10000 (65.96%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.315077, Accuracy: 91.02\n",
      "Train Epoch: 23 [2560/50000 (6%)]\tLoss: 0.325903, Accuracy: 89.26\n",
      "Train Epoch: 23 [5120/50000 (11%)]\tLoss: 0.398022, Accuracy: 87.70\n",
      "Train Epoch: 23 [7680/50000 (17%)]\tLoss: 0.326171, Accuracy: 89.06\n",
      "Train Epoch: 23 [10240/50000 (23%)]\tLoss: 0.334596, Accuracy: 88.87\n",
      "Train Epoch: 23 [12800/50000 (28%)]\tLoss: 0.328633, Accuracy: 90.04\n",
      "Train Epoch: 23 [15360/50000 (34%)]\tLoss: 0.288326, Accuracy: 90.82\n",
      "Train Epoch: 23 [17920/50000 (40%)]\tLoss: 0.307431, Accuracy: 88.09\n",
      "Train Epoch: 23 [20480/50000 (45%)]\tLoss: 0.319752, Accuracy: 89.45\n",
      "Train Epoch: 23 [23040/50000 (51%)]\tLoss: 0.351144, Accuracy: 88.28\n",
      "Train Epoch: 23 [25600/50000 (57%)]\tLoss: 0.333818, Accuracy: 88.48\n",
      "Train Epoch: 23 [28160/50000 (62%)]\tLoss: 0.377598, Accuracy: 87.11\n",
      "Train Epoch: 23 [30720/50000 (68%)]\tLoss: 0.333402, Accuracy: 87.70\n",
      "Train Epoch: 23 [33280/50000 (74%)]\tLoss: 0.358305, Accuracy: 86.72\n",
      "Train Epoch: 23 [35840/50000 (80%)]\tLoss: 0.366970, Accuracy: 87.50\n",
      "Train Epoch: 23 [38400/50000 (85%)]\tLoss: 0.380563, Accuracy: 86.72\n",
      "Train Epoch: 23 [40960/50000 (91%)]\tLoss: 0.321530, Accuracy: 89.65\n",
      "Train Epoch: 23 [43520/50000 (97%)]\tLoss: 0.324998, Accuracy: 89.26\n",
      "\n",
      "Validation set: Average loss: 0.6709, Accuracy: 3870/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[39.44735288619995 s]\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.305360, Accuracy: 88.87\n",
      "Train Epoch: 24 [2560/50000 (6%)]\tLoss: 0.331798, Accuracy: 87.70\n",
      "Train Epoch: 24 [5120/50000 (11%)]\tLoss: 0.307929, Accuracy: 89.06\n",
      "Train Epoch: 24 [7680/50000 (17%)]\tLoss: 0.311269, Accuracy: 89.26\n",
      "Train Epoch: 24 [10240/50000 (23%)]\tLoss: 0.281373, Accuracy: 89.84\n",
      "Train Epoch: 24 [12800/50000 (28%)]\tLoss: 0.350303, Accuracy: 87.11\n",
      "Train Epoch: 24 [15360/50000 (34%)]\tLoss: 0.324038, Accuracy: 88.67\n",
      "Train Epoch: 24 [17920/50000 (40%)]\tLoss: 0.335047, Accuracy: 87.30\n",
      "Train Epoch: 24 [20480/50000 (45%)]\tLoss: 0.363974, Accuracy: 87.30\n",
      "Train Epoch: 24 [23040/50000 (51%)]\tLoss: 0.359454, Accuracy: 88.09\n",
      "Train Epoch: 24 [25600/50000 (57%)]\tLoss: 0.324118, Accuracy: 88.48\n",
      "Train Epoch: 24 [28160/50000 (62%)]\tLoss: 0.387531, Accuracy: 85.35\n",
      "Train Epoch: 24 [30720/50000 (68%)]\tLoss: 0.377078, Accuracy: 87.50\n",
      "Train Epoch: 24 [33280/50000 (74%)]\tLoss: 0.323276, Accuracy: 87.11\n",
      "Train Epoch: 24 [35840/50000 (80%)]\tLoss: 0.287782, Accuracy: 90.62\n",
      "Train Epoch: 24 [38400/50000 (85%)]\tLoss: 0.299110, Accuracy: 89.65\n",
      "Train Epoch: 24 [40960/50000 (91%)]\tLoss: 0.292035, Accuracy: 90.04\n",
      "Train Epoch: 24 [43520/50000 (97%)]\tLoss: 0.359993, Accuracy: 89.45\n",
      "\n",
      "Validation set: Average loss: 0.6924, Accuracy: 3910/5000 (78.00%)\n",
      "\n",
      "the time of this epoch:[36.361772537231445 s]\n",
      "\n",
      "Test set: Average loss: 0.7085, Accuracy: 7784/10000 (77.84%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.275952, Accuracy: 90.43\n",
      "Train Epoch: 25 [2560/50000 (6%)]\tLoss: 0.317551, Accuracy: 89.65\n",
      "Train Epoch: 25 [5120/50000 (11%)]\tLoss: 0.280632, Accuracy: 91.41\n",
      "Train Epoch: 25 [7680/50000 (17%)]\tLoss: 0.319000, Accuracy: 89.26\n",
      "Train Epoch: 25 [10240/50000 (23%)]\tLoss: 0.333914, Accuracy: 87.70\n",
      "Train Epoch: 25 [12800/50000 (28%)]\tLoss: 0.290804, Accuracy: 89.65\n",
      "Train Epoch: 25 [15360/50000 (34%)]\tLoss: 0.318808, Accuracy: 88.09\n",
      "Train Epoch: 25 [17920/50000 (40%)]\tLoss: 0.301742, Accuracy: 89.84\n",
      "Train Epoch: 25 [20480/50000 (45%)]\tLoss: 0.361335, Accuracy: 87.89\n",
      "Train Epoch: 25 [23040/50000 (51%)]\tLoss: 0.271334, Accuracy: 89.84\n",
      "Train Epoch: 25 [25600/50000 (57%)]\tLoss: 0.362464, Accuracy: 86.91\n",
      "Train Epoch: 25 [28160/50000 (62%)]\tLoss: 0.294524, Accuracy: 90.04\n",
      "Train Epoch: 25 [30720/50000 (68%)]\tLoss: 0.371615, Accuracy: 87.11\n",
      "Train Epoch: 25 [33280/50000 (74%)]\tLoss: 0.390422, Accuracy: 85.94\n",
      "Train Epoch: 25 [35840/50000 (80%)]\tLoss: 0.355978, Accuracy: 87.70\n",
      "Train Epoch: 25 [38400/50000 (85%)]\tLoss: 0.309315, Accuracy: 90.23\n",
      "Train Epoch: 25 [40960/50000 (91%)]\tLoss: 0.307724, Accuracy: 90.62\n",
      "Train Epoch: 25 [43520/50000 (97%)]\tLoss: 0.344922, Accuracy: 87.50\n",
      "\n",
      "Validation set: Average loss: 0.6410, Accuracy: 3938/5000 (78.00%)\n",
      "\n",
      "the time of this epoch:[39.387062549591064 s]\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.304072, Accuracy: 89.65\n",
      "Train Epoch: 26 [2560/50000 (6%)]\tLoss: 0.357214, Accuracy: 88.48\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0567]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.2836]],\n",
      "\n",
      "        [[ 0.0653]],\n",
      "\n",
      "        [[ 0.0173]],\n",
      "\n",
      "        [[ 0.0840]],\n",
      "\n",
      "        [[ 0.0166]],\n",
      "\n",
      "        [[ 0.0241]],\n",
      "\n",
      "        [[ 0.0893]],\n",
      "\n",
      "        [[ 0.0120]],\n",
      "\n",
      "        [[ 0.1138]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0427]],\n",
      "\n",
      "        [[ 0.0359]],\n",
      "\n",
      "        [[ 0.1139]],\n",
      "\n",
      "        [[ 0.1238]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0271]],\n",
      "\n",
      "        [[ 0.0242]],\n",
      "\n",
      "        [[ 0.0441]],\n",
      "\n",
      "        [[ 0.1290]],\n",
      "\n",
      "        [[ 0.2207]],\n",
      "\n",
      "        [[ 0.1422]],\n",
      "\n",
      "        [[ 0.0759]],\n",
      "\n",
      "        [[ 0.0254]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.1764]],\n",
      "\n",
      "        [[ 0.1319]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1268]],\n",
      "\n",
      "        [[ 0.0845]],\n",
      "\n",
      "        [[ 0.0470]],\n",
      "\n",
      "        [[ 0.0400]],\n",
      "\n",
      "        [[ 0.0794]],\n",
      "\n",
      "        [[ 0.1257]],\n",
      "\n",
      "        [[ 0.2098]],\n",
      "\n",
      "        [[ 0.1412]],\n",
      "\n",
      "        [[ 0.1263]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0105]],\n",
      "\n",
      "        [[ 0.0074]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0182]],\n",
      "\n",
      "        [[ 0.0664]],\n",
      "\n",
      "        [[ 0.0372]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.1588]],\n",
      "\n",
      "        [[ 0.2717]],\n",
      "\n",
      "        [[ 0.1133]],\n",
      "\n",
      "        [[ 0.0613]],\n",
      "\n",
      "        [[ 0.0119]],\n",
      "\n",
      "        [[ 0.0353]],\n",
      "\n",
      "        [[ 0.0776]],\n",
      "\n",
      "        [[ 0.0835]],\n",
      "\n",
      "        [[ 0.0952]],\n",
      "\n",
      "        [[ 0.1781]],\n",
      "\n",
      "        [[ 0.1426]],\n",
      "\n",
      "        [[ 0.2184]],\n",
      "\n",
      "        [[ 0.0614]],\n",
      "\n",
      "        [[ 0.0196]],\n",
      "\n",
      "        [[ 0.0738]],\n",
      "\n",
      "        [[ 0.0320]],\n",
      "\n",
      "        [[ 0.0486]],\n",
      "\n",
      "        [[ 0.0199]],\n",
      "\n",
      "        [[ 0.0285]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0697]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.0544]],\n",
      "\n",
      "        [[ 0.0196]],\n",
      "\n",
      "        [[ 0.1068]],\n",
      "\n",
      "        [[ 0.0387]],\n",
      "\n",
      "        [[ 0.0360]],\n",
      "\n",
      "        [[ 0.0107]],\n",
      "\n",
      "        [[ 0.1498]],\n",
      "\n",
      "        [[ 0.0544]],\n",
      "\n",
      "        [[ 0.1454]],\n",
      "\n",
      "        [[ 0.0261]],\n",
      "\n",
      "        [[ 0.0707]],\n",
      "\n",
      "        [[ 0.0750]],\n",
      "\n",
      "        [[ 0.0987]],\n",
      "\n",
      "        [[ 0.1297]],\n",
      "\n",
      "        [[ 0.0601]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0752]],\n",
      "\n",
      "        [[ 0.1181]],\n",
      "\n",
      "        [[ 0.1156]],\n",
      "\n",
      "        [[ 0.0547]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0313]],\n",
      "\n",
      "        [[ 0.1338]],\n",
      "\n",
      "        [[ 0.1239]],\n",
      "\n",
      "        [[ 0.0540]],\n",
      "\n",
      "        [[ 0.0782]],\n",
      "\n",
      "        [[ 0.0407]],\n",
      "\n",
      "        [[ 0.1316]],\n",
      "\n",
      "        [[ 0.0275]],\n",
      "\n",
      "        [[ 0.0104]],\n",
      "\n",
      "        [[ 0.0433]],\n",
      "\n",
      "        [[ 0.0462]],\n",
      "\n",
      "        [[ 0.0494]],\n",
      "\n",
      "        [[ 0.0244]],\n",
      "\n",
      "        [[ 0.1569]],\n",
      "\n",
      "        [[ 0.0408]],\n",
      "\n",
      "        [[ 0.0804]],\n",
      "\n",
      "        [[ 0.0455]],\n",
      "\n",
      "        [[ 0.0256]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.0370]],\n",
      "\n",
      "        [[ 0.0369]],\n",
      "\n",
      "        [[ 0.0217]],\n",
      "\n",
      "        [[ 0.0550]],\n",
      "\n",
      "        [[ 0.0393]],\n",
      "\n",
      "        [[ 0.0595]],\n",
      "\n",
      "        [[ 0.1032]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0628]],\n",
      "\n",
      "        [[ 0.0581]],\n",
      "\n",
      "        [[ 0.0255]],\n",
      "\n",
      "        [[ 0.0372]],\n",
      "\n",
      "        [[ 0.0387]],\n",
      "\n",
      "        [[ 0.0815]],\n",
      "\n",
      "        [[ 0.0682]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0402]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0752]],\n",
      "\n",
      "        [[ 0.0651]],\n",
      "\n",
      "        [[ 0.1191]],\n",
      "\n",
      "        [[ 0.0682]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0136]],\n",
      "\n",
      "        [[ 0.0815]],\n",
      "\n",
      "        [[ 0.0482]],\n",
      "\n",
      "        [[ 0.0528]],\n",
      "\n",
      "        [[ 0.0565]],\n",
      "\n",
      "        [[ 0.1028]],\n",
      "\n",
      "        [[ 0.0471]],\n",
      "\n",
      "        [[ 0.1145]],\n",
      "\n",
      "        [[ 0.0380]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.0265]],\n",
      "\n",
      "        [[ 0.0286]],\n",
      "\n",
      "        [[ 0.0447]],\n",
      "\n",
      "        [[ 0.1136]],\n",
      "\n",
      "        [[ 0.0172]],\n",
      "\n",
      "        [[ 0.0437]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0212]],\n",
      "\n",
      "        [[ 0.0676]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.1255]],\n",
      "\n",
      "        [[ 0.0654]],\n",
      "\n",
      "        [[ 0.1235]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0898]],\n",
      "\n",
      "        [[ 0.1081]],\n",
      "\n",
      "        [[ 0.0725]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0597]],\n",
      "\n",
      "        [[ 0.0536]],\n",
      "\n",
      "        [[ 0.1207]],\n",
      "\n",
      "        [[ 0.1209]],\n",
      "\n",
      "        [[ 0.1547]],\n",
      "\n",
      "        [[ 0.0328]],\n",
      "\n",
      "        [[ 0.0549]],\n",
      "\n",
      "        [[ 0.1192]],\n",
      "\n",
      "        [[ 0.1210]],\n",
      "\n",
      "        [[ 0.0166]],\n",
      "\n",
      "        [[ 0.0666]],\n",
      "\n",
      "        [[ 0.0257]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0254]],\n",
      "\n",
      "        [[ 0.0581]],\n",
      "\n",
      "        [[ 0.0551]],\n",
      "\n",
      "        [[ 0.0959]],\n",
      "\n",
      "        [[ 0.1512]],\n",
      "\n",
      "        [[ 0.0145]],\n",
      "\n",
      "        [[ 0.0355]],\n",
      "\n",
      "        [[ 0.0380]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1447]],\n",
      "\n",
      "        [[ 0.1380]],\n",
      "\n",
      "        [[ 0.0939]],\n",
      "\n",
      "        [[ 0.0673]],\n",
      "\n",
      "        [[ 0.0688]],\n",
      "\n",
      "        [[ 0.0066]],\n",
      "\n",
      "        [[ 0.1298]],\n",
      "\n",
      "        [[ 0.1311]],\n",
      "\n",
      "        [[ 0.0641]],\n",
      "\n",
      "        [[ 0.0228]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.1148]],\n",
      "\n",
      "        [[ 0.0439]],\n",
      "\n",
      "        [[ 0.1522]],\n",
      "\n",
      "        [[ 0.0052]],\n",
      "\n",
      "        [[ 0.0513]],\n",
      "\n",
      "        [[ 0.0329]],\n",
      "\n",
      "        [[ 0.0422]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0566]],\n",
      "\n",
      "        [[ 0.0467]],\n",
      "\n",
      "        [[ 0.0035]],\n",
      "\n",
      "        [[ 0.0724]],\n",
      "\n",
      "        [[ 0.0900]],\n",
      "\n",
      "        [[ 0.0275]],\n",
      "\n",
      "        [[ 0.0023]],\n",
      "\n",
      "        [[ 0.0425]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0582]],\n",
      "\n",
      "        [[ 0.0416]],\n",
      "\n",
      "        [[ 0.0377]],\n",
      "\n",
      "        [[ 0.0981]],\n",
      "\n",
      "        [[ 0.0819]],\n",
      "\n",
      "        [[ 0.1365]],\n",
      "\n",
      "        [[ 0.0333]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1443]],\n",
      "\n",
      "        [[ 0.0628]],\n",
      "\n",
      "        [[ 0.1585]],\n",
      "\n",
      "        [[ 0.0157]],\n",
      "\n",
      "        [[ 0.0300]],\n",
      "\n",
      "        [[ 0.1329]],\n",
      "\n",
      "        [[ 0.0496]],\n",
      "\n",
      "        [[ 0.0851]],\n",
      "\n",
      "        [[ 0.1348]],\n",
      "\n",
      "        [[ 0.0982]],\n",
      "\n",
      "        [[ 0.1439]],\n",
      "\n",
      "        [[ 0.0618]],\n",
      "\n",
      "        [[ 0.0822]],\n",
      "\n",
      "        [[ 0.0650]],\n",
      "\n",
      "        [[ 0.0290]],\n",
      "\n",
      "        [[ 0.0092]],\n",
      "\n",
      "        [[ 0.0809]],\n",
      "\n",
      "        [[ 0.0384]],\n",
      "\n",
      "        [[ 0.0434]],\n",
      "\n",
      "        [[ 0.0374]],\n",
      "\n",
      "        [[ 0.0308]],\n",
      "\n",
      "        [[ 0.0620]],\n",
      "\n",
      "        [[ 0.1146]],\n",
      "\n",
      "        [[ 0.0738]],\n",
      "\n",
      "        [[ 0.0384]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1098]],\n",
      "\n",
      "        [[ 0.0736]],\n",
      "\n",
      "        [[ 0.0685]],\n",
      "\n",
      "        [[ 0.0611]],\n",
      "\n",
      "        [[ 0.0708]],\n",
      "\n",
      "        [[ 0.0645]],\n",
      "\n",
      "        [[ 0.0692]],\n",
      "\n",
      "        [[ 0.0523]],\n",
      "\n",
      "        [[ 0.0754]],\n",
      "\n",
      "        [[ 0.0357]],\n",
      "\n",
      "        [[ 0.0747]],\n",
      "\n",
      "        [[ 0.0430]],\n",
      "\n",
      "        [[ 0.1479]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0538]],\n",
      "\n",
      "        [[ 0.0317]],\n",
      "\n",
      "        [[ 0.0536]],\n",
      "\n",
      "        [[ 0.0326]],\n",
      "\n",
      "        [[ 0.0417]],\n",
      "\n",
      "        [[ 0.0892]],\n",
      "\n",
      "        [[ 0.0647]],\n",
      "\n",
      "        [[ 0.0903]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.1506]],\n",
      "\n",
      "        [[ 0.1234]],\n",
      "\n",
      "        [[ 0.0723]],\n",
      "\n",
      "        [[ 0.0345]],\n",
      "\n",
      "        [[ 0.0437]],\n",
      "\n",
      "        [[ 0.0403]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.0115]],\n",
      "\n",
      "        [[ 0.0384]],\n",
      "\n",
      "        [[ 0.1368]],\n",
      "\n",
      "        [[ 0.0239]],\n",
      "\n",
      "        [[ 0.0257]],\n",
      "\n",
      "        [[ 0.1309]],\n",
      "\n",
      "        [[ 0.0238]],\n",
      "\n",
      "        [[ 0.0523]],\n",
      "\n",
      "        [[ 0.0600]],\n",
      "\n",
      "        [[ 0.0169]],\n",
      "\n",
      "        [[ 0.1311]],\n",
      "\n",
      "        [[ 0.0857]],\n",
      "\n",
      "        [[ 0.0459]],\n",
      "\n",
      "        [[ 0.0647]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0729]],\n",
      "\n",
      "        [[ 0.0563]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.1609]],\n",
      "\n",
      "        [[ 0.0611]],\n",
      "\n",
      "        [[ 0.0469]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0571]],\n",
      "\n",
      "        [[ 0.0652]],\n",
      "\n",
      "        [[ 0.0246]],\n",
      "\n",
      "        [[ 0.0244]],\n",
      "\n",
      "        [[ 0.0520]],\n",
      "\n",
      "        [[ 0.1233]],\n",
      "\n",
      "        [[ 0.0758]],\n",
      "\n",
      "        [[ 0.0611]],\n",
      "\n",
      "        [[ 0.0460]],\n",
      "\n",
      "        [[ 0.0887]],\n",
      "\n",
      "        [[ 0.0639]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0234]],\n",
      "\n",
      "        [[ 0.0531]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.0256]],\n",
      "\n",
      "        [[ 0.0307]],\n",
      "\n",
      "        [[ 0.0707]],\n",
      "\n",
      "        [[ 0.0948]],\n",
      "\n",
      "        [[ 0.1309]],\n",
      "\n",
      "        [[ 0.0741]],\n",
      "\n",
      "        [[ 0.0514]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0084]],\n",
      "\n",
      "        [[ 0.0876]],\n",
      "\n",
      "        [[ 0.0412]],\n",
      "\n",
      "        [[ 0.0845]],\n",
      "\n",
      "        [[ 0.0633]],\n",
      "\n",
      "        [[ 0.0477]],\n",
      "\n",
      "        [[ 0.0228]],\n",
      "\n",
      "        [[ 0.1687]],\n",
      "\n",
      "        [[ 0.0686]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.1397]],\n",
      "\n",
      "        [[ 0.0482]],\n",
      "\n",
      "        [[ 0.0642]],\n",
      "\n",
      "        [[ 0.0697]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.0540]],\n",
      "\n",
      "        [[ 0.0504]],\n",
      "\n",
      "        [[ 0.0936]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0634]],\n",
      "\n",
      "        [[ 0.0790]],\n",
      "\n",
      "        [[ 0.0710]],\n",
      "\n",
      "        [[ 0.0351]],\n",
      "\n",
      "        [[ 0.0241]],\n",
      "\n",
      "        [[ 0.0624]],\n",
      "\n",
      "        [[ 0.0285]],\n",
      "\n",
      "        [[ 0.1271]],\n",
      "\n",
      "        [[ 0.0582]],\n",
      "\n",
      "        [[ 0.1505]],\n",
      "\n",
      "        [[ 0.0379]],\n",
      "\n",
      "        [[ 0.0159]],\n",
      "\n",
      "        [[ 0.0454]],\n",
      "\n",
      "        [[ 0.0766]],\n",
      "\n",
      "        [[ 0.0702]],\n",
      "\n",
      "        [[ 0.0808]],\n",
      "\n",
      "        [[ 0.0372]],\n",
      "\n",
      "        [[ 0.0650]],\n",
      "\n",
      "        [[ 0.0551]],\n",
      "\n",
      "        [[ 0.0090]],\n",
      "\n",
      "        [[ 0.0663]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0411]],\n",
      "\n",
      "        [[ 0.0306]],\n",
      "\n",
      "        [[ 0.0749]],\n",
      "\n",
      "        [[ 0.0569]],\n",
      "\n",
      "        [[ 0.0233]],\n",
      "\n",
      "        [[ 0.0068]],\n",
      "\n",
      "        [[ 0.0554]],\n",
      "\n",
      "        [[ 0.0268]],\n",
      "\n",
      "        [[ 0.0314]],\n",
      "\n",
      "        [[ 0.1104]],\n",
      "\n",
      "        [[ 0.1292]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0716]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.1221]],\n",
      "\n",
      "        [[ 0.1223]],\n",
      "\n",
      "        [[ 0.0267]],\n",
      "\n",
      "        [[ 0.0291]],\n",
      "\n",
      "        [[ 0.0629]],\n",
      "\n",
      "        [[ 0.0393]],\n",
      "\n",
      "        [[ 0.1464]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0748]],\n",
      "\n",
      "        [[ 0.0770]],\n",
      "\n",
      "        [[ 0.0178]],\n",
      "\n",
      "        [[ 0.0565]],\n",
      "\n",
      "        [[ 0.0605]],\n",
      "\n",
      "        [[ 0.0382]],\n",
      "\n",
      "        [[ 0.0707]],\n",
      "\n",
      "        [[ 0.0205]],\n",
      "\n",
      "        [[ 0.1143]],\n",
      "\n",
      "        [[ 0.1233]],\n",
      "\n",
      "        [[ 0.0224]],\n",
      "\n",
      "        [[ 0.0590]],\n",
      "\n",
      "        [[ 0.0207]],\n",
      "\n",
      "        [[ 0.0699]],\n",
      "\n",
      "        [[ 0.0762]],\n",
      "\n",
      "        [[ 0.0785]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0497]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0688]],\n",
      "\n",
      "        [[ 0.1072]],\n",
      "\n",
      "        [[ 0.0219]],\n",
      "\n",
      "        [[ 0.0786]],\n",
      "\n",
      "        [[ 0.0962]],\n",
      "\n",
      "        [[ 0.0077]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0736]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0065]],\n",
      "\n",
      "        [[ 0.0909]],\n",
      "\n",
      "        [[ 0.1651]],\n",
      "\n",
      "        [[ 0.0357]],\n",
      "\n",
      "        [[ 0.0480]],\n",
      "\n",
      "        [[ 0.0317]],\n",
      "\n",
      "        [[ 0.0216]],\n",
      "\n",
      "        [[ 0.0553]],\n",
      "\n",
      "        [[ 0.0465]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1277]],\n",
      "\n",
      "        [[ 0.0395]],\n",
      "\n",
      "        [[ 0.0476]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0619]],\n",
      "\n",
      "        [[ 0.1453]],\n",
      "\n",
      "        [[ 0.0175]],\n",
      "\n",
      "        [[ 0.0943]],\n",
      "\n",
      "        [[ 0.0458]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.1075]],\n",
      "\n",
      "        [[ 0.0608]],\n",
      "\n",
      "        [[ 0.0690]],\n",
      "\n",
      "        [[ 0.0433]],\n",
      "\n",
      "        [[ 0.0696]],\n",
      "\n",
      "        [[ 0.1355]],\n",
      "\n",
      "        [[ 0.0322]],\n",
      "\n",
      "        [[ 0.0486]],\n",
      "\n",
      "        [[ 0.1262]],\n",
      "\n",
      "        [[ 0.0618]],\n",
      "\n",
      "        [[ 0.0722]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0430]],\n",
      "\n",
      "        [[ 0.0153]],\n",
      "\n",
      "        [[ 0.1464]],\n",
      "\n",
      "        [[ 0.1362]],\n",
      "\n",
      "        [[ 0.0618]],\n",
      "\n",
      "        [[ 0.0059]],\n",
      "\n",
      "        [[ 0.0486]],\n",
      "\n",
      "        [[ 0.0931]],\n",
      "\n",
      "        [[ 0.1526]],\n",
      "\n",
      "        [[ 0.0964]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1516]],\n",
      "\n",
      "        [[ 0.0358]],\n",
      "\n",
      "        [[ 0.0271]],\n",
      "\n",
      "        [[ 0.0184]],\n",
      "\n",
      "        [[ 0.0892]],\n",
      "\n",
      "        [[ 0.1270]],\n",
      "\n",
      "        [[ 0.0712]],\n",
      "\n",
      "        [[ 0.0609]],\n",
      "\n",
      "        [[ 0.1311]],\n",
      "\n",
      "        [[ 0.0307]],\n",
      "\n",
      "        [[ 0.0244]],\n",
      "\n",
      "        [[ 0.0439]],\n",
      "\n",
      "        [[ 0.1598]],\n",
      "\n",
      "        [[ 0.1424]],\n",
      "\n",
      "        [[ 0.0451]],\n",
      "\n",
      "        [[ 0.0388]],\n",
      "\n",
      "        [[ 0.0345]],\n",
      "\n",
      "        [[ 0.0475]],\n",
      "\n",
      "        [[ 0.0669]],\n",
      "\n",
      "        [[ 0.0646]],\n",
      "\n",
      "        [[ 0.0136]],\n",
      "\n",
      "        [[ 0.0783]],\n",
      "\n",
      "        [[ 0.0693]],\n",
      "\n",
      "        [[ 0.0759]],\n",
      "\n",
      "        [[ 0.0146]],\n",
      "\n",
      "        [[ 0.0812]],\n",
      "\n",
      "        [[ 0.0828]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0171]],\n",
      "\n",
      "        [[ 0.0493]],\n",
      "\n",
      "        [[ 0.0588]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0614]],\n",
      "\n",
      "        [[ 0.0472]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0367]],\n",
      "\n",
      "        [[ 0.0585]],\n",
      "\n",
      "        [[ 0.0490]],\n",
      "\n",
      "        [[ 0.0519]],\n",
      "\n",
      "        [[ 0.1193]],\n",
      "\n",
      "        [[ 0.0792]],\n",
      "\n",
      "        [[ 0.0746]],\n",
      "\n",
      "        [[ 0.0346]],\n",
      "\n",
      "        [[ 0.0077]],\n",
      "\n",
      "        [[ 0.0532]],\n",
      "\n",
      "        [[ 0.0609]],\n",
      "\n",
      "        [[ 0.0275]],\n",
      "\n",
      "        [[ 0.0298]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0254]],\n",
      "\n",
      "        [[ 0.0314]],\n",
      "\n",
      "        [[ 0.0205]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-03 *\n",
      "       [[[-8.6169]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[-0.0825,  0.1022, -0.0356,  ..., -0.0297, -0.0808, -0.0851]],\n",
      "\n",
      "        [[ 0.0592, -0.0942, -0.0577,  ..., -0.0828,  0.0934,  0.0590]],\n",
      "\n",
      "        [[ 0.0736, -0.1027, -0.0558,  ..., -0.0055,  0.0451, -0.0425]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0375,  0.0156, -0.0371,  ..., -0.0466,  0.0228,  0.1021]],\n",
      "\n",
      "        [[-0.0234,  0.0510,  0.0753,  ...,  0.0928, -0.0082, -0.0370]],\n",
      "\n",
      "        [[-0.0598,  0.0245, -0.0056,  ..., -0.0159, -0.0633, -0.0484]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor([[[-4.5345e-03, -5.5947e-03, -6.5310e-03,  ..., -1.7171e-03,\n",
      "          -9.4921e-04, -4.0893e-04],\n",
      "         [-9.2414e-04, -1.9610e-04, -1.6763e-03,  ..., -6.5523e-03,\n",
      "          -4.4083e-03, -4.1619e-03],\n",
      "         [ 2.4558e-04, -6.9820e-04, -2.1911e-03,  ..., -2.9943e-03,\n",
      "          -2.8501e-03, -6.6963e-04],\n",
      "         ...,\n",
      "         [ 3.8274e-03,  4.1805e-03,  3.2062e-03,  ...,  1.3576e-03,\n",
      "           2.5673e-03,  2.6510e-03],\n",
      "         [ 2.8744e-03,  1.7265e-03,  3.6634e-03,  ...,  5.5622e-03,\n",
      "           3.1584e-03,  3.1337e-03],\n",
      "         [-3.2762e-03, -3.9262e-03, -4.0616e-03,  ..., -2.0868e-03,\n",
      "          -1.6739e-03, -2.3647e-03]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [5120/50000 (11%)]\tLoss: 0.335607, Accuracy: 89.45\n",
      "Train Epoch: 26 [7680/50000 (17%)]\tLoss: 0.351789, Accuracy: 87.89\n",
      "Train Epoch: 26 [10240/50000 (23%)]\tLoss: 0.288355, Accuracy: 89.84\n",
      "Train Epoch: 26 [12800/50000 (28%)]\tLoss: 0.324689, Accuracy: 88.48\n",
      "Train Epoch: 26 [15360/50000 (34%)]\tLoss: 0.342731, Accuracy: 88.48\n",
      "Train Epoch: 26 [17920/50000 (40%)]\tLoss: 0.294947, Accuracy: 89.65\n",
      "Train Epoch: 26 [20480/50000 (45%)]\tLoss: 0.345791, Accuracy: 88.09\n",
      "Train Epoch: 26 [23040/50000 (51%)]\tLoss: 0.304627, Accuracy: 89.65\n",
      "Train Epoch: 26 [25600/50000 (57%)]\tLoss: 0.392066, Accuracy: 86.33\n",
      "Train Epoch: 26 [28160/50000 (62%)]\tLoss: 0.302649, Accuracy: 89.84\n",
      "Train Epoch: 26 [30720/50000 (68%)]\tLoss: 0.364487, Accuracy: 88.28\n",
      "Train Epoch: 26 [33280/50000 (74%)]\tLoss: 0.362856, Accuracy: 87.30\n",
      "Train Epoch: 26 [35840/50000 (80%)]\tLoss: 0.350140, Accuracy: 87.89\n",
      "Train Epoch: 26 [38400/50000 (85%)]\tLoss: 0.413597, Accuracy: 84.57\n",
      "Train Epoch: 26 [40960/50000 (91%)]\tLoss: 0.354986, Accuracy: 88.28\n",
      "Train Epoch: 26 [43520/50000 (97%)]\tLoss: 0.358923, Accuracy: 88.09\n",
      "\n",
      "Validation set: Average loss: 0.6609, Accuracy: 3965/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[36.33463788032532 s]\n",
      "\n",
      "Test set: Average loss: 0.6624, Accuracy: 7959/10000 (79.59%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.333701, Accuracy: 88.67\n",
      "Train Epoch: 27 [2560/50000 (6%)]\tLoss: 0.276520, Accuracy: 90.43\n",
      "Train Epoch: 27 [5120/50000 (11%)]\tLoss: 0.328062, Accuracy: 89.26\n",
      "Train Epoch: 27 [7680/50000 (17%)]\tLoss: 0.268290, Accuracy: 89.84\n",
      "Train Epoch: 27 [10240/50000 (23%)]\tLoss: 0.317733, Accuracy: 88.48\n",
      "Train Epoch: 27 [12800/50000 (28%)]\tLoss: 0.317208, Accuracy: 90.23\n",
      "Train Epoch: 27 [15360/50000 (34%)]\tLoss: 0.340841, Accuracy: 88.67\n",
      "Train Epoch: 27 [17920/50000 (40%)]\tLoss: 0.379610, Accuracy: 86.13\n",
      "Train Epoch: 27 [20480/50000 (45%)]\tLoss: 0.302673, Accuracy: 90.43\n",
      "Train Epoch: 27 [23040/50000 (51%)]\tLoss: 0.305948, Accuracy: 88.67\n",
      "Train Epoch: 27 [25600/50000 (57%)]\tLoss: 0.317992, Accuracy: 89.06\n",
      "Train Epoch: 27 [28160/50000 (62%)]\tLoss: 0.318362, Accuracy: 88.48\n",
      "Train Epoch: 27 [30720/50000 (68%)]\tLoss: 0.435186, Accuracy: 84.77\n",
      "Train Epoch: 27 [33280/50000 (74%)]\tLoss: 0.306205, Accuracy: 88.87\n",
      "Train Epoch: 27 [35840/50000 (80%)]\tLoss: 0.313576, Accuracy: 89.45\n",
      "Train Epoch: 27 [38400/50000 (85%)]\tLoss: 0.309871, Accuracy: 88.09\n",
      "Train Epoch: 27 [40960/50000 (91%)]\tLoss: 0.319480, Accuracy: 89.84\n",
      "Train Epoch: 27 [43520/50000 (97%)]\tLoss: 0.369003, Accuracy: 86.72\n",
      "\n",
      "Validation set: Average loss: 0.5748, Accuracy: 4080/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[39.46088528633118 s]\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.299373, Accuracy: 89.65\n",
      "Train Epoch: 28 [2560/50000 (6%)]\tLoss: 0.289299, Accuracy: 90.43\n",
      "Train Epoch: 28 [5120/50000 (11%)]\tLoss: 0.317732, Accuracy: 90.04\n",
      "Train Epoch: 28 [7680/50000 (17%)]\tLoss: 0.279203, Accuracy: 90.23\n",
      "Train Epoch: 28 [10240/50000 (23%)]\tLoss: 0.268043, Accuracy: 92.19\n",
      "Train Epoch: 28 [12800/50000 (28%)]\tLoss: 0.236903, Accuracy: 91.80\n",
      "Train Epoch: 28 [15360/50000 (34%)]\tLoss: 0.306012, Accuracy: 89.45\n",
      "Train Epoch: 28 [17920/50000 (40%)]\tLoss: 0.275578, Accuracy: 91.41\n",
      "Train Epoch: 28 [20480/50000 (45%)]\tLoss: 0.284349, Accuracy: 89.45\n",
      "Train Epoch: 28 [23040/50000 (51%)]\tLoss: 0.283122, Accuracy: 90.82\n",
      "Train Epoch: 28 [25600/50000 (57%)]\tLoss: 0.362901, Accuracy: 87.30\n",
      "Train Epoch: 28 [28160/50000 (62%)]\tLoss: 0.273747, Accuracy: 90.62\n",
      "Train Epoch: 28 [30720/50000 (68%)]\tLoss: 0.345124, Accuracy: 89.06\n",
      "Train Epoch: 28 [33280/50000 (74%)]\tLoss: 0.333079, Accuracy: 86.91\n",
      "Train Epoch: 28 [35840/50000 (80%)]\tLoss: 0.335337, Accuracy: 87.89\n",
      "Train Epoch: 28 [38400/50000 (85%)]\tLoss: 0.278475, Accuracy: 90.04\n",
      "Train Epoch: 28 [40960/50000 (91%)]\tLoss: 0.361607, Accuracy: 87.89\n",
      "Train Epoch: 28 [43520/50000 (97%)]\tLoss: 0.281193, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.7260, Accuracy: 3863/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[36.81368684768677 s]\n",
      "\n",
      "Test set: Average loss: 0.7328, Accuracy: 7694/10000 (76.94%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.301595, Accuracy: 89.26\n",
      "Train Epoch: 29 [2560/50000 (6%)]\tLoss: 0.308792, Accuracy: 89.26\n",
      "Train Epoch: 29 [5120/50000 (11%)]\tLoss: 0.274064, Accuracy: 89.84\n",
      "Train Epoch: 29 [7680/50000 (17%)]\tLoss: 0.303455, Accuracy: 88.67\n",
      "Train Epoch: 29 [10240/50000 (23%)]\tLoss: 0.321422, Accuracy: 89.84\n",
      "Train Epoch: 29 [12800/50000 (28%)]\tLoss: 0.311977, Accuracy: 89.06\n",
      "Train Epoch: 29 [15360/50000 (34%)]\tLoss: 0.264223, Accuracy: 91.02\n",
      "Train Epoch: 29 [17920/50000 (40%)]\tLoss: 0.298841, Accuracy: 89.06\n",
      "Train Epoch: 29 [20480/50000 (45%)]\tLoss: 0.365167, Accuracy: 87.50\n",
      "Train Epoch: 29 [23040/50000 (51%)]\tLoss: 0.363256, Accuracy: 88.48\n",
      "Train Epoch: 29 [25600/50000 (57%)]\tLoss: 0.323380, Accuracy: 89.26\n",
      "Train Epoch: 29 [28160/50000 (62%)]\tLoss: 0.278518, Accuracy: 89.45\n",
      "Train Epoch: 29 [30720/50000 (68%)]\tLoss: 0.308510, Accuracy: 90.04\n",
      "Train Epoch: 29 [33280/50000 (74%)]\tLoss: 0.327680, Accuracy: 89.26\n",
      "Train Epoch: 29 [35840/50000 (80%)]\tLoss: 0.358390, Accuracy: 87.70\n",
      "Train Epoch: 29 [38400/50000 (85%)]\tLoss: 0.332399, Accuracy: 89.84\n",
      "Train Epoch: 29 [40960/50000 (91%)]\tLoss: 0.365597, Accuracy: 87.30\n",
      "Train Epoch: 29 [43520/50000 (97%)]\tLoss: 0.268753, Accuracy: 91.02\n",
      "\n",
      "Validation set: Average loss: 0.7311, Accuracy: 3859/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[39.716992139816284 s]\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.278840, Accuracy: 90.23\n",
      "Train Epoch: 30 [2560/50000 (6%)]\tLoss: 0.312221, Accuracy: 88.67\n",
      "Train Epoch: 30 [5120/50000 (11%)]\tLoss: 0.266394, Accuracy: 90.82\n",
      "Train Epoch: 30 [7680/50000 (17%)]\tLoss: 0.324994, Accuracy: 89.65\n",
      "Train Epoch: 30 [10240/50000 (23%)]\tLoss: 0.321789, Accuracy: 91.02\n",
      "Train Epoch: 30 [12800/50000 (28%)]\tLoss: 0.258266, Accuracy: 90.43\n",
      "Train Epoch: 30 [15360/50000 (34%)]\tLoss: 0.358495, Accuracy: 89.06\n",
      "Train Epoch: 30 [17920/50000 (40%)]\tLoss: 0.261899, Accuracy: 91.99\n",
      "Train Epoch: 30 [20480/50000 (45%)]\tLoss: 0.260887, Accuracy: 91.60\n",
      "Train Epoch: 30 [23040/50000 (51%)]\tLoss: 0.277090, Accuracy: 91.60\n",
      "Train Epoch: 30 [25600/50000 (57%)]\tLoss: 0.325007, Accuracy: 89.45\n",
      "Train Epoch: 30 [28160/50000 (62%)]\tLoss: 0.249437, Accuracy: 91.99\n",
      "Train Epoch: 30 [30720/50000 (68%)]\tLoss: 0.322028, Accuracy: 88.87\n",
      "Train Epoch: 30 [33280/50000 (74%)]\tLoss: 0.264395, Accuracy: 90.43\n",
      "Train Epoch: 30 [35840/50000 (80%)]\tLoss: 0.319804, Accuracy: 89.45\n",
      "Train Epoch: 30 [38400/50000 (85%)]\tLoss: 0.293635, Accuracy: 90.04\n",
      "Train Epoch: 30 [40960/50000 (91%)]\tLoss: 0.253124, Accuracy: 91.99\n",
      "Train Epoch: 30 [43520/50000 (97%)]\tLoss: 0.298526, Accuracy: 90.43\n",
      "\n",
      "Validation set: Average loss: 0.7375, Accuracy: 3884/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[36.77315282821655 s]\n",
      "\n",
      "Test set: Average loss: 0.7819, Accuracy: 7683/10000 (76.83%)\n",
      "\n",
      "Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.292479, Accuracy: 91.02\n",
      "Train Epoch: 31 [2560/50000 (6%)]\tLoss: 0.347495, Accuracy: 88.28\n",
      "Train Epoch: 31 [5120/50000 (11%)]\tLoss: 0.317643, Accuracy: 88.28\n",
      "Train Epoch: 31 [7680/50000 (17%)]\tLoss: 0.295945, Accuracy: 89.26\n",
      "Train Epoch: 31 [10240/50000 (23%)]\tLoss: 0.324930, Accuracy: 89.84\n",
      "Train Epoch: 31 [12800/50000 (28%)]\tLoss: 0.323771, Accuracy: 88.67\n",
      "Train Epoch: 31 [15360/50000 (34%)]\tLoss: 0.333337, Accuracy: 87.30\n",
      "Train Epoch: 31 [17920/50000 (40%)]\tLoss: 0.289241, Accuracy: 90.04\n",
      "Train Epoch: 31 [20480/50000 (45%)]\tLoss: 0.317790, Accuracy: 89.06\n",
      "Train Epoch: 31 [23040/50000 (51%)]\tLoss: 0.295925, Accuracy: 89.84\n",
      "Train Epoch: 31 [25600/50000 (57%)]\tLoss: 0.336116, Accuracy: 88.87\n",
      "Train Epoch: 31 [28160/50000 (62%)]\tLoss: 0.346481, Accuracy: 89.06\n",
      "Train Epoch: 31 [30720/50000 (68%)]\tLoss: 0.322311, Accuracy: 89.45\n",
      "Train Epoch: 31 [33280/50000 (74%)]\tLoss: 0.305650, Accuracy: 90.04\n",
      "Train Epoch: 31 [35840/50000 (80%)]\tLoss: 0.299096, Accuracy: 91.02\n",
      "Train Epoch: 31 [38400/50000 (85%)]\tLoss: 0.259131, Accuracy: 90.62\n",
      "Train Epoch: 31 [40960/50000 (91%)]\tLoss: 0.314313, Accuracy: 89.06\n",
      "Train Epoch: 31 [43520/50000 (97%)]\tLoss: 0.290433, Accuracy: 90.43\n",
      "\n",
      "Validation set: Average loss: 0.7006, Accuracy: 3898/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[39.59933543205261 s]\n",
      "Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.302913, Accuracy: 89.65\n",
      "Train Epoch: 32 [2560/50000 (6%)]\tLoss: 0.271134, Accuracy: 91.21\n",
      "Train Epoch: 32 [5120/50000 (11%)]\tLoss: 0.299512, Accuracy: 90.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [7680/50000 (17%)]\tLoss: 0.281339, Accuracy: 90.23\n",
      "Train Epoch: 32 [10240/50000 (23%)]\tLoss: 0.259563, Accuracy: 91.80\n",
      "Train Epoch: 32 [12800/50000 (28%)]\tLoss: 0.259241, Accuracy: 91.99\n",
      "Train Epoch: 32 [15360/50000 (34%)]\tLoss: 0.288023, Accuracy: 90.62\n",
      "Train Epoch: 32 [17920/50000 (40%)]\tLoss: 0.249938, Accuracy: 91.99\n",
      "Train Epoch: 32 [20480/50000 (45%)]\tLoss: 0.412489, Accuracy: 85.55\n",
      "Train Epoch: 32 [23040/50000 (51%)]\tLoss: 0.342289, Accuracy: 88.67\n",
      "Train Epoch: 32 [25600/50000 (57%)]\tLoss: 0.296568, Accuracy: 89.65\n",
      "Train Epoch: 32 [28160/50000 (62%)]\tLoss: 0.338945, Accuracy: 87.50\n",
      "Train Epoch: 32 [30720/50000 (68%)]\tLoss: 0.356561, Accuracy: 88.48\n",
      "Train Epoch: 32 [33280/50000 (74%)]\tLoss: 0.296072, Accuracy: 91.60\n",
      "Train Epoch: 32 [35840/50000 (80%)]\tLoss: 0.293242, Accuracy: 89.45\n",
      "Train Epoch: 32 [38400/50000 (85%)]\tLoss: 0.330750, Accuracy: 89.45\n",
      "Train Epoch: 32 [40960/50000 (91%)]\tLoss: 0.271232, Accuracy: 91.21\n",
      "Train Epoch: 32 [43520/50000 (97%)]\tLoss: 0.287865, Accuracy: 91.21\n",
      "\n",
      "Validation set: Average loss: 0.7325, Accuracy: 3891/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[36.35786986351013 s]\n",
      "\n",
      "Test set: Average loss: 0.7850, Accuracy: 7710/10000 (77.10%)\n",
      "\n",
      "Train Epoch: 33 [0/50000 (0%)]\tLoss: 0.249428, Accuracy: 91.60\n",
      "Train Epoch: 33 [2560/50000 (6%)]\tLoss: 0.270084, Accuracy: 90.23\n",
      "Train Epoch: 33 [5120/50000 (11%)]\tLoss: 0.273006, Accuracy: 91.02\n",
      "Train Epoch: 33 [7680/50000 (17%)]\tLoss: 0.262971, Accuracy: 90.82\n",
      "Train Epoch: 33 [10240/50000 (23%)]\tLoss: 0.281620, Accuracy: 91.41\n",
      "Train Epoch: 33 [12800/50000 (28%)]\tLoss: 0.300211, Accuracy: 89.84\n",
      "Train Epoch: 33 [15360/50000 (34%)]\tLoss: 0.244097, Accuracy: 91.80\n",
      "Train Epoch: 33 [17920/50000 (40%)]\tLoss: 0.253790, Accuracy: 91.21\n",
      "Train Epoch: 33 [20480/50000 (45%)]\tLoss: 0.291360, Accuracy: 90.62\n",
      "Train Epoch: 33 [23040/50000 (51%)]\tLoss: 0.337114, Accuracy: 88.67\n",
      "Train Epoch: 33 [25600/50000 (57%)]\tLoss: 0.360146, Accuracy: 88.87\n",
      "Train Epoch: 33 [28160/50000 (62%)]\tLoss: 0.301221, Accuracy: 91.41\n",
      "Train Epoch: 33 [30720/50000 (68%)]\tLoss: 0.344305, Accuracy: 88.09\n",
      "Train Epoch: 33 [33280/50000 (74%)]\tLoss: 0.277190, Accuracy: 91.02\n",
      "Train Epoch: 33 [35840/50000 (80%)]\tLoss: 0.306039, Accuracy: 90.43\n",
      "Train Epoch: 33 [38400/50000 (85%)]\tLoss: 0.332747, Accuracy: 89.26\n",
      "Train Epoch: 33 [40960/50000 (91%)]\tLoss: 0.296861, Accuracy: 91.02\n",
      "Train Epoch: 33 [43520/50000 (97%)]\tLoss: 0.307853, Accuracy: 89.06\n",
      "\n",
      "Validation set: Average loss: 0.6106, Accuracy: 4059/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[39.46945881843567 s]\n",
      "Train Epoch: 34 [0/50000 (0%)]\tLoss: 0.215892, Accuracy: 93.36\n",
      "Train Epoch: 34 [2560/50000 (6%)]\tLoss: 0.259579, Accuracy: 91.80\n",
      "Train Epoch: 34 [5120/50000 (11%)]\tLoss: 0.250149, Accuracy: 90.62\n",
      "Train Epoch: 34 [7680/50000 (17%)]\tLoss: 0.370417, Accuracy: 87.70\n",
      "Train Epoch: 34 [10240/50000 (23%)]\tLoss: 0.280672, Accuracy: 90.62\n",
      "Train Epoch: 34 [12800/50000 (28%)]\tLoss: 0.340088, Accuracy: 89.45\n",
      "Train Epoch: 34 [15360/50000 (34%)]\tLoss: 0.247226, Accuracy: 90.82\n",
      "Train Epoch: 34 [17920/50000 (40%)]\tLoss: 0.292314, Accuracy: 90.43\n",
      "Train Epoch: 34 [20480/50000 (45%)]\tLoss: 0.302866, Accuracy: 90.04\n",
      "Train Epoch: 34 [23040/50000 (51%)]\tLoss: 0.351771, Accuracy: 87.30\n",
      "Train Epoch: 34 [25600/50000 (57%)]\tLoss: 0.323155, Accuracy: 87.89\n",
      "Train Epoch: 34 [28160/50000 (62%)]\tLoss: 0.276531, Accuracy: 89.65\n",
      "Train Epoch: 34 [30720/50000 (68%)]\tLoss: 0.303360, Accuracy: 90.43\n",
      "Train Epoch: 34 [33280/50000 (74%)]\tLoss: 0.280699, Accuracy: 91.21\n",
      "Train Epoch: 34 [35840/50000 (80%)]\tLoss: 0.375568, Accuracy: 87.50\n",
      "Train Epoch: 34 [38400/50000 (85%)]\tLoss: 0.300801, Accuracy: 89.65\n",
      "Train Epoch: 34 [40960/50000 (91%)]\tLoss: 0.284839, Accuracy: 89.06\n",
      "Train Epoch: 34 [43520/50000 (97%)]\tLoss: 0.280573, Accuracy: 90.23\n",
      "\n",
      "Validation set: Average loss: 0.5495, Accuracy: 4152/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.33922076225281 s]\n",
      "\n",
      "Test set: Average loss: 0.5531, Accuracy: 8222/10000 (82.22%)\n",
      "\n",
      "Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.324585, Accuracy: 88.09\n",
      "Train Epoch: 35 [2560/50000 (6%)]\tLoss: 0.281948, Accuracy: 91.02\n",
      "Train Epoch: 35 [5120/50000 (11%)]\tLoss: 0.309127, Accuracy: 89.84\n",
      "Train Epoch: 35 [7680/50000 (17%)]\tLoss: 0.250103, Accuracy: 91.41\n",
      "Train Epoch: 35 [10240/50000 (23%)]\tLoss: 0.277862, Accuracy: 91.41\n",
      "Train Epoch: 35 [12800/50000 (28%)]\tLoss: 0.236961, Accuracy: 92.19\n",
      "Train Epoch: 35 [15360/50000 (34%)]\tLoss: 0.277763, Accuracy: 89.65\n",
      "Train Epoch: 35 [17920/50000 (40%)]\tLoss: 0.263978, Accuracy: 91.21\n",
      "Train Epoch: 35 [20480/50000 (45%)]\tLoss: 0.282967, Accuracy: 90.04\n",
      "Train Epoch: 35 [23040/50000 (51%)]\tLoss: 0.241380, Accuracy: 91.21\n",
      "Train Epoch: 35 [25600/50000 (57%)]\tLoss: 0.353771, Accuracy: 87.30\n",
      "Train Epoch: 35 [28160/50000 (62%)]\tLoss: 0.328303, Accuracy: 89.65\n",
      "Train Epoch: 35 [30720/50000 (68%)]\tLoss: 0.280675, Accuracy: 90.23\n",
      "Train Epoch: 35 [33280/50000 (74%)]\tLoss: 0.281739, Accuracy: 89.65\n",
      "Train Epoch: 35 [35840/50000 (80%)]\tLoss: 0.289764, Accuracy: 89.45\n",
      "Train Epoch: 35 [38400/50000 (85%)]\tLoss: 0.282204, Accuracy: 90.82\n",
      "Train Epoch: 35 [40960/50000 (91%)]\tLoss: 0.277328, Accuracy: 91.21\n",
      "Train Epoch: 35 [43520/50000 (97%)]\tLoss: 0.306360, Accuracy: 89.65\n",
      "\n",
      "Validation set: Average loss: 0.7320, Accuracy: 3917/5000 (78.00%)\n",
      "\n",
      "the time of this epoch:[39.369747161865234 s]\n",
      "Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.292367, Accuracy: 89.26\n",
      "Train Epoch: 36 [2560/50000 (6%)]\tLoss: 0.292466, Accuracy: 90.82\n",
      "Train Epoch: 36 [5120/50000 (11%)]\tLoss: 0.226461, Accuracy: 92.19\n",
      "Train Epoch: 36 [7680/50000 (17%)]\tLoss: 0.279336, Accuracy: 90.62\n",
      "Train Epoch: 36 [10240/50000 (23%)]\tLoss: 0.250059, Accuracy: 91.02\n",
      "Train Epoch: 36 [12800/50000 (28%)]\tLoss: 0.274877, Accuracy: 89.84\n",
      "Train Epoch: 36 [15360/50000 (34%)]\tLoss: 0.281377, Accuracy: 90.62\n",
      "Train Epoch: 36 [17920/50000 (40%)]\tLoss: 0.209126, Accuracy: 93.16\n",
      "Train Epoch: 36 [20480/50000 (45%)]\tLoss: 0.259725, Accuracy: 91.02\n",
      "Train Epoch: 36 [23040/50000 (51%)]\tLoss: 0.359059, Accuracy: 88.67\n",
      "Train Epoch: 36 [25600/50000 (57%)]\tLoss: 0.290254, Accuracy: 90.82\n",
      "Train Epoch: 36 [28160/50000 (62%)]\tLoss: 0.356457, Accuracy: 87.89\n",
      "Train Epoch: 36 [30720/50000 (68%)]\tLoss: 0.305521, Accuracy: 88.67\n",
      "Train Epoch: 36 [33280/50000 (74%)]\tLoss: 0.270348, Accuracy: 90.23\n",
      "Train Epoch: 36 [35840/50000 (80%)]\tLoss: 0.286438, Accuracy: 90.82\n",
      "Train Epoch: 36 [38400/50000 (85%)]\tLoss: 0.267382, Accuracy: 91.41\n",
      "Train Epoch: 36 [40960/50000 (91%)]\tLoss: 0.263300, Accuracy: 90.62\n",
      "Train Epoch: 36 [43520/50000 (97%)]\tLoss: 0.321211, Accuracy: 89.26\n",
      "\n",
      "Validation set: Average loss: 0.5685, Accuracy: 4103/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[36.34802865982056 s]\n",
      "\n",
      "Test set: Average loss: 0.5762, Accuracy: 8206/10000 (82.06%)\n",
      "\n",
      "Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.247032, Accuracy: 91.41\n",
      "Train Epoch: 37 [2560/50000 (6%)]\tLoss: 0.249373, Accuracy: 91.02\n",
      "Train Epoch: 37 [5120/50000 (11%)]\tLoss: 0.283300, Accuracy: 90.04\n",
      "Train Epoch: 37 [7680/50000 (17%)]\tLoss: 0.254980, Accuracy: 91.02\n",
      "Train Epoch: 37 [10240/50000 (23%)]\tLoss: 0.259893, Accuracy: 90.82\n",
      "Train Epoch: 37 [12800/50000 (28%)]\tLoss: 0.266124, Accuracy: 91.80\n",
      "Train Epoch: 37 [15360/50000 (34%)]\tLoss: 0.249667, Accuracy: 91.21\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0000]],\n",
      "\n",
      "        [[ 0.1501]],\n",
      "\n",
      "        [[ 0.0314]],\n",
      "\n",
      "        [[ 0.0067]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0569]],\n",
      "\n",
      "        [[ 0.0523]],\n",
      "\n",
      "        [[ 0.0122]],\n",
      "\n",
      "        [[ 0.0479]],\n",
      "\n",
      "        [[ 0.1143]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.1418]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0706]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0976]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0724]],\n",
      "\n",
      "        [[ 0.1276]],\n",
      "\n",
      "        [[ 0.0333]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1298]],\n",
      "\n",
      "        [[ 0.0187]],\n",
      "\n",
      "        [[ 0.0532]],\n",
      "\n",
      "        [[ 0.1079]],\n",
      "\n",
      "        [[ 0.0418]],\n",
      "\n",
      "        [[ 0.0927]],\n",
      "\n",
      "        [[ 0.0937]],\n",
      "\n",
      "        [[ 0.0309]],\n",
      "\n",
      "        [[ 0.0120]],\n",
      "\n",
      "        [[ 0.1269]],\n",
      "\n",
      "        [[ 0.0976]],\n",
      "\n",
      "        [[ 0.0672]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0974]],\n",
      "\n",
      "        [[ 0.1104]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0842]],\n",
      "\n",
      "        [[ 0.1253]],\n",
      "\n",
      "        [[ 0.1013]],\n",
      "\n",
      "        [[ 0.0052]],\n",
      "\n",
      "        [[ 0.1719]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0367]],\n",
      "\n",
      "        [[ 0.0055]],\n",
      "\n",
      "        [[ 0.0022]],\n",
      "\n",
      "        [[ 0.0472]],\n",
      "\n",
      "        [[ 0.0516]],\n",
      "\n",
      "        [[ 0.0962]],\n",
      "\n",
      "        [[ 0.0291]],\n",
      "\n",
      "        [[ 0.0523]],\n",
      "\n",
      "        [[ 0.0559]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0692]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0907]],\n",
      "\n",
      "        [[ 0.1125]],\n",
      "\n",
      "        [[ 0.0680]],\n",
      "\n",
      "        [[ 0.0661]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0718]],\n",
      "\n",
      "        [[ 0.0570]],\n",
      "\n",
      "        [[ 0.1274]],\n",
      "\n",
      "        [[ 0.0148]],\n",
      "\n",
      "        [[ 0.0267]],\n",
      "\n",
      "        [[ 0.1343]],\n",
      "\n",
      "        [[ 0.0484]],\n",
      "\n",
      "        [[ 0.1526]],\n",
      "\n",
      "        [[ 0.0964]],\n",
      "\n",
      "        [[ 0.0711]],\n",
      "\n",
      "        [[ 0.0617]],\n",
      "\n",
      "        [[ 0.1333]],\n",
      "\n",
      "        [[ 0.0449]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1002]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0528]],\n",
      "\n",
      "        [[ 0.1498]],\n",
      "\n",
      "        [[ 0.0946]],\n",
      "\n",
      "        [[ 0.0392]],\n",
      "\n",
      "        [[ 0.0652]],\n",
      "\n",
      "        [[ 0.0356]],\n",
      "\n",
      "        [[ 0.0531]],\n",
      "\n",
      "        [[ 0.0240]],\n",
      "\n",
      "        [[ 0.0470]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0885]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0703]],\n",
      "\n",
      "        [[ 0.0390]],\n",
      "\n",
      "        [[ 0.0360]],\n",
      "\n",
      "        [[ 0.0517]],\n",
      "\n",
      "        [[ 0.0638]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0864]],\n",
      "\n",
      "        [[ 0.1041]],\n",
      "\n",
      "        [[ 0.0567]],\n",
      "\n",
      "        [[ 0.0377]],\n",
      "\n",
      "        [[ 0.0296]],\n",
      "\n",
      "        [[ 0.0306]],\n",
      "\n",
      "        [[ 0.0417]],\n",
      "\n",
      "        [[ 0.1580]],\n",
      "\n",
      "        [[ 0.0706]],\n",
      "\n",
      "        [[ 0.0516]],\n",
      "\n",
      "        [[ 0.0960]],\n",
      "\n",
      "        [[ 0.0604]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.0969]],\n",
      "\n",
      "        [[ 0.0390]],\n",
      "\n",
      "        [[ 0.0698]],\n",
      "\n",
      "        [[ 0.0381]],\n",
      "\n",
      "        [[ 0.0336]],\n",
      "\n",
      "        [[ 0.0441]],\n",
      "\n",
      "        [[ 0.0141]],\n",
      "\n",
      "        [[ 0.1245]],\n",
      "\n",
      "        [[ 0.1636]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.0636]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0100]],\n",
      "\n",
      "        [[ 0.0360]],\n",
      "\n",
      "        [[ 0.0592]],\n",
      "\n",
      "        [[ 0.0532]],\n",
      "\n",
      "        [[ 0.0204]],\n",
      "\n",
      "        [[ 0.0499]],\n",
      "\n",
      "        [[ 0.0101]],\n",
      "\n",
      "        [[ 0.1206]],\n",
      "\n",
      "        [[ 0.0381]],\n",
      "\n",
      "        [[ 0.0400]],\n",
      "\n",
      "        [[ 0.0392]],\n",
      "\n",
      "        [[ 0.0344]],\n",
      "\n",
      "        [[ 0.0583]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0422]],\n",
      "\n",
      "        [[ 0.0707]],\n",
      "\n",
      "        [[ 0.0474]],\n",
      "\n",
      "        [[ 0.0997]],\n",
      "\n",
      "        [[ 0.0531]],\n",
      "\n",
      "        [[ 0.0518]],\n",
      "\n",
      "        [[ 0.0958]],\n",
      "\n",
      "        [[ 0.0343]],\n",
      "\n",
      "        [[ 0.0522]],\n",
      "\n",
      "        [[ 0.1356]],\n",
      "\n",
      "        [[ 0.0448]],\n",
      "\n",
      "        [[ 0.0621]],\n",
      "\n",
      "        [[ 0.0537]],\n",
      "\n",
      "        [[ 0.0407]],\n",
      "\n",
      "        [[ 0.0205]],\n",
      "\n",
      "        [[ 0.0456]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.1449]],\n",
      "\n",
      "        [[ 0.0419]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0548]],\n",
      "\n",
      "        [[ 0.0643]],\n",
      "\n",
      "        [[ 0.1067]],\n",
      "\n",
      "        [[ 0.0328]],\n",
      "\n",
      "        [[ 0.0235]],\n",
      "\n",
      "        [[ 0.0394]],\n",
      "\n",
      "        [[ 0.0427]],\n",
      "\n",
      "        [[ 0.1527]],\n",
      "\n",
      "        [[ 0.0451]],\n",
      "\n",
      "        [[ 0.0500]],\n",
      "\n",
      "        [[ 0.0161]],\n",
      "\n",
      "        [[ 0.0610]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0446]],\n",
      "\n",
      "        [[ 0.0382]],\n",
      "\n",
      "        [[ 0.0433]],\n",
      "\n",
      "        [[ 0.0369]],\n",
      "\n",
      "        [[ 0.0566]],\n",
      "\n",
      "        [[ 0.1329]],\n",
      "\n",
      "        [[ 0.0564]],\n",
      "\n",
      "        [[ 0.0287]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0783]],\n",
      "\n",
      "        [[ 0.0649]],\n",
      "\n",
      "        [[ 0.0286]],\n",
      "\n",
      "        [[ 0.0132]],\n",
      "\n",
      "        [[ 0.0509]],\n",
      "\n",
      "        [[ 0.0467]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.1243]],\n",
      "\n",
      "        [[ 0.0547]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.0319]],\n",
      "\n",
      "        [[ 0.0766]],\n",
      "\n",
      "        [[ 0.0209]],\n",
      "\n",
      "        [[ 0.0586]],\n",
      "\n",
      "        [[ 0.1589]],\n",
      "\n",
      "        [[ 0.0659]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0357]],\n",
      "\n",
      "        [[ 0.0392]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0601]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0303]],\n",
      "\n",
      "        [[ 0.0311]],\n",
      "\n",
      "        [[ 0.0346]],\n",
      "\n",
      "        [[ 0.0198]],\n",
      "\n",
      "        [[ 0.0174]],\n",
      "\n",
      "        [[ 0.0350]],\n",
      "\n",
      "        [[ 0.0533]],\n",
      "\n",
      "        [[ 0.0634]],\n",
      "\n",
      "        [[ 0.0335]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0628]],\n",
      "\n",
      "        [[ 0.1136]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.1287]],\n",
      "\n",
      "        [[ 0.0369]],\n",
      "\n",
      "        [[ 0.0418]],\n",
      "\n",
      "        [[ 0.1341]],\n",
      "\n",
      "        [[ 0.0081]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.0313]],\n",
      "\n",
      "        [[ 0.1423]],\n",
      "\n",
      "        [[ 0.0484]],\n",
      "\n",
      "        [[ 0.0363]],\n",
      "\n",
      "        [[ 0.0508]],\n",
      "\n",
      "        [[ 0.0290]],\n",
      "\n",
      "        [[ 0.0405]],\n",
      "\n",
      "        [[ 0.1669]],\n",
      "\n",
      "        [[ 0.0482]],\n",
      "\n",
      "        [[ 0.0353]],\n",
      "\n",
      "        [[ 0.1293]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0402]],\n",
      "\n",
      "        [[ 0.1280]],\n",
      "\n",
      "        [[ 0.0298]],\n",
      "\n",
      "        [[ 0.0486]],\n",
      "\n",
      "        [[ 0.0564]],\n",
      "\n",
      "        [[ 0.0400]],\n",
      "\n",
      "        [[ 0.0450]],\n",
      "\n",
      "        [[ 0.0226]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0245]],\n",
      "\n",
      "        [[ 0.1203]],\n",
      "\n",
      "        [[ 0.0164]],\n",
      "\n",
      "        [[ 0.0242]],\n",
      "\n",
      "        [[ 0.0358]],\n",
      "\n",
      "        [[ 0.0305]],\n",
      "\n",
      "        [[ 0.0353]],\n",
      "\n",
      "        [[ 0.0178]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1408]],\n",
      "\n",
      "        [[ 0.0420]],\n",
      "\n",
      "        [[ 0.0533]],\n",
      "\n",
      "        [[ 0.0595]],\n",
      "\n",
      "        [[ 0.0784]],\n",
      "\n",
      "        [[ 0.1180]],\n",
      "\n",
      "        [[ 0.1442]],\n",
      "\n",
      "        [[ 0.0543]],\n",
      "\n",
      "        [[ 0.0470]],\n",
      "\n",
      "        [[ 0.1420]],\n",
      "\n",
      "        [[ 0.1291]],\n",
      "\n",
      "        [[ 0.0377]],\n",
      "\n",
      "        [[ 0.1024]],\n",
      "\n",
      "        [[ 0.0558]],\n",
      "\n",
      "        [[ 0.1053]],\n",
      "\n",
      "        [[ 0.1236]],\n",
      "\n",
      "        [[ 0.0390]],\n",
      "\n",
      "        [[ 0.0748]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.0637]],\n",
      "\n",
      "        [[ 0.1237]],\n",
      "\n",
      "        [[ 0.1154]],\n",
      "\n",
      "        [[ 0.0459]],\n",
      "\n",
      "        [[ 0.0363]],\n",
      "\n",
      "        [[ 0.0501]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0268]],\n",
      "\n",
      "        [[ 0.1182]],\n",
      "\n",
      "        [[ 0.0215]],\n",
      "\n",
      "        [[ 0.0395]],\n",
      "\n",
      "        [[ 0.0217]],\n",
      "\n",
      "        [[ 0.0169]],\n",
      "\n",
      "        [[ 0.1026]],\n",
      "\n",
      "        [[ 0.0208]],\n",
      "\n",
      "        [[ 0.0260]],\n",
      "\n",
      "        [[ 0.1493]],\n",
      "\n",
      "        [[ 0.0564]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0307]],\n",
      "\n",
      "        [[ 0.1290]],\n",
      "\n",
      "        [[ 0.1153]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0326]],\n",
      "\n",
      "        [[ 0.0493]],\n",
      "\n",
      "        [[ 0.0563]],\n",
      "\n",
      "        [[ 0.1436]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0528]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0356]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1425]],\n",
      "\n",
      "        [[ 0.0512]],\n",
      "\n",
      "        [[ 0.1144]],\n",
      "\n",
      "        [[ 0.0224]],\n",
      "\n",
      "        [[ 0.0437]],\n",
      "\n",
      "        [[ 0.0363]],\n",
      "\n",
      "        [[ 0.0037]],\n",
      "\n",
      "        [[ 0.0724]],\n",
      "\n",
      "        [[ 0.1349]],\n",
      "\n",
      "        [[ 0.0026]],\n",
      "\n",
      "        [[ 0.0128]],\n",
      "\n",
      "        [[ 0.0311]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.1087]],\n",
      "\n",
      "        [[ 0.0226]],\n",
      "\n",
      "        [[ 0.0853]],\n",
      "\n",
      "        [[ 0.0217]],\n",
      "\n",
      "        [[ 0.0413]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0484]],\n",
      "\n",
      "        [[ 0.0482]],\n",
      "\n",
      "        [[ 0.0120]],\n",
      "\n",
      "        [[ 0.0415]],\n",
      "\n",
      "        [[ 0.0297]],\n",
      "\n",
      "        [[ 0.0399]],\n",
      "\n",
      "        [[ 0.0164]],\n",
      "\n",
      "        [[ 0.0533]],\n",
      "\n",
      "        [[ 0.0297]],\n",
      "\n",
      "        [[ 0.0511]],\n",
      "\n",
      "        [[ 0.0680]],\n",
      "\n",
      "        [[ 0.0564]],\n",
      "\n",
      "        [[ 0.0051]],\n",
      "\n",
      "        [[ 0.1168]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0876]],\n",
      "\n",
      "        [[ 0.1268]],\n",
      "\n",
      "        [[ 0.0213]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0377]],\n",
      "\n",
      "        [[ 0.0580]],\n",
      "\n",
      "        [[ 0.0303]],\n",
      "\n",
      "        [[ 0.0337]],\n",
      "\n",
      "        [[ 0.0999]],\n",
      "\n",
      "        [[ 0.0013]],\n",
      "\n",
      "        [[ 0.0888]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1168]],\n",
      "\n",
      "        [[ 0.0599]],\n",
      "\n",
      "        [[ 0.0378]],\n",
      "\n",
      "        [[ 0.0564]],\n",
      "\n",
      "        [[ 0.0362]],\n",
      "\n",
      "        [[ 0.0560]],\n",
      "\n",
      "        [[ 0.0417]],\n",
      "\n",
      "        [[ 0.0459]],\n",
      "\n",
      "        [[ 0.0526]],\n",
      "\n",
      "        [[ 0.0559]],\n",
      "\n",
      "        [[ 0.0629]],\n",
      "\n",
      "        [[ 0.1501]],\n",
      "\n",
      "        [[ 0.0755]],\n",
      "\n",
      "        [[ 0.1565]],\n",
      "\n",
      "        [[ 0.1656]],\n",
      "\n",
      "        [[ 0.0787]],\n",
      "\n",
      "        [[ 0.1377]],\n",
      "\n",
      "        [[ 0.0518]],\n",
      "\n",
      "        [[ 0.0409]],\n",
      "\n",
      "        [[ 0.0478]],\n",
      "\n",
      "        [[ 0.1238]],\n",
      "\n",
      "        [[ 0.0547]],\n",
      "\n",
      "        [[ 0.0573]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.1385]],\n",
      "\n",
      "        [[ 0.0597]],\n",
      "\n",
      "        [[ 0.0191]],\n",
      "\n",
      "        [[ 0.0446]],\n",
      "\n",
      "        [[ 0.0445]],\n",
      "\n",
      "        [[ 0.0555]],\n",
      "\n",
      "        [[ 0.0555]],\n",
      "\n",
      "        [[ 0.0100]],\n",
      "\n",
      "        [[ 0.0130]],\n",
      "\n",
      "        [[ 0.0338]],\n",
      "\n",
      "        [[ 0.0136]],\n",
      "\n",
      "        [[ 0.1086]],\n",
      "\n",
      "        [[ 0.1182]],\n",
      "\n",
      "        [[ 0.0518]],\n",
      "\n",
      "        [[ 0.0894]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0352]],\n",
      "\n",
      "        [[ 0.1590]],\n",
      "\n",
      "        [[ 0.0382]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.1629]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.0313]],\n",
      "\n",
      "        [[ 0.0509]],\n",
      "\n",
      "        [[ 0.0441]],\n",
      "\n",
      "        [[ 0.0477]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0605]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0214]],\n",
      "\n",
      "        [[ 0.1479]],\n",
      "\n",
      "        [[ 0.0638]],\n",
      "\n",
      "        [[ 0.1586]],\n",
      "\n",
      "        [[ 0.1699]],\n",
      "\n",
      "        [[ 0.0957]],\n",
      "\n",
      "        [[ 0.0319]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0152]],\n",
      "\n",
      "        [[ 0.0544]],\n",
      "\n",
      "        [[ 0.1318]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0394]],\n",
      "\n",
      "        [[ 0.1489]],\n",
      "\n",
      "        [[ 0.0680]],\n",
      "\n",
      "        [[ 0.0512]],\n",
      "\n",
      "        [[ 0.1047]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.1298]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.1286]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0440]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0367]],\n",
      "\n",
      "        [[ 0.0948]],\n",
      "\n",
      "        [[ 0.0365]],\n",
      "\n",
      "        [[ 0.0107]],\n",
      "\n",
      "        [[ 0.0162]],\n",
      "\n",
      "        [[ 0.0303]],\n",
      "\n",
      "        [[ 0.0901]],\n",
      "\n",
      "        [[ 0.0262]],\n",
      "\n",
      "        [[ 0.0501]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0063]],\n",
      "\n",
      "        [[ 0.0461]],\n",
      "\n",
      "        [[ 0.0201]],\n",
      "\n",
      "        [[ 0.1134]],\n",
      "\n",
      "        [[ 0.0360]],\n",
      "\n",
      "        [[ 0.0395]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.1649]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0383]],\n",
      "\n",
      "        [[ 0.1202]],\n",
      "\n",
      "        [[ 0.0204]],\n",
      "\n",
      "        [[ 0.0326]],\n",
      "\n",
      "        [[ 0.0106]],\n",
      "\n",
      "        [[ 0.0545]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.0478]],\n",
      "\n",
      "        [[ 0.1282]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.1538]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0418]],\n",
      "\n",
      "        [[ 0.0726]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0526]],\n",
      "\n",
      "        [[ 0.0788]],\n",
      "\n",
      "        [[ 0.0795]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0382]],\n",
      "\n",
      "        [[ 0.0093]],\n",
      "\n",
      "        [[ 0.0248]],\n",
      "\n",
      "        [[ 0.0494]],\n",
      "\n",
      "        [[ 0.0178]],\n",
      "\n",
      "        [[ 0.0496]],\n",
      "\n",
      "        [[ 0.0212]],\n",
      "\n",
      "        [[ 0.0102]],\n",
      "\n",
      "        [[ 0.0757]],\n",
      "\n",
      "        [[ 0.0370]],\n",
      "\n",
      "        [[ 0.0381]],\n",
      "\n",
      "        [[ 0.0515]],\n",
      "\n",
      "        [[ 0.0441]],\n",
      "\n",
      "        [[ 0.0498]],\n",
      "\n",
      "        [[ 0.0441]],\n",
      "\n",
      "        [[ 0.0260]],\n",
      "\n",
      "        [[ 0.0355]],\n",
      "\n",
      "        [[ 0.0317]],\n",
      "\n",
      "        [[ 0.0171]],\n",
      "\n",
      "        [[ 0.0551]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.1245]],\n",
      "\n",
      "        [[ 0.1359]],\n",
      "\n",
      "        [[ 0.0656]],\n",
      "\n",
      "        [[ 0.0505]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1146]],\n",
      "\n",
      "        [[ 0.0390]],\n",
      "\n",
      "        [[ 0.0433]],\n",
      "\n",
      "        [[ 0.0797]],\n",
      "\n",
      "        [[ 0.1198]],\n",
      "\n",
      "        [[ 0.1110]],\n",
      "\n",
      "        [[ 0.0582]],\n",
      "\n",
      "        [[ 0.0517]],\n",
      "\n",
      "        [[ 0.0394]],\n",
      "\n",
      "        [[ 0.1282]],\n",
      "\n",
      "        [[ 0.0451]],\n",
      "\n",
      "        [[ 0.0371]],\n",
      "\n",
      "        [[ 0.1317]],\n",
      "\n",
      "        [[ 0.0801]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0104]],\n",
      "\n",
      "        [[ 0.0361]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0760]],\n",
      "\n",
      "        [[ 0.0373]],\n",
      "\n",
      "        [[ 0.1105]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 5.8532]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0041, -0.0618,  0.0515,  ..., -0.0224, -0.0135,  0.0175]],\n",
      "\n",
      "        [[ 0.0121, -0.0462,  0.0364,  ...,  0.0271, -0.0601,  0.0569]],\n",
      "\n",
      "        [[ 0.0050,  0.0560,  0.0404,  ..., -0.0318, -0.0326, -0.0055]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0456, -0.0614, -0.0630,  ...,  0.0324, -0.0062, -0.0503]],\n",
      "\n",
      "        [[ 0.0261, -0.0106,  0.0205,  ...,  0.0462, -0.0180, -0.0264]],\n",
      "\n",
      "        [[ 0.0161, -0.0477, -0.0072,  ...,  0.0614,  0.0216, -0.0158]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor([[[-4.1349e-04, -1.1848e-03, -2.1189e-03,  ..., -1.0285e-03,\n",
      "          -1.4353e-03, -1.5019e-03],\n",
      "         [ 1.2727e-03,  2.1337e-03,  1.4515e-03,  ...,  2.0569e-03,\n",
      "           2.5002e-03,  1.8460e-03],\n",
      "         [ 2.0112e-03,  1.2542e-03,  5.4500e-04,  ..., -1.2994e-03,\n",
      "          -1.8682e-03, -2.9061e-03],\n",
      "         ...,\n",
      "         [ 5.8752e-04,  9.0992e-04,  2.0462e-03,  ...,  1.3429e-03,\n",
      "           1.9067e-03,  1.9555e-03],\n",
      "         [ 1.2290e-03,  8.2826e-04,  4.8392e-04,  ..., -5.8864e-04,\n",
      "           2.0298e-04, -8.6321e-04],\n",
      "         [-8.9165e-04, -1.8761e-03, -1.6657e-03,  ..., -9.9456e-04,\n",
      "           1.2349e-04,  7.2438e-04]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37 [17920/50000 (40%)]\tLoss: 0.287548, Accuracy: 90.23\n",
      "Train Epoch: 37 [20480/50000 (45%)]\tLoss: 0.276140, Accuracy: 90.62\n",
      "Train Epoch: 37 [23040/50000 (51%)]\tLoss: 0.278601, Accuracy: 91.21\n",
      "Train Epoch: 37 [25600/50000 (57%)]\tLoss: 0.275688, Accuracy: 90.23\n",
      "Train Epoch: 37 [28160/50000 (62%)]\tLoss: 0.278951, Accuracy: 90.43\n",
      "Train Epoch: 37 [30720/50000 (68%)]\tLoss: 0.246836, Accuracy: 91.80\n",
      "Train Epoch: 37 [33280/50000 (74%)]\tLoss: 0.310609, Accuracy: 89.06\n",
      "Train Epoch: 37 [35840/50000 (80%)]\tLoss: 0.249429, Accuracy: 90.82\n",
      "Train Epoch: 37 [38400/50000 (85%)]\tLoss: 0.301456, Accuracy: 88.67\n",
      "Train Epoch: 37 [40960/50000 (91%)]\tLoss: 0.265713, Accuracy: 89.06\n",
      "Train Epoch: 37 [43520/50000 (97%)]\tLoss: 0.338722, Accuracy: 87.70\n",
      "\n",
      "Validation set: Average loss: 0.6814, Accuracy: 3955/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[39.496328592300415 s]\n",
      "Train Epoch: 38 [0/50000 (0%)]\tLoss: 0.237670, Accuracy: 90.82\n",
      "Train Epoch: 38 [2560/50000 (6%)]\tLoss: 0.220158, Accuracy: 92.58\n",
      "Train Epoch: 38 [5120/50000 (11%)]\tLoss: 0.218641, Accuracy: 92.77\n",
      "Train Epoch: 38 [7680/50000 (17%)]\tLoss: 0.318575, Accuracy: 90.43\n",
      "Train Epoch: 38 [10240/50000 (23%)]\tLoss: 0.201664, Accuracy: 93.36\n",
      "Train Epoch: 38 [12800/50000 (28%)]\tLoss: 0.253501, Accuracy: 91.41\n",
      "Train Epoch: 38 [15360/50000 (34%)]\tLoss: 0.197424, Accuracy: 93.36\n",
      "Train Epoch: 38 [17920/50000 (40%)]\tLoss: 0.261448, Accuracy: 90.04\n",
      "Train Epoch: 38 [20480/50000 (45%)]\tLoss: 0.318899, Accuracy: 89.06\n",
      "Train Epoch: 38 [23040/50000 (51%)]\tLoss: 0.233937, Accuracy: 91.41\n",
      "Train Epoch: 38 [25600/50000 (57%)]\tLoss: 0.290114, Accuracy: 89.26\n",
      "Train Epoch: 38 [28160/50000 (62%)]\tLoss: 0.246129, Accuracy: 92.19\n",
      "Train Epoch: 38 [30720/50000 (68%)]\tLoss: 0.317330, Accuracy: 89.06\n",
      "Train Epoch: 38 [33280/50000 (74%)]\tLoss: 0.344206, Accuracy: 87.50\n",
      "Train Epoch: 38 [35840/50000 (80%)]\tLoss: 0.271098, Accuracy: 91.41\n",
      "Train Epoch: 38 [38400/50000 (85%)]\tLoss: 0.295358, Accuracy: 89.26\n",
      "Train Epoch: 38 [40960/50000 (91%)]\tLoss: 0.290729, Accuracy: 91.02\n",
      "Train Epoch: 38 [43520/50000 (97%)]\tLoss: 0.341539, Accuracy: 88.28\n",
      "\n",
      "Validation set: Average loss: 0.5005, Accuracy: 4176/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.335601806640625 s]\n",
      "\n",
      "Test set: Average loss: 0.5132, Accuracy: 8376/10000 (83.76%)\n",
      "\n",
      "Train Epoch: 39 [0/50000 (0%)]\tLoss: 0.292303, Accuracy: 90.43\n",
      "Train Epoch: 39 [2560/50000 (6%)]\tLoss: 0.257550, Accuracy: 91.80\n",
      "Train Epoch: 39 [5120/50000 (11%)]\tLoss: 0.265626, Accuracy: 91.02\n",
      "Train Epoch: 39 [7680/50000 (17%)]\tLoss: 0.292413, Accuracy: 90.23\n",
      "Train Epoch: 39 [10240/50000 (23%)]\tLoss: 0.287454, Accuracy: 89.84\n",
      "Train Epoch: 39 [12800/50000 (28%)]\tLoss: 0.242392, Accuracy: 91.21\n",
      "Train Epoch: 39 [15360/50000 (34%)]\tLoss: 0.287673, Accuracy: 89.65\n",
      "Train Epoch: 39 [17920/50000 (40%)]\tLoss: 0.237015, Accuracy: 90.82\n",
      "Train Epoch: 39 [20480/50000 (45%)]\tLoss: 0.289186, Accuracy: 90.43\n",
      "Train Epoch: 39 [23040/50000 (51%)]\tLoss: 0.266806, Accuracy: 91.41\n",
      "Train Epoch: 39 [25600/50000 (57%)]\tLoss: 0.249415, Accuracy: 90.82\n",
      "Train Epoch: 39 [28160/50000 (62%)]\tLoss: 0.277528, Accuracy: 91.02\n",
      "Train Epoch: 39 [30720/50000 (68%)]\tLoss: 0.255814, Accuracy: 91.60\n",
      "Train Epoch: 39 [33280/50000 (74%)]\tLoss: 0.241180, Accuracy: 91.80\n",
      "Train Epoch: 39 [35840/50000 (80%)]\tLoss: 0.242120, Accuracy: 91.99\n",
      "Train Epoch: 39 [38400/50000 (85%)]\tLoss: 0.271211, Accuracy: 91.02\n",
      "Train Epoch: 39 [40960/50000 (91%)]\tLoss: 0.281212, Accuracy: 90.23\n",
      "Train Epoch: 39 [43520/50000 (97%)]\tLoss: 0.303144, Accuracy: 90.43\n",
      "\n",
      "Validation set: Average loss: 0.5111, Accuracy: 4183/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.605881452560425 s]\n",
      "Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.276955, Accuracy: 90.04\n",
      "Train Epoch: 40 [2560/50000 (6%)]\tLoss: 0.226305, Accuracy: 92.77\n",
      "Train Epoch: 40 [5120/50000 (11%)]\tLoss: 0.261607, Accuracy: 91.41\n",
      "Train Epoch: 40 [7680/50000 (17%)]\tLoss: 0.218800, Accuracy: 92.77\n",
      "Train Epoch: 40 [10240/50000 (23%)]\tLoss: 0.277026, Accuracy: 90.43\n",
      "Train Epoch: 40 [12800/50000 (28%)]\tLoss: 0.209249, Accuracy: 92.97\n",
      "Train Epoch: 40 [15360/50000 (34%)]\tLoss: 0.240302, Accuracy: 92.19\n",
      "Train Epoch: 40 [17920/50000 (40%)]\tLoss: 0.248846, Accuracy: 91.60\n",
      "Train Epoch: 40 [20480/50000 (45%)]\tLoss: 0.260091, Accuracy: 91.99\n",
      "Train Epoch: 40 [23040/50000 (51%)]\tLoss: 0.261884, Accuracy: 89.84\n",
      "Train Epoch: 40 [25600/50000 (57%)]\tLoss: 0.283423, Accuracy: 89.06\n",
      "Train Epoch: 40 [28160/50000 (62%)]\tLoss: 0.269808, Accuracy: 90.82\n",
      "Train Epoch: 40 [30720/50000 (68%)]\tLoss: 0.306117, Accuracy: 88.67\n",
      "Train Epoch: 40 [33280/50000 (74%)]\tLoss: 0.328554, Accuracy: 89.06\n",
      "Train Epoch: 40 [35840/50000 (80%)]\tLoss: 0.250995, Accuracy: 91.02\n",
      "Train Epoch: 40 [38400/50000 (85%)]\tLoss: 0.291495, Accuracy: 89.45\n",
      "Train Epoch: 40 [40960/50000 (91%)]\tLoss: 0.243928, Accuracy: 92.97\n",
      "Train Epoch: 40 [43520/50000 (97%)]\tLoss: 0.246702, Accuracy: 91.60\n",
      "\n",
      "Validation set: Average loss: 0.6382, Accuracy: 4038/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.417503356933594 s]\n",
      "\n",
      "Test set: Average loss: 0.6599, Accuracy: 8019/10000 (80.19%)\n",
      "\n",
      "Train Epoch: 41 [0/50000 (0%)]\tLoss: 0.234574, Accuracy: 91.41\n",
      "Train Epoch: 41 [2560/50000 (6%)]\tLoss: 0.255158, Accuracy: 92.19\n",
      "Train Epoch: 41 [5120/50000 (11%)]\tLoss: 0.207461, Accuracy: 93.95\n",
      "Train Epoch: 41 [7680/50000 (17%)]\tLoss: 0.218984, Accuracy: 91.99\n",
      "Train Epoch: 41 [10240/50000 (23%)]\tLoss: 0.272516, Accuracy: 91.21\n",
      "Train Epoch: 41 [12800/50000 (28%)]\tLoss: 0.249948, Accuracy: 91.60\n",
      "Train Epoch: 41 [15360/50000 (34%)]\tLoss: 0.276835, Accuracy: 90.23\n",
      "Train Epoch: 41 [17920/50000 (40%)]\tLoss: 0.320591, Accuracy: 90.04\n",
      "Train Epoch: 41 [20480/50000 (45%)]\tLoss: 0.278747, Accuracy: 90.62\n",
      "Train Epoch: 41 [23040/50000 (51%)]\tLoss: 0.298740, Accuracy: 90.04\n",
      "Train Epoch: 41 [25600/50000 (57%)]\tLoss: 0.279169, Accuracy: 90.43\n",
      "Train Epoch: 41 [28160/50000 (62%)]\tLoss: 0.354593, Accuracy: 89.26\n",
      "Train Epoch: 41 [30720/50000 (68%)]\tLoss: 0.278829, Accuracy: 89.45\n",
      "Train Epoch: 41 [33280/50000 (74%)]\tLoss: 0.301908, Accuracy: 87.70\n",
      "Train Epoch: 41 [35840/50000 (80%)]\tLoss: 0.305239, Accuracy: 89.26\n",
      "Train Epoch: 41 [38400/50000 (85%)]\tLoss: 0.332394, Accuracy: 88.48\n",
      "Train Epoch: 41 [40960/50000 (91%)]\tLoss: 0.242621, Accuracy: 92.97\n",
      "Train Epoch: 41 [43520/50000 (97%)]\tLoss: 0.295773, Accuracy: 90.82\n",
      "\n",
      "Validation set: Average loss: 0.6304, Accuracy: 4023/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[38.91997289657593 s]\n",
      "Train Epoch: 42 [0/50000 (0%)]\tLoss: 0.287798, Accuracy: 89.65\n",
      "Train Epoch: 42 [2560/50000 (6%)]\tLoss: 0.238688, Accuracy: 92.19\n",
      "Train Epoch: 42 [5120/50000 (11%)]\tLoss: 0.249499, Accuracy: 91.41\n",
      "Train Epoch: 42 [7680/50000 (17%)]\tLoss: 0.329137, Accuracy: 87.30\n",
      "Train Epoch: 42 [10240/50000 (23%)]\tLoss: 0.239761, Accuracy: 91.02\n",
      "Train Epoch: 42 [12800/50000 (28%)]\tLoss: 0.268655, Accuracy: 91.02\n",
      "Train Epoch: 42 [15360/50000 (34%)]\tLoss: 0.290892, Accuracy: 90.62\n",
      "Train Epoch: 42 [17920/50000 (40%)]\tLoss: 0.213671, Accuracy: 92.97\n",
      "Train Epoch: 42 [20480/50000 (45%)]\tLoss: 0.269353, Accuracy: 91.21\n",
      "Train Epoch: 42 [23040/50000 (51%)]\tLoss: 0.238966, Accuracy: 90.82\n",
      "Train Epoch: 42 [25600/50000 (57%)]\tLoss: 0.246890, Accuracy: 90.62\n",
      "Train Epoch: 42 [28160/50000 (62%)]\tLoss: 0.332016, Accuracy: 89.26\n",
      "Train Epoch: 42 [30720/50000 (68%)]\tLoss: 0.283736, Accuracy: 88.67\n",
      "Train Epoch: 42 [33280/50000 (74%)]\tLoss: 0.313671, Accuracy: 90.23\n",
      "Train Epoch: 42 [35840/50000 (80%)]\tLoss: 0.272464, Accuracy: 91.80\n",
      "Train Epoch: 42 [38400/50000 (85%)]\tLoss: 0.325812, Accuracy: 87.50\n",
      "Train Epoch: 42 [40960/50000 (91%)]\tLoss: 0.267672, Accuracy: 90.62\n",
      "Train Epoch: 42 [43520/50000 (97%)]\tLoss: 0.289491, Accuracy: 91.21\n",
      "\n",
      "Validation set: Average loss: 0.5274, Accuracy: 4097/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.27731966972351 s]\n",
      "\n",
      "Test set: Average loss: 0.5557, Accuracy: 8155/10000 (81.55%)\n",
      "\n",
      "Train Epoch: 43 [0/50000 (0%)]\tLoss: 0.224810, Accuracy: 92.58\n",
      "Train Epoch: 43 [2560/50000 (6%)]\tLoss: 0.308653, Accuracy: 90.43\n",
      "Train Epoch: 43 [5120/50000 (11%)]\tLoss: 0.262926, Accuracy: 90.43\n",
      "Train Epoch: 43 [7680/50000 (17%)]\tLoss: 0.271158, Accuracy: 90.04\n",
      "Train Epoch: 43 [10240/50000 (23%)]\tLoss: 0.233133, Accuracy: 93.36\n",
      "Train Epoch: 43 [12800/50000 (28%)]\tLoss: 0.317804, Accuracy: 89.84\n",
      "Train Epoch: 43 [15360/50000 (34%)]\tLoss: 0.317175, Accuracy: 90.04\n",
      "Train Epoch: 43 [17920/50000 (40%)]\tLoss: 0.261682, Accuracy: 91.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 43 [20480/50000 (45%)]\tLoss: 0.230724, Accuracy: 92.58\n",
      "Train Epoch: 43 [23040/50000 (51%)]\tLoss: 0.240627, Accuracy: 91.02\n",
      "Train Epoch: 43 [25600/50000 (57%)]\tLoss: 0.221810, Accuracy: 92.77\n",
      "Train Epoch: 43 [28160/50000 (62%)]\tLoss: 0.261164, Accuracy: 90.23\n",
      "Train Epoch: 43 [30720/50000 (68%)]\tLoss: 0.292547, Accuracy: 91.41\n",
      "Train Epoch: 43 [33280/50000 (74%)]\tLoss: 0.263243, Accuracy: 91.80\n",
      "Train Epoch: 43 [35840/50000 (80%)]\tLoss: 0.236711, Accuracy: 91.21\n",
      "Train Epoch: 43 [38400/50000 (85%)]\tLoss: 0.234784, Accuracy: 90.62\n",
      "Train Epoch: 43 [40960/50000 (91%)]\tLoss: 0.237057, Accuracy: 92.58\n",
      "Train Epoch: 43 [43520/50000 (97%)]\tLoss: 0.251332, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.4671, Accuracy: 4262/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[39.25848913192749 s]\n",
      "Train Epoch: 44 [0/50000 (0%)]\tLoss: 0.223752, Accuracy: 92.19\n",
      "Train Epoch: 44 [2560/50000 (6%)]\tLoss: 0.218995, Accuracy: 91.99\n",
      "Train Epoch: 44 [5120/50000 (11%)]\tLoss: 0.230334, Accuracy: 92.97\n",
      "Train Epoch: 44 [7680/50000 (17%)]\tLoss: 0.283348, Accuracy: 90.43\n",
      "Train Epoch: 44 [10240/50000 (23%)]\tLoss: 0.231749, Accuracy: 91.99\n",
      "Train Epoch: 44 [12800/50000 (28%)]\tLoss: 0.235983, Accuracy: 91.21\n",
      "Train Epoch: 44 [15360/50000 (34%)]\tLoss: 0.248911, Accuracy: 90.43\n",
      "Train Epoch: 44 [17920/50000 (40%)]\tLoss: 0.244741, Accuracy: 91.41\n",
      "Train Epoch: 44 [20480/50000 (45%)]\tLoss: 0.283698, Accuracy: 89.84\n",
      "Train Epoch: 44 [23040/50000 (51%)]\tLoss: 0.234124, Accuracy: 91.02\n",
      "Train Epoch: 44 [25600/50000 (57%)]\tLoss: 0.264025, Accuracy: 91.02\n",
      "Train Epoch: 44 [28160/50000 (62%)]\tLoss: 0.239515, Accuracy: 91.60\n",
      "Train Epoch: 44 [30720/50000 (68%)]\tLoss: 0.310891, Accuracy: 88.87\n",
      "Train Epoch: 44 [33280/50000 (74%)]\tLoss: 0.324550, Accuracy: 89.06\n",
      "Train Epoch: 44 [35840/50000 (80%)]\tLoss: 0.299089, Accuracy: 90.62\n",
      "Train Epoch: 44 [38400/50000 (85%)]\tLoss: 0.224589, Accuracy: 91.60\n",
      "Train Epoch: 44 [40960/50000 (91%)]\tLoss: 0.303202, Accuracy: 89.26\n",
      "Train Epoch: 44 [43520/50000 (97%)]\tLoss: 0.267858, Accuracy: 89.65\n",
      "\n",
      "Validation set: Average loss: 0.9577, Accuracy: 3651/5000 (73.00%)\n",
      "\n",
      "the time of this epoch:[36.22310662269592 s]\n",
      "\n",
      "Test set: Average loss: 0.9502, Accuracy: 7242/10000 (72.42%)\n",
      "\n",
      "Train Epoch: 45 [0/50000 (0%)]\tLoss: 0.293522, Accuracy: 91.80\n",
      "Train Epoch: 45 [2560/50000 (6%)]\tLoss: 0.255665, Accuracy: 90.82\n",
      "Train Epoch: 45 [5120/50000 (11%)]\tLoss: 0.189343, Accuracy: 93.95\n",
      "Train Epoch: 45 [7680/50000 (17%)]\tLoss: 0.209021, Accuracy: 92.38\n",
      "Train Epoch: 45 [10240/50000 (23%)]\tLoss: 0.240311, Accuracy: 92.97\n",
      "Train Epoch: 45 [12800/50000 (28%)]\tLoss: 0.270900, Accuracy: 90.04\n",
      "Train Epoch: 45 [15360/50000 (34%)]\tLoss: 0.246831, Accuracy: 90.82\n",
      "Train Epoch: 45 [17920/50000 (40%)]\tLoss: 0.237188, Accuracy: 91.60\n",
      "Train Epoch: 45 [20480/50000 (45%)]\tLoss: 0.288197, Accuracy: 89.84\n",
      "Train Epoch: 45 [23040/50000 (51%)]\tLoss: 0.259248, Accuracy: 90.04\n",
      "Train Epoch: 45 [25600/50000 (57%)]\tLoss: 0.269922, Accuracy: 91.41\n",
      "Train Epoch: 45 [28160/50000 (62%)]\tLoss: 0.253334, Accuracy: 91.60\n",
      "Train Epoch: 45 [30720/50000 (68%)]\tLoss: 0.253627, Accuracy: 90.82\n",
      "Train Epoch: 45 [33280/50000 (74%)]\tLoss: 0.235060, Accuracy: 91.80\n",
      "Train Epoch: 45 [35840/50000 (80%)]\tLoss: 0.280788, Accuracy: 92.19\n",
      "Train Epoch: 45 [38400/50000 (85%)]\tLoss: 0.272856, Accuracy: 91.41\n",
      "Train Epoch: 45 [40960/50000 (91%)]\tLoss: 0.265852, Accuracy: 89.65\n",
      "Train Epoch: 45 [43520/50000 (97%)]\tLoss: 0.209637, Accuracy: 92.19\n",
      "\n",
      "Validation set: Average loss: 0.5824, Accuracy: 4130/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.41232872009277 s]\n",
      "Train Epoch: 46 [0/50000 (0%)]\tLoss: 0.180920, Accuracy: 94.92\n",
      "Train Epoch: 46 [2560/50000 (6%)]\tLoss: 0.210055, Accuracy: 92.77\n",
      "Train Epoch: 46 [5120/50000 (11%)]\tLoss: 0.286990, Accuracy: 90.43\n",
      "Train Epoch: 46 [7680/50000 (17%)]\tLoss: 0.241276, Accuracy: 92.38\n",
      "Train Epoch: 46 [10240/50000 (23%)]\tLoss: 0.261092, Accuracy: 90.62\n",
      "Train Epoch: 46 [12800/50000 (28%)]\tLoss: 0.242924, Accuracy: 90.62\n",
      "Train Epoch: 46 [15360/50000 (34%)]\tLoss: 0.201487, Accuracy: 92.97\n",
      "Train Epoch: 46 [17920/50000 (40%)]\tLoss: 0.273258, Accuracy: 90.82\n",
      "Train Epoch: 46 [20480/50000 (45%)]\tLoss: 0.245232, Accuracy: 90.43\n",
      "Train Epoch: 46 [23040/50000 (51%)]\tLoss: 0.237460, Accuracy: 90.82\n",
      "Train Epoch: 46 [25600/50000 (57%)]\tLoss: 0.253098, Accuracy: 91.80\n",
      "Train Epoch: 46 [28160/50000 (62%)]\tLoss: 0.270124, Accuracy: 91.99\n",
      "Train Epoch: 46 [30720/50000 (68%)]\tLoss: 0.270454, Accuracy: 90.82\n",
      "Train Epoch: 46 [33280/50000 (74%)]\tLoss: 0.249852, Accuracy: 91.80\n",
      "Train Epoch: 46 [35840/50000 (80%)]\tLoss: 0.313936, Accuracy: 89.65\n",
      "Train Epoch: 46 [38400/50000 (85%)]\tLoss: 0.234755, Accuracy: 91.99\n",
      "Train Epoch: 46 [40960/50000 (91%)]\tLoss: 0.225036, Accuracy: 93.55\n",
      "Train Epoch: 46 [43520/50000 (97%)]\tLoss: 0.249653, Accuracy: 91.60\n",
      "\n",
      "Validation set: Average loss: 0.6849, Accuracy: 4031/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.259464740753174 s]\n",
      "\n",
      "Test set: Average loss: 0.7040, Accuracy: 7983/10000 (79.83%)\n",
      "\n",
      "Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.285144, Accuracy: 90.04\n",
      "Train Epoch: 47 [2560/50000 (6%)]\tLoss: 0.351559, Accuracy: 87.70\n",
      "Train Epoch: 47 [5120/50000 (11%)]\tLoss: 0.330320, Accuracy: 89.26\n",
      "Train Epoch: 47 [7680/50000 (17%)]\tLoss: 0.244948, Accuracy: 91.02\n",
      "Train Epoch: 47 [10240/50000 (23%)]\tLoss: 0.260536, Accuracy: 90.62\n",
      "Train Epoch: 47 [12800/50000 (28%)]\tLoss: 0.210319, Accuracy: 91.60\n",
      "Train Epoch: 47 [15360/50000 (34%)]\tLoss: 0.239981, Accuracy: 92.19\n",
      "Train Epoch: 47 [17920/50000 (40%)]\tLoss: 0.251897, Accuracy: 90.23\n",
      "Train Epoch: 47 [20480/50000 (45%)]\tLoss: 0.261108, Accuracy: 90.82\n",
      "Train Epoch: 47 [23040/50000 (51%)]\tLoss: 0.279002, Accuracy: 90.04\n",
      "Train Epoch: 47 [25600/50000 (57%)]\tLoss: 0.271010, Accuracy: 90.82\n",
      "Train Epoch: 47 [28160/50000 (62%)]\tLoss: 0.330935, Accuracy: 89.45\n",
      "Train Epoch: 47 [30720/50000 (68%)]\tLoss: 0.255639, Accuracy: 90.62\n",
      "Train Epoch: 47 [33280/50000 (74%)]\tLoss: 0.237390, Accuracy: 92.58\n",
      "Train Epoch: 47 [35840/50000 (80%)]\tLoss: 0.252061, Accuracy: 90.23\n",
      "Train Epoch: 47 [38400/50000 (85%)]\tLoss: 0.261562, Accuracy: 90.62\n",
      "Train Epoch: 47 [40960/50000 (91%)]\tLoss: 0.222738, Accuracy: 92.19\n",
      "Train Epoch: 47 [43520/50000 (97%)]\tLoss: 0.203290, Accuracy: 92.19\n",
      "\n",
      "Validation set: Average loss: 1.0860, Accuracy: 3616/5000 (72.00%)\n",
      "\n",
      "the time of this epoch:[39.31718564033508 s]\n",
      "Train Epoch: 48 [0/50000 (0%)]\tLoss: 0.227377, Accuracy: 93.16\n",
      "Train Epoch: 48 [2560/50000 (6%)]\tLoss: 0.260480, Accuracy: 91.21\n",
      "Train Epoch: 48 [5120/50000 (11%)]\tLoss: 0.254601, Accuracy: 90.62\n",
      "Train Epoch: 48 [7680/50000 (17%)]\tLoss: 0.269152, Accuracy: 91.99\n",
      "Train Epoch: 48 [10240/50000 (23%)]\tLoss: 0.234974, Accuracy: 91.41\n",
      "Train Epoch: 48 [12800/50000 (28%)]\tLoss: 0.186649, Accuracy: 93.55\n",
      "Train Epoch: 48 [15360/50000 (34%)]\tLoss: 0.220122, Accuracy: 92.77\n",
      "Train Epoch: 48 [17920/50000 (40%)]\tLoss: 0.201935, Accuracy: 92.77\n",
      "Train Epoch: 48 [20480/50000 (45%)]\tLoss: 0.220013, Accuracy: 92.58\n",
      "Train Epoch: 48 [23040/50000 (51%)]\tLoss: 0.219909, Accuracy: 93.16\n",
      "Train Epoch: 48 [25600/50000 (57%)]\tLoss: 0.247487, Accuracy: 92.77\n",
      "Train Epoch: 48 [28160/50000 (62%)]\tLoss: 0.291315, Accuracy: 89.45\n",
      "Train Epoch: 48 [30720/50000 (68%)]\tLoss: 0.183353, Accuracy: 93.75\n",
      "Train Epoch: 48 [33280/50000 (74%)]\tLoss: 0.245869, Accuracy: 91.41\n",
      "Train Epoch: 48 [35840/50000 (80%)]\tLoss: 0.230739, Accuracy: 92.58\n",
      "Train Epoch: 48 [38400/50000 (85%)]\tLoss: 0.261699, Accuracy: 90.23\n",
      "Train Epoch: 48 [40960/50000 (91%)]\tLoss: 0.284580, Accuracy: 89.45\n",
      "Train Epoch: 48 [43520/50000 (97%)]\tLoss: 0.300963, Accuracy: 90.23\n",
      "\n",
      "Validation set: Average loss: 0.8287, Accuracy: 3864/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[36.25862431526184 s]\n",
      "\n",
      "Test set: Average loss: 0.8094, Accuracy: 7715/10000 (77.15%)\n",
      "\n",
      "Train Epoch: 49 [0/50000 (0%)]\tLoss: 0.210948, Accuracy: 93.16\n",
      "Train Epoch: 49 [2560/50000 (6%)]\tLoss: 0.248837, Accuracy: 91.99\n",
      "Train Epoch: 49 [5120/50000 (11%)]\tLoss: 0.246406, Accuracy: 91.60\n",
      "Train Epoch: 49 [7680/50000 (17%)]\tLoss: 0.221466, Accuracy: 92.77\n",
      "Train Epoch: 49 [10240/50000 (23%)]\tLoss: 0.274653, Accuracy: 90.04\n",
      "Train Epoch: 49 [12800/50000 (28%)]\tLoss: 0.255600, Accuracy: 91.99\n",
      "Train Epoch: 49 [15360/50000 (34%)]\tLoss: 0.216870, Accuracy: 92.19\n",
      "Train Epoch: 49 [17920/50000 (40%)]\tLoss: 0.262122, Accuracy: 91.60\n",
      "Train Epoch: 49 [20480/50000 (45%)]\tLoss: 0.241939, Accuracy: 92.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [23040/50000 (51%)]\tLoss: 0.308353, Accuracy: 87.89\n",
      "Train Epoch: 49 [25600/50000 (57%)]\tLoss: 0.226572, Accuracy: 92.38\n",
      "Train Epoch: 49 [28160/50000 (62%)]\tLoss: 0.276818, Accuracy: 90.23\n",
      "Train Epoch: 49 [30720/50000 (68%)]\tLoss: 0.216155, Accuracy: 92.38\n",
      "Train Epoch: 49 [33280/50000 (74%)]\tLoss: 0.229560, Accuracy: 91.80\n",
      "Train Epoch: 49 [35840/50000 (80%)]\tLoss: 0.263856, Accuracy: 91.21\n",
      "Train Epoch: 49 [38400/50000 (85%)]\tLoss: 0.248799, Accuracy: 91.21\n",
      "Train Epoch: 49 [40960/50000 (91%)]\tLoss: 0.342386, Accuracy: 89.84\n",
      "Train Epoch: 49 [43520/50000 (97%)]\tLoss: 0.234191, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.6608, Accuracy: 3992/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[39.28378915786743 s]\n",
      "Train Epoch: 50 [0/50000 (0%)]\tLoss: 0.264161, Accuracy: 90.43\n",
      "Train Epoch: 50 [2560/50000 (6%)]\tLoss: 0.233509, Accuracy: 91.21\n",
      "Train Epoch: 50 [5120/50000 (11%)]\tLoss: 0.186074, Accuracy: 93.55\n",
      "Train Epoch: 50 [7680/50000 (17%)]\tLoss: 0.224211, Accuracy: 93.55\n",
      "Train Epoch: 50 [10240/50000 (23%)]\tLoss: 0.216541, Accuracy: 91.99\n",
      "Train Epoch: 50 [12800/50000 (28%)]\tLoss: 0.259802, Accuracy: 92.19\n",
      "Train Epoch: 50 [15360/50000 (34%)]\tLoss: 0.237769, Accuracy: 91.99\n",
      "Train Epoch: 50 [17920/50000 (40%)]\tLoss: 0.305217, Accuracy: 90.23\n",
      "Train Epoch: 50 [20480/50000 (45%)]\tLoss: 0.239472, Accuracy: 91.80\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0493]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0539]],\n",
      "\n",
      "        [[ 0.0222]],\n",
      "\n",
      "        [[ 0.0219]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0517]],\n",
      "\n",
      "        [[ 0.0270]],\n",
      "\n",
      "        [[ 0.0554]],\n",
      "\n",
      "        [[ 0.0118]],\n",
      "\n",
      "        [[ 0.0331]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0204]],\n",
      "\n",
      "        [[ 0.0417]],\n",
      "\n",
      "        [[ 0.0171]],\n",
      "\n",
      "        [[ 0.0684]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.0408]],\n",
      "\n",
      "        [[ 0.0594]],\n",
      "\n",
      "        [[ 0.0696]],\n",
      "\n",
      "        [[ 0.0803]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0314]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1029]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0288]],\n",
      "\n",
      "        [[ 0.0197]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.0725]],\n",
      "\n",
      "        [[ 0.0169]],\n",
      "\n",
      "        [[ 0.0693]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0270]],\n",
      "\n",
      "        [[ 0.0162]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0428]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1031]],\n",
      "\n",
      "        [[ 0.0334]],\n",
      "\n",
      "        [[ 0.0274]],\n",
      "\n",
      "        [[ 0.0419]],\n",
      "\n",
      "        [[ 0.0351]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0598]],\n",
      "\n",
      "        [[ 0.0744]],\n",
      "\n",
      "        [[ 0.0776]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0078]],\n",
      "\n",
      "        [[ 0.0337]],\n",
      "\n",
      "        [[ 0.0123]],\n",
      "\n",
      "        [[ 0.0273]],\n",
      "\n",
      "        [[ 0.0252]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0302]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0228]],\n",
      "\n",
      "        [[ 0.0899]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0263]],\n",
      "\n",
      "        [[ 0.0459]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0689]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0126]],\n",
      "\n",
      "        [[ 0.0660]],\n",
      "\n",
      "        [[ 0.0249]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0558]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0462]],\n",
      "\n",
      "        [[ 0.0243]],\n",
      "\n",
      "        [[ 0.0603]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0179]],\n",
      "\n",
      "        [[ 0.0581]],\n",
      "\n",
      "        [[ 0.0328]],\n",
      "\n",
      "        [[ 0.0087]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.0698]],\n",
      "\n",
      "        [[ 0.0073]],\n",
      "\n",
      "        [[ 0.0524]],\n",
      "\n",
      "        [[ 0.0142]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0180]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0604]],\n",
      "\n",
      "        [[ 0.0104]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0374]],\n",
      "\n",
      "        [[ 0.0063]],\n",
      "\n",
      "        [[ 0.0069]],\n",
      "\n",
      "        [[ 0.0959]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0451]],\n",
      "\n",
      "        [[ 0.0780]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0994]],\n",
      "\n",
      "        [[ 0.0746]],\n",
      "\n",
      "        [[ 0.0411]],\n",
      "\n",
      "        [[ 0.0255]],\n",
      "\n",
      "        [[ 0.0537]],\n",
      "\n",
      "        [[ 0.0207]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0159]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0634]],\n",
      "\n",
      "        [[ 0.0654]],\n",
      "\n",
      "        [[ 0.0356]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0213]],\n",
      "\n",
      "        [[ 0.0279]],\n",
      "\n",
      "        [[ 0.0476]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.0580]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.1071]],\n",
      "\n",
      "        [[ 0.0676]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0227]],\n",
      "\n",
      "        [[ 0.1122]],\n",
      "\n",
      "        [[ 0.0397]],\n",
      "\n",
      "        [[ 0.0272]],\n",
      "\n",
      "        [[ 0.0081]],\n",
      "\n",
      "        [[ 0.0495]],\n",
      "\n",
      "        [[ 0.0448]],\n",
      "\n",
      "        [[ 0.0578]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0081]],\n",
      "\n",
      "        [[ 0.0548]],\n",
      "\n",
      "        [[ 0.0487]],\n",
      "\n",
      "        [[ 0.0738]],\n",
      "\n",
      "        [[ 0.0272]],\n",
      "\n",
      "        [[ 0.0091]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0249]],\n",
      "\n",
      "        [[ 0.0243]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0188]],\n",
      "\n",
      "        [[ 0.0547]],\n",
      "\n",
      "        [[ 0.0055]],\n",
      "\n",
      "        [[ 0.0296]],\n",
      "\n",
      "        [[ 0.0611]],\n",
      "\n",
      "        [[ 0.0471]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0170]],\n",
      "\n",
      "        [[ 0.0339]],\n",
      "\n",
      "        [[ 0.0293]],\n",
      "\n",
      "        [[ 0.0446]],\n",
      "\n",
      "        [[ 0.0444]],\n",
      "\n",
      "        [[ 0.0214]],\n",
      "\n",
      "        [[ 0.0452]],\n",
      "\n",
      "        [[ 0.0343]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0918]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0195]],\n",
      "\n",
      "        [[ 0.0312]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0456]],\n",
      "\n",
      "        [[ 0.0210]],\n",
      "\n",
      "        [[ 0.0653]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0379]],\n",
      "\n",
      "        [[ 0.0282]],\n",
      "\n",
      "        [[ 0.0106]],\n",
      "\n",
      "        [[ 0.0319]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0106]],\n",
      "\n",
      "        [[ 0.0325]],\n",
      "\n",
      "        [[ 0.0676]],\n",
      "\n",
      "        [[ 0.0839]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.1933]],\n",
      "\n",
      "        [[ 0.0658]],\n",
      "\n",
      "        [[ 0.0317]],\n",
      "\n",
      "        [[ 0.0386]],\n",
      "\n",
      "        [[ 0.0195]],\n",
      "\n",
      "        [[ 0.0261]],\n",
      "\n",
      "        [[ 0.0242]],\n",
      "\n",
      "        [[ 0.0843]],\n",
      "\n",
      "        [[ 0.0415]],\n",
      "\n",
      "        [[ 0.0592]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0037]],\n",
      "\n",
      "        [[ 0.0377]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.0235]],\n",
      "\n",
      "        [[ 0.0710]],\n",
      "\n",
      "        [[ 0.0250]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0257]],\n",
      "\n",
      "        [[ 0.0691]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.0437]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0291]],\n",
      "\n",
      "        [[ 0.0152]],\n",
      "\n",
      "        [[ 0.0122]],\n",
      "\n",
      "        [[ 0.0469]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0724]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0912]],\n",
      "\n",
      "        [[ 0.1719]],\n",
      "\n",
      "        [[ 0.0715]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0404]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0276]],\n",
      "\n",
      "        [[ 0.1269]],\n",
      "\n",
      "        [[ 0.0105]],\n",
      "\n",
      "        [[ 0.0571]],\n",
      "\n",
      "        [[ 0.0326]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0857]],\n",
      "\n",
      "        [[ 0.0431]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0391]],\n",
      "\n",
      "        [[ 0.0461]],\n",
      "\n",
      "        [[ 0.0683]],\n",
      "\n",
      "        [[ 0.0247]],\n",
      "\n",
      "        [[ 0.0450]],\n",
      "\n",
      "        [[ 0.0265]],\n",
      "\n",
      "        [[ 0.0988]],\n",
      "\n",
      "        [[ 0.0477]],\n",
      "\n",
      "        [[ 0.0349]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.0363]],\n",
      "\n",
      "        [[ 0.0069]],\n",
      "\n",
      "        [[ 0.0691]],\n",
      "\n",
      "        [[ 0.0376]],\n",
      "\n",
      "        [[ 0.0629]],\n",
      "\n",
      "        [[ 0.0245]],\n",
      "\n",
      "        [[ 0.0540]],\n",
      "\n",
      "        [[ 0.0097]],\n",
      "\n",
      "        [[ 0.0509]],\n",
      "\n",
      "        [[ 0.0717]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0059]],\n",
      "\n",
      "        [[ 0.0516]],\n",
      "\n",
      "        [[ 0.0013]],\n",
      "\n",
      "        [[ 0.0251]],\n",
      "\n",
      "        [[ 0.0210]],\n",
      "\n",
      "        [[ 0.0206]],\n",
      "\n",
      "        [[ 0.0473]],\n",
      "\n",
      "        [[ 0.0361]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0499]],\n",
      "\n",
      "        [[ 0.0126]],\n",
      "\n",
      "        [[ 0.0470]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.0341]],\n",
      "\n",
      "        [[ 0.0354]],\n",
      "\n",
      "        [[ 0.0412]],\n",
      "\n",
      "        [[ 0.1649]],\n",
      "\n",
      "        [[ 0.0169]],\n",
      "\n",
      "        [[ 0.0201]],\n",
      "\n",
      "        [[ 0.0282]],\n",
      "\n",
      "        [[ 0.0424]],\n",
      "\n",
      "        [[ 0.0111]],\n",
      "\n",
      "        [[ 0.1072]],\n",
      "\n",
      "        [[ 0.0391]],\n",
      "\n",
      "        [[ 0.0268]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0911]],\n",
      "\n",
      "        [[ 0.0133]],\n",
      "\n",
      "        [[ 0.0147]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.0416]],\n",
      "\n",
      "        [[ 0.0164]],\n",
      "\n",
      "        [[ 0.0175]],\n",
      "\n",
      "        [[ 0.0191]],\n",
      "\n",
      "        [[ 0.0175]],\n",
      "\n",
      "        [[ 0.0541]],\n",
      "\n",
      "        [[ 0.0378]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.0355]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0363]],\n",
      "\n",
      "        [[ 0.0337]],\n",
      "\n",
      "        [[ 0.0185]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0650]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0642]],\n",
      "\n",
      "        [[ 0.0333]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0187]],\n",
      "\n",
      "        [[ 0.0396]],\n",
      "\n",
      "        [[ 0.0451]],\n",
      "\n",
      "        [[ 0.0324]],\n",
      "\n",
      "        [[ 0.0181]],\n",
      "\n",
      "        [[ 0.0414]],\n",
      "\n",
      "        [[ 0.0342]],\n",
      "\n",
      "        [[ 0.0263]],\n",
      "\n",
      "        [[ 0.0492]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0079]],\n",
      "\n",
      "        [[ 0.0299]],\n",
      "\n",
      "        [[ 0.0375]],\n",
      "\n",
      "        [[ 0.0140]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0632]],\n",
      "\n",
      "        [[ 0.0307]],\n",
      "\n",
      "        [[ 0.0197]],\n",
      "\n",
      "        [[ 0.0815]],\n",
      "\n",
      "        [[ 0.0092]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0474]],\n",
      "\n",
      "        [[ 0.0059]],\n",
      "\n",
      "        [[ 0.0720]],\n",
      "\n",
      "        [[ 0.0516]],\n",
      "\n",
      "        [[ 0.0343]],\n",
      "\n",
      "        [[ 0.0067]],\n",
      "\n",
      "        [[ 0.0299]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1433]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0068]],\n",
      "\n",
      "        [[ 0.0349]],\n",
      "\n",
      "        [[ 0.0239]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0469]],\n",
      "\n",
      "        [[ 0.0184]],\n",
      "\n",
      "        [[ 0.0177]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0280]],\n",
      "\n",
      "        [[ 0.0860]],\n",
      "\n",
      "        [[ 0.0499]],\n",
      "\n",
      "        [[ 0.0106]],\n",
      "\n",
      "        [[ 0.0328]],\n",
      "\n",
      "        [[ 0.0430]],\n",
      "\n",
      "        [[ 0.0862]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.0086]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0108]],\n",
      "\n",
      "        [[ 0.0416]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0640]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.0263]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0260]],\n",
      "\n",
      "        [[ 0.0444]],\n",
      "\n",
      "        [[ 0.0156]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0022]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0326]],\n",
      "\n",
      "        [[ 0.0109]],\n",
      "\n",
      "        [[ 0.0434]],\n",
      "\n",
      "        [[ 0.0380]],\n",
      "\n",
      "        [[ 0.0775]],\n",
      "\n",
      "        [[ 0.0249]],\n",
      "\n",
      "        [[ 0.0363]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0347]],\n",
      "\n",
      "        [[ 0.0280]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.0052]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0257]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0682]],\n",
      "\n",
      "        [[ 0.0217]],\n",
      "\n",
      "        [[ 0.1428]],\n",
      "\n",
      "        [[ 0.0374]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0162]],\n",
      "\n",
      "        [[ 0.0117]],\n",
      "\n",
      "        [[ 0.0246]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1042]],\n",
      "\n",
      "        [[ 0.0080]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0364]],\n",
      "\n",
      "        [[ 0.0450]],\n",
      "\n",
      "        [[ 0.0364]],\n",
      "\n",
      "        [[ 0.0476]],\n",
      "\n",
      "        [[ 0.0651]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0266]],\n",
      "\n",
      "        [[ 0.0559]],\n",
      "\n",
      "        [[ 0.0414]],\n",
      "\n",
      "        [[ 0.0372]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0095]],\n",
      "\n",
      "        [[ 0.0595]],\n",
      "\n",
      "        [[ 0.0472]],\n",
      "\n",
      "        [[ 0.0347]],\n",
      "\n",
      "        [[ 0.0416]],\n",
      "\n",
      "        [[ 0.0234]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0478]],\n",
      "\n",
      "        [[ 0.0344]],\n",
      "\n",
      "        [[ 0.0179]],\n",
      "\n",
      "        [[ 0.0958]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0540]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.0802]],\n",
      "\n",
      "        [[ 0.0392]],\n",
      "\n",
      "        [[ 0.0652]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.1101]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0405]],\n",
      "\n",
      "        [[ 0.0352]],\n",
      "\n",
      "        [[ 0.0694]],\n",
      "\n",
      "        [[ 0.0928]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0724]],\n",
      "\n",
      "        [[ 0.0478]],\n",
      "\n",
      "        [[ 0.0101]],\n",
      "\n",
      "        [[ 0.0562]],\n",
      "\n",
      "        [[ 0.0071]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0361]],\n",
      "\n",
      "        [[ 0.1112]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0299]],\n",
      "\n",
      "        [[ 0.0426]],\n",
      "\n",
      "        [[ 0.0542]],\n",
      "\n",
      "        [[ 0.0467]],\n",
      "\n",
      "        [[ 0.0706]],\n",
      "\n",
      "        [[ 0.0132]],\n",
      "\n",
      "        [[ 0.0265]],\n",
      "\n",
      "        [[ 0.0130]],\n",
      "\n",
      "        [[ 0.0645]],\n",
      "\n",
      "        [[ 0.0628]],\n",
      "\n",
      "        [[ 0.0079]],\n",
      "\n",
      "        [[ 0.0346]],\n",
      "\n",
      "        [[ 0.0400]],\n",
      "\n",
      "        [[ 0.0305]],\n",
      "\n",
      "        [[ 0.0274]],\n",
      "\n",
      "        [[ 0.0529]],\n",
      "\n",
      "        [[ 0.0023]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0615]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.0290]],\n",
      "\n",
      "        [[ 0.1337]],\n",
      "\n",
      "        [[ 0.0529]],\n",
      "\n",
      "        [[ 0.0370]],\n",
      "\n",
      "        [[ 0.0591]],\n",
      "\n",
      "        [[ 0.0441]],\n",
      "\n",
      "        [[ 0.0308]],\n",
      "\n",
      "        [[ 0.0499]],\n",
      "\n",
      "        [[ 0.0303]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0146]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0263]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0243]],\n",
      "\n",
      "        [[ 0.0133]],\n",
      "\n",
      "        [[ 0.0077]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0611]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.1942]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.1665]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0184]],\n",
      "\n",
      "        [[ 0.0228]],\n",
      "\n",
      "        [[ 0.0623]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0296]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 6.5792]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[ 4.1709e-03,  1.7752e-02,  2.4181e-02,  ...,  1.7303e-02,\n",
      "           2.0669e-02, -4.5924e-03]],\n",
      "\n",
      "        [[-1.5845e-02,  2.2664e-02, -3.6412e-03,  ...,  4.9034e-03,\n",
      "          -4.1533e-02, -2.9249e-02]],\n",
      "\n",
      "        [[ 5.2012e-03,  3.6461e-02, -1.6587e-02,  ...,  9.1216e-03,\n",
      "           6.8627e-03,  3.0804e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.8577e-02,  3.1469e-02,  2.8871e-02,  ..., -3.6145e-02,\n",
      "          -2.4465e-02, -3.5099e-02]],\n",
      "\n",
      "        [[ 6.4253e-03,  2.4880e-02, -7.4889e-03,  ..., -2.8410e-02,\n",
      "           2.2868e-02, -2.2333e-02]],\n",
      "\n",
      "        [[-2.6422e-02,  1.0918e-02, -2.3866e-02,  ...,  2.4926e-02,\n",
      "          -3.7675e-03,  2.2282e-02]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor([[[-1.0773e-03, -1.4079e-03, -1.8546e-03,  ..., -1.9599e-03,\n",
      "          -1.6441e-03, -1.4899e-03],\n",
      "         [ 1.3244e-03,  1.3265e-03,  1.3990e-03,  ..., -6.0313e-05,\n",
      "           4.8798e-05, -3.4144e-05],\n",
      "         [-2.0047e-04, -1.1245e-04,  1.8869e-04,  ...,  1.2469e-04,\n",
      "           2.9369e-04, -1.2983e-05],\n",
      "         ...,\n",
      "         [ 2.7000e-03,  2.7044e-03,  2.1814e-03,  ..., -1.5743e-05,\n",
      "          -7.0881e-04, -1.3504e-03],\n",
      "         [ 1.3388e-04, -6.3149e-05,  6.8677e-05,  ..., -4.9504e-04,\n",
      "          -1.0230e-03, -8.5985e-04],\n",
      "         [ 4.8078e-05,  5.6767e-04,  1.0002e-03,  ...,  1.6045e-03,\n",
      "           2.1013e-03,  2.0197e-03]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [23040/50000 (51%)]\tLoss: 0.289459, Accuracy: 90.62\n",
      "Train Epoch: 50 [25600/50000 (57%)]\tLoss: 0.258578, Accuracy: 91.21\n",
      "Train Epoch: 50 [28160/50000 (62%)]\tLoss: 0.321268, Accuracy: 89.84\n",
      "Train Epoch: 50 [30720/50000 (68%)]\tLoss: 0.255934, Accuracy: 90.43\n",
      "Train Epoch: 50 [33280/50000 (74%)]\tLoss: 0.215966, Accuracy: 93.55\n",
      "Train Epoch: 50 [35840/50000 (80%)]\tLoss: 0.203787, Accuracy: 92.58\n",
      "Train Epoch: 50 [38400/50000 (85%)]\tLoss: 0.212752, Accuracy: 92.97\n",
      "Train Epoch: 50 [40960/50000 (91%)]\tLoss: 0.261147, Accuracy: 90.62\n",
      "Train Epoch: 50 [43520/50000 (97%)]\tLoss: 0.225702, Accuracy: 93.16\n",
      "\n",
      "Validation set: Average loss: 0.4804, Accuracy: 4252/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[36.249977827072144 s]\n",
      "\n",
      "Test set: Average loss: 0.5165, Accuracy: 8379/10000 (83.79%)\n",
      "\n",
      "Train Epoch: 51 [0/50000 (0%)]\tLoss: 0.204942, Accuracy: 92.97\n",
      "Train Epoch: 51 [2560/50000 (6%)]\tLoss: 0.282670, Accuracy: 88.48\n",
      "Train Epoch: 51 [5120/50000 (11%)]\tLoss: 0.261224, Accuracy: 91.99\n",
      "Train Epoch: 51 [7680/50000 (17%)]\tLoss: 0.242285, Accuracy: 91.21\n",
      "Train Epoch: 51 [10240/50000 (23%)]\tLoss: 0.241707, Accuracy: 92.38\n",
      "Train Epoch: 51 [12800/50000 (28%)]\tLoss: 0.241004, Accuracy: 92.38\n",
      "Train Epoch: 51 [15360/50000 (34%)]\tLoss: 0.250939, Accuracy: 91.21\n",
      "Train Epoch: 51 [17920/50000 (40%)]\tLoss: 0.188744, Accuracy: 92.38\n",
      "Train Epoch: 51 [20480/50000 (45%)]\tLoss: 0.328660, Accuracy: 89.84\n",
      "Train Epoch: 51 [23040/50000 (51%)]\tLoss: 0.226756, Accuracy: 91.80\n",
      "Train Epoch: 51 [25600/50000 (57%)]\tLoss: 0.219560, Accuracy: 92.19\n",
      "Train Epoch: 51 [28160/50000 (62%)]\tLoss: 0.253349, Accuracy: 91.99\n",
      "Train Epoch: 51 [30720/50000 (68%)]\tLoss: 0.272993, Accuracy: 91.41\n",
      "Train Epoch: 51 [33280/50000 (74%)]\tLoss: 0.234150, Accuracy: 92.19\n",
      "Train Epoch: 51 [35840/50000 (80%)]\tLoss: 0.258327, Accuracy: 91.41\n",
      "Train Epoch: 51 [38400/50000 (85%)]\tLoss: 0.234045, Accuracy: 92.38\n",
      "Train Epoch: 51 [40960/50000 (91%)]\tLoss: 0.255080, Accuracy: 91.99\n",
      "Train Epoch: 51 [43520/50000 (97%)]\tLoss: 0.227996, Accuracy: 93.95\n",
      "\n",
      "Validation set: Average loss: 0.8028, Accuracy: 3866/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[39.31984329223633 s]\n",
      "Train Epoch: 52 [0/50000 (0%)]\tLoss: 0.229862, Accuracy: 91.80\n",
      "Train Epoch: 52 [2560/50000 (6%)]\tLoss: 0.229892, Accuracy: 91.99\n",
      "Train Epoch: 52 [5120/50000 (11%)]\tLoss: 0.244117, Accuracy: 91.60\n",
      "Train Epoch: 52 [7680/50000 (17%)]\tLoss: 0.239201, Accuracy: 91.80\n",
      "Train Epoch: 52 [10240/50000 (23%)]\tLoss: 0.248592, Accuracy: 92.77\n",
      "Train Epoch: 52 [12800/50000 (28%)]\tLoss: 0.219533, Accuracy: 91.99\n",
      "Train Epoch: 52 [15360/50000 (34%)]\tLoss: 0.302978, Accuracy: 88.28\n",
      "Train Epoch: 52 [17920/50000 (40%)]\tLoss: 0.240316, Accuracy: 92.58\n",
      "Train Epoch: 52 [20480/50000 (45%)]\tLoss: 0.236856, Accuracy: 92.58\n",
      "Train Epoch: 52 [23040/50000 (51%)]\tLoss: 0.271657, Accuracy: 91.41\n",
      "Train Epoch: 52 [25600/50000 (57%)]\tLoss: 0.266779, Accuracy: 92.19\n",
      "Train Epoch: 52 [28160/50000 (62%)]\tLoss: 0.228206, Accuracy: 91.99\n",
      "Train Epoch: 52 [30720/50000 (68%)]\tLoss: 0.258246, Accuracy: 90.43\n",
      "Train Epoch: 52 [33280/50000 (74%)]\tLoss: 0.258120, Accuracy: 91.02\n",
      "Train Epoch: 52 [35840/50000 (80%)]\tLoss: 0.237244, Accuracy: 92.77\n",
      "Train Epoch: 52 [38400/50000 (85%)]\tLoss: 0.286759, Accuracy: 89.65\n",
      "Train Epoch: 52 [40960/50000 (91%)]\tLoss: 0.242417, Accuracy: 91.60\n",
      "Train Epoch: 52 [43520/50000 (97%)]\tLoss: 0.220569, Accuracy: 91.60\n",
      "\n",
      "Validation set: Average loss: 0.5164, Accuracy: 4177/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.25167632102966 s]\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0777]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0213]],\n",
      "\n",
      "        [[ 0.0218]],\n",
      "\n",
      "        [[ 0.0484]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0661]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0994]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0438]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0644]],\n",
      "\n",
      "        [[ 0.0930]],\n",
      "\n",
      "        [[ 0.0124]],\n",
      "\n",
      "        [[ 0.1648]],\n",
      "\n",
      "        [[ 0.0105]],\n",
      "\n",
      "        [[ 0.0523]],\n",
      "\n",
      "        [[ 0.0087]],\n",
      "\n",
      "        [[ 0.1870]],\n",
      "\n",
      "        [[ 0.0349]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.0161]],\n",
      "\n",
      "        [[ 0.0363]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0233]],\n",
      "\n",
      "        [[ 0.0080]],\n",
      "\n",
      "        [[ 0.0296]],\n",
      "\n",
      "        [[ 0.0090]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0182]],\n",
      "\n",
      "        [[ 0.1209]],\n",
      "\n",
      "        [[ 0.0303]],\n",
      "\n",
      "        [[ 0.0964]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0923]],\n",
      "\n",
      "        [[ 0.0111]],\n",
      "\n",
      "        [[ 0.0390]],\n",
      "\n",
      "        [[ 0.0546]],\n",
      "\n",
      "        [[ 0.0061]],\n",
      "\n",
      "        [[ 0.1554]],\n",
      "\n",
      "        [[ 0.0381]],\n",
      "\n",
      "        [[ 0.0997]],\n",
      "\n",
      "        [[ 0.0477]],\n",
      "\n",
      "        [[ 0.0797]],\n",
      "\n",
      "        [[ 0.0085]],\n",
      "\n",
      "        [[ 0.0082]],\n",
      "\n",
      "        [[ 0.0705]],\n",
      "\n",
      "        [[ 0.1373]],\n",
      "\n",
      "        [[ 0.0931]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.0169]],\n",
      "\n",
      "        [[ 0.0707]],\n",
      "\n",
      "        [[ 0.0556]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0453]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0420]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0198]],\n",
      "\n",
      "        [[ 0.1333]],\n",
      "\n",
      "        [[ 0.0020]],\n",
      "\n",
      "        [[ 0.0425]],\n",
      "\n",
      "        [[ 0.0405]],\n",
      "\n",
      "        [[ 0.0301]],\n",
      "\n",
      "        [[ 0.0561]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0189]],\n",
      "\n",
      "        [[ 0.1166]],\n",
      "\n",
      "        [[ 0.0369]],\n",
      "\n",
      "        [[ 0.0325]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0345]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0645]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.0600]],\n",
      "\n",
      "        [[ 0.0103]],\n",
      "\n",
      "        [[ 0.0175]],\n",
      "\n",
      "        [[ 0.0256]],\n",
      "\n",
      "        [[ 0.0437]],\n",
      "\n",
      "        [[ 0.0753]],\n",
      "\n",
      "        [[ 0.0453]],\n",
      "\n",
      "        [[ 0.0456]],\n",
      "\n",
      "        [[ 0.0311]],\n",
      "\n",
      "        [[ 0.0430]],\n",
      "\n",
      "        [[ 0.0383]],\n",
      "\n",
      "        [[ 0.0325]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0100]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0956]],\n",
      "\n",
      "        [[ 0.0239]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.0288]],\n",
      "\n",
      "        [[ 0.0182]],\n",
      "\n",
      "        [[ 0.1205]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0813]],\n",
      "\n",
      "        [[ 0.0872]],\n",
      "\n",
      "        [[ 0.0509]],\n",
      "\n",
      "        [[ 0.0531]],\n",
      "\n",
      "        [[ 0.0535]],\n",
      "\n",
      "        [[ 0.0430]],\n",
      "\n",
      "        [[ 0.0352]],\n",
      "\n",
      "        [[ 0.0195]],\n",
      "\n",
      "        [[ 0.0247]],\n",
      "\n",
      "        [[ 0.0786]],\n",
      "\n",
      "        [[ 0.0120]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0402]],\n",
      "\n",
      "        [[ 0.0440]],\n",
      "\n",
      "        [[ 0.0356]],\n",
      "\n",
      "        [[ 0.1249]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0187]],\n",
      "\n",
      "        [[ 0.0323]],\n",
      "\n",
      "        [[ 0.0209]],\n",
      "\n",
      "        [[ 0.0411]],\n",
      "\n",
      "        [[ 0.0147]],\n",
      "\n",
      "        [[ 0.0300]],\n",
      "\n",
      "        [[ 0.1983]],\n",
      "\n",
      "        [[ 0.1071]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0264]],\n",
      "\n",
      "        [[ 0.0296]],\n",
      "\n",
      "        [[ 0.1318]],\n",
      "\n",
      "        [[ 0.0350]],\n",
      "\n",
      "        [[ 0.0973]],\n",
      "\n",
      "        [[ 0.0096]],\n",
      "\n",
      "        [[ 0.0323]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.0153]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0411]],\n",
      "\n",
      "        [[ 0.0455]],\n",
      "\n",
      "        [[ 0.1000]],\n",
      "\n",
      "        [[ 0.0550]],\n",
      "\n",
      "        [[ 0.0247]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0263]],\n",
      "\n",
      "        [[ 0.1102]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0232]],\n",
      "\n",
      "        [[ 0.0212]],\n",
      "\n",
      "        [[ 0.0391]],\n",
      "\n",
      "        [[ 0.0164]],\n",
      "\n",
      "        [[ 0.0558]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0270]],\n",
      "\n",
      "        [[ 0.0257]],\n",
      "\n",
      "        [[ 0.0649]],\n",
      "\n",
      "        [[ 0.0184]],\n",
      "\n",
      "        [[ 0.0592]],\n",
      "\n",
      "        [[ 0.0861]],\n",
      "\n",
      "        [[ 0.0312]],\n",
      "\n",
      "        [[ 0.0692]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0617]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.1183]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0613]],\n",
      "\n",
      "        [[ 0.0477]],\n",
      "\n",
      "        [[ 0.1651]],\n",
      "\n",
      "        [[ 0.0458]],\n",
      "\n",
      "        [[ 0.0099]],\n",
      "\n",
      "        [[ 0.0368]],\n",
      "\n",
      "        [[ 0.0154]],\n",
      "\n",
      "        [[ 0.1047]],\n",
      "\n",
      "        [[ 0.0409]],\n",
      "\n",
      "        [[ 0.0099]],\n",
      "\n",
      "        [[ 0.0286]],\n",
      "\n",
      "        [[ 0.0377]],\n",
      "\n",
      "        [[ 0.0642]],\n",
      "\n",
      "        [[ 0.0251]],\n",
      "\n",
      "        [[ 0.0469]],\n",
      "\n",
      "        [[ 0.1423]],\n",
      "\n",
      "        [[ 0.0384]],\n",
      "\n",
      "        [[ 0.0448]],\n",
      "\n",
      "        [[ 0.0717]],\n",
      "\n",
      "        [[ 0.0287]],\n",
      "\n",
      "        [[ 0.0167]],\n",
      "\n",
      "        [[ 0.0648]],\n",
      "\n",
      "        [[ 0.0562]],\n",
      "\n",
      "        [[ 0.0800]],\n",
      "\n",
      "        [[ 0.0462]],\n",
      "\n",
      "        [[ 0.0091]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0421]],\n",
      "\n",
      "        [[ 0.0257]],\n",
      "\n",
      "        [[ 0.0556]],\n",
      "\n",
      "        [[ 0.0604]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0187]],\n",
      "\n",
      "        [[ 0.1957]],\n",
      "\n",
      "        [[ 0.0148]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.0973]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0665]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.0672]],\n",
      "\n",
      "        [[ 0.1732]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0686]],\n",
      "\n",
      "        [[ 0.0341]],\n",
      "\n",
      "        [[ 0.1859]],\n",
      "\n",
      "        [[ 0.0425]],\n",
      "\n",
      "        [[ 0.0751]],\n",
      "\n",
      "        [[ 0.0429]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0792]],\n",
      "\n",
      "        [[ 0.0206]],\n",
      "\n",
      "        [[ 0.0290]],\n",
      "\n",
      "        [[ 0.0656]],\n",
      "\n",
      "        [[ 0.0112]],\n",
      "\n",
      "        [[ 0.0865]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0925]],\n",
      "\n",
      "        [[ 0.0227]],\n",
      "\n",
      "        [[ 0.0273]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.1328]],\n",
      "\n",
      "        [[ 0.0988]],\n",
      "\n",
      "        [[ 0.0555]],\n",
      "\n",
      "        [[ 0.0813]],\n",
      "\n",
      "        [[ 0.1001]],\n",
      "\n",
      "        [[ 0.1102]],\n",
      "\n",
      "        [[ 0.0981]],\n",
      "\n",
      "        [[ 0.0193]],\n",
      "\n",
      "        [[ 0.0084]],\n",
      "\n",
      "        [[ 0.0167]],\n",
      "\n",
      "        [[ 0.0563]],\n",
      "\n",
      "        [[ 0.1100]],\n",
      "\n",
      "        [[ 0.1458]],\n",
      "\n",
      "        [[ 0.0877]],\n",
      "\n",
      "        [[ 0.0233]],\n",
      "\n",
      "        [[ 0.0051]],\n",
      "\n",
      "        [[ 0.0262]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0669]],\n",
      "\n",
      "        [[ 0.0359]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.0065]],\n",
      "\n",
      "        [[ 0.0306]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0475]],\n",
      "\n",
      "        [[ 0.0829]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.0173]],\n",
      "\n",
      "        [[ 0.0096]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0423]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0304]],\n",
      "\n",
      "        [[ 0.0195]],\n",
      "\n",
      "        [[ 0.0167]],\n",
      "\n",
      "        [[ 0.0483]],\n",
      "\n",
      "        [[ 0.0172]],\n",
      "\n",
      "        [[ 0.1438]],\n",
      "\n",
      "        [[ 0.0284]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0254]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0900]],\n",
      "\n",
      "        [[ 0.0319]],\n",
      "\n",
      "        [[ 0.0701]],\n",
      "\n",
      "        [[ 0.0371]],\n",
      "\n",
      "        [[ 0.0618]],\n",
      "\n",
      "        [[ 0.1097]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0318]],\n",
      "\n",
      "        [[ 0.0769]],\n",
      "\n",
      "        [[ 0.0300]],\n",
      "\n",
      "        [[ 0.0628]],\n",
      "\n",
      "        [[ 0.0303]],\n",
      "\n",
      "        [[ 0.0614]],\n",
      "\n",
      "        [[ 0.0433]],\n",
      "\n",
      "        [[ 0.0164]],\n",
      "\n",
      "        [[ 0.0195]],\n",
      "\n",
      "        [[ 0.0243]],\n",
      "\n",
      "        [[ 0.0350]],\n",
      "\n",
      "        [[ 0.0435]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0424]],\n",
      "\n",
      "        [[ 0.0545]],\n",
      "\n",
      "        [[ 0.0445]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0158]],\n",
      "\n",
      "        [[ 0.0615]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0390]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0749]],\n",
      "\n",
      "        [[ 0.0786]],\n",
      "\n",
      "        [[ 0.0122]],\n",
      "\n",
      "        [[ 0.0668]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.0484]],\n",
      "\n",
      "        [[ 0.0279]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0332]],\n",
      "\n",
      "        [[ 0.0297]],\n",
      "\n",
      "        [[ 0.0397]],\n",
      "\n",
      "        [[ 0.0492]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0894]],\n",
      "\n",
      "        [[ 0.0759]],\n",
      "\n",
      "        [[ 0.0713]],\n",
      "\n",
      "        [[ 0.0694]],\n",
      "\n",
      "        [[ 0.1217]],\n",
      "\n",
      "        [[ 0.0442]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0489]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.1247]],\n",
      "\n",
      "        [[ 0.1540]],\n",
      "\n",
      "        [[ 0.0414]],\n",
      "\n",
      "        [[ 0.0092]],\n",
      "\n",
      "        [[ 0.0502]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0322]],\n",
      "\n",
      "        [[ 0.0069]],\n",
      "\n",
      "        [[ 0.0196]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0443]],\n",
      "\n",
      "        [[ 0.0168]],\n",
      "\n",
      "        [[ 0.0405]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0565]],\n",
      "\n",
      "        [[ 0.0427]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0757]],\n",
      "\n",
      "        [[ 0.0932]],\n",
      "\n",
      "        [[ 0.0840]],\n",
      "\n",
      "        [[ 0.0300]],\n",
      "\n",
      "        [[ 0.0373]],\n",
      "\n",
      "        [[ 0.0542]],\n",
      "\n",
      "        [[ 0.0934]],\n",
      "\n",
      "        [[ 0.0409]],\n",
      "\n",
      "        [[ 0.0356]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0386]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0950]],\n",
      "\n",
      "        [[ 0.0680]],\n",
      "\n",
      "        [[ 0.1348]],\n",
      "\n",
      "        [[ 0.0116]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0182]],\n",
      "\n",
      "        [[ 0.0328]],\n",
      "\n",
      "        [[ 0.0253]],\n",
      "\n",
      "        [[ 0.0628]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0122]],\n",
      "\n",
      "        [[ 0.0586]],\n",
      "\n",
      "        [[ 0.0342]],\n",
      "\n",
      "        [[ 0.0542]],\n",
      "\n",
      "        [[ 0.0515]],\n",
      "\n",
      "        [[ 0.0136]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0308]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.0232]],\n",
      "\n",
      "        [[ 0.0737]],\n",
      "\n",
      "        [[ 0.0299]],\n",
      "\n",
      "        [[ 0.0535]],\n",
      "\n",
      "        [[ 0.0204]],\n",
      "\n",
      "        [[ 0.0926]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.2221]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.0139]],\n",
      "\n",
      "        [[ 0.0414]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0222]],\n",
      "\n",
      "        [[ 0.0210]],\n",
      "\n",
      "        [[ 0.0141]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0466]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0140]],\n",
      "\n",
      "        [[ 0.0133]],\n",
      "\n",
      "        [[ 0.0313]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0724]],\n",
      "\n",
      "        [[ 0.0622]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0666]],\n",
      "\n",
      "        [[ 0.0133]],\n",
      "\n",
      "        [[ 0.0633]],\n",
      "\n",
      "        [[ 0.0282]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.0834]],\n",
      "\n",
      "        [[ 0.0603]],\n",
      "\n",
      "        [[ 0.0299]],\n",
      "\n",
      "        [[ 0.0223]],\n",
      "\n",
      "        [[ 0.0397]],\n",
      "\n",
      "        [[ 0.0142]],\n",
      "\n",
      "        [[ 0.0941]],\n",
      "\n",
      "        [[ 0.0147]],\n",
      "\n",
      "        [[ 0.1161]],\n",
      "\n",
      "        [[ 0.0544]],\n",
      "\n",
      "        [[ 0.0531]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0199]],\n",
      "\n",
      "        [[ 0.0379]],\n",
      "\n",
      "        [[ 0.1270]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0626]],\n",
      "\n",
      "        [[ 0.0679]],\n",
      "\n",
      "        [[ 0.1946]],\n",
      "\n",
      "        [[ 0.0108]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0244]],\n",
      "\n",
      "        [[ 0.1253]],\n",
      "\n",
      "        [[ 0.0854]],\n",
      "\n",
      "        [[ 0.0242]],\n",
      "\n",
      "        [[ 0.1853]],\n",
      "\n",
      "        [[ 0.0484]],\n",
      "\n",
      "        [[ 0.0671]],\n",
      "\n",
      "        [[ 0.0423]],\n",
      "\n",
      "        [[ 0.0219]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0164]],\n",
      "\n",
      "        [[ 0.0256]],\n",
      "\n",
      "        [[ 0.0272]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0242]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0821]],\n",
      "\n",
      "        [[ 0.0519]],\n",
      "\n",
      "        [[ 0.0129]],\n",
      "\n",
      "        [[ 0.0838]],\n",
      "\n",
      "        [[ 0.0463]],\n",
      "\n",
      "        [[ 0.0233]],\n",
      "\n",
      "        [[ 0.0990]],\n",
      "\n",
      "        [[ 0.1125]],\n",
      "\n",
      "        [[ 0.0124]],\n",
      "\n",
      "        [[ 0.0307]],\n",
      "\n",
      "        [[ 0.0338]],\n",
      "\n",
      "        [[ 0.0496]],\n",
      "\n",
      "        [[ 0.0370]],\n",
      "\n",
      "        [[ 0.0534]],\n",
      "\n",
      "        [[ 0.0329]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0471]],\n",
      "\n",
      "        [[ 0.0342]],\n",
      "\n",
      "        [[ 0.0551]],\n",
      "\n",
      "        [[ 0.0679]],\n",
      "\n",
      "        [[ 0.1018]],\n",
      "\n",
      "        [[ 0.0416]],\n",
      "\n",
      "        [[ 0.0724]],\n",
      "\n",
      "        [[ 0.0655]],\n",
      "\n",
      "        [[ 0.0313]],\n",
      "\n",
      "        [[ 0.1380]],\n",
      "\n",
      "        [[ 0.0454]],\n",
      "\n",
      "        [[ 0.0247]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0440]],\n",
      "\n",
      "        [[ 0.0357]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0218]],\n",
      "\n",
      "        [[ 0.0664]],\n",
      "\n",
      "        [[ 0.0943]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.0991]],\n",
      "\n",
      "        [[ 0.1615]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.2314]],\n",
      "\n",
      "        [[ 0.0123]],\n",
      "\n",
      "        [[ 0.0071]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.1124]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0512]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 6.0690]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 0.3818,  1.5981,  2.1725,  ...,  1.5565,  1.8564, -0.4051]],\n",
      "\n",
      "        [[-1.4345,  1.9950, -0.3647,  ...,  0.3608, -3.7889, -2.6593]],\n",
      "\n",
      "        [[ 0.4706,  3.2709, -1.4812,  ...,  0.8217,  0.6194,  2.7641]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.5085,  2.7712,  2.5488,  ..., -3.3162, -2.2504, -3.1883]],\n",
      "\n",
      "        [[ 0.6704,  2.3220, -0.5809,  ..., -2.4560,  2.1393, -1.9098]],\n",
      "\n",
      "        [[-2.4260,  0.9387, -2.1779,  ...,  2.1908, -0.3936,  1.9402]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor([[[-1.0444e-03, -1.3576e-03, -1.7894e-03,  ..., -1.8874e-03,\n",
      "          -1.5848e-03, -1.4324e-03],\n",
      "         [ 1.3310e-03,  1.3427e-03,  1.4243e-03,  ...,  9.3948e-06,\n",
      "           9.7878e-05,  5.7593e-06],\n",
      "         [-1.9758e-04, -1.0970e-04,  1.7943e-04,  ...,  1.1403e-04,\n",
      "           2.7161e-04, -2.3529e-05],\n",
      "         ...,\n",
      "         [ 2.6561e-03,  2.6731e-03,  2.1749e-03,  ...,  5.9805e-05,\n",
      "          -6.2387e-04, -1.2474e-03],\n",
      "         [ 4.3986e-05, -1.6479e-04, -6.6384e-05,  ..., -5.9726e-04,\n",
      "          -1.0887e-03, -9.1758e-04],\n",
      "         [ 8.3054e-05,  5.9582e-04,  1.0329e-03,  ...,  1.5897e-03,\n",
      "           2.0604e-03,  1.9761e-03]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5526, Accuracy: 8312/10000 (83.12%)\n",
      "\n",
      "Train Epoch: 53 [0/50000 (0%)]\tLoss: 0.204973, Accuracy: 93.36\n",
      "Train Epoch: 53 [2560/50000 (6%)]\tLoss: 0.212016, Accuracy: 92.58\n",
      "Train Epoch: 53 [5120/50000 (11%)]\tLoss: 0.196836, Accuracy: 92.97\n",
      "Train Epoch: 53 [7680/50000 (17%)]\tLoss: 0.178123, Accuracy: 93.55\n",
      "Train Epoch: 53 [10240/50000 (23%)]\tLoss: 0.196962, Accuracy: 93.16\n",
      "Train Epoch: 53 [12800/50000 (28%)]\tLoss: 0.187503, Accuracy: 93.55\n",
      "Train Epoch: 53 [15360/50000 (34%)]\tLoss: 0.267527, Accuracy: 90.43\n",
      "Train Epoch: 53 [17920/50000 (40%)]\tLoss: 0.205996, Accuracy: 93.16\n",
      "Train Epoch: 53 [20480/50000 (45%)]\tLoss: 0.347089, Accuracy: 89.06\n",
      "Train Epoch: 53 [23040/50000 (51%)]\tLoss: 0.205650, Accuracy: 93.75\n",
      "Train Epoch: 53 [25600/50000 (57%)]\tLoss: 0.257227, Accuracy: 91.99\n",
      "Train Epoch: 53 [28160/50000 (62%)]\tLoss: 0.277023, Accuracy: 90.23\n",
      "Train Epoch: 53 [30720/50000 (68%)]\tLoss: 0.235963, Accuracy: 91.99\n",
      "Train Epoch: 53 [33280/50000 (74%)]\tLoss: 0.269571, Accuracy: 90.43\n",
      "Train Epoch: 53 [35840/50000 (80%)]\tLoss: 0.320733, Accuracy: 88.28\n",
      "Train Epoch: 53 [38400/50000 (85%)]\tLoss: 0.224081, Accuracy: 91.80\n",
      "Train Epoch: 53 [40960/50000 (91%)]\tLoss: 0.253219, Accuracy: 91.02\n",
      "Train Epoch: 53 [43520/50000 (97%)]\tLoss: 0.243698, Accuracy: 90.82\n",
      "\n",
      "Validation set: Average loss: 0.8814, Accuracy: 3810/5000 (76.00%)\n",
      "\n",
      "the time of this epoch:[39.46451544761658 s]\n",
      "Train Epoch: 54 [0/50000 (0%)]\tLoss: 0.155053, Accuracy: 94.53\n",
      "Train Epoch: 54 [2560/50000 (6%)]\tLoss: 0.206871, Accuracy: 92.77\n",
      "Train Epoch: 54 [5120/50000 (11%)]\tLoss: 0.246888, Accuracy: 90.43\n",
      "Train Epoch: 54 [7680/50000 (17%)]\tLoss: 0.258929, Accuracy: 90.23\n",
      "Train Epoch: 54 [10240/50000 (23%)]\tLoss: 0.183590, Accuracy: 93.55\n",
      "Train Epoch: 54 [12800/50000 (28%)]\tLoss: 0.220377, Accuracy: 93.36\n",
      "Train Epoch: 54 [15360/50000 (34%)]\tLoss: 0.243088, Accuracy: 92.97\n",
      "Train Epoch: 54 [17920/50000 (40%)]\tLoss: 0.258384, Accuracy: 91.99\n",
      "Train Epoch: 54 [20480/50000 (45%)]\tLoss: 0.288874, Accuracy: 90.23\n",
      "Train Epoch: 54 [23040/50000 (51%)]\tLoss: 0.194899, Accuracy: 93.55\n",
      "Train Epoch: 54 [25600/50000 (57%)]\tLoss: 0.286000, Accuracy: 90.62\n",
      "Train Epoch: 54 [28160/50000 (62%)]\tLoss: 0.225176, Accuracy: 92.38\n",
      "Train Epoch: 54 [30720/50000 (68%)]\tLoss: 0.203655, Accuracy: 92.77\n",
      "Train Epoch: 54 [33280/50000 (74%)]\tLoss: 0.217146, Accuracy: 92.19\n",
      "Train Epoch: 54 [35840/50000 (80%)]\tLoss: 0.249861, Accuracy: 91.80\n",
      "Train Epoch: 54 [38400/50000 (85%)]\tLoss: 0.294878, Accuracy: 91.41\n",
      "Train Epoch: 54 [40960/50000 (91%)]\tLoss: 0.233310, Accuracy: 93.36\n",
      "Train Epoch: 54 [43520/50000 (97%)]\tLoss: 0.225084, Accuracy: 92.19\n",
      "\n",
      "Validation set: Average loss: 0.5060, Accuracy: 4184/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.27211785316467 s]\n",
      "\n",
      "Test set: Average loss: 0.5077, Accuracy: 8393/10000 (83.93%)\n",
      "\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0000]],\n",
      "\n",
      "        [[ 0.1062]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.1101]],\n",
      "\n",
      "        [[ 0.0694]],\n",
      "\n",
      "        [[ 0.0757]],\n",
      "\n",
      "        [[ 0.0096]],\n",
      "\n",
      "        [[ 0.0292]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0872]],\n",
      "\n",
      "        [[ 0.1526]],\n",
      "\n",
      "        [[ 0.0106]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0615]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0215]],\n",
      "\n",
      "        [[ 0.0329]],\n",
      "\n",
      "        [[ 0.0530]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0505]],\n",
      "\n",
      "        [[ 0.0059]],\n",
      "\n",
      "        [[ 0.0902]],\n",
      "\n",
      "        [[ 0.0938]],\n",
      "\n",
      "        [[ 0.0457]],\n",
      "\n",
      "        [[ 0.0166]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0658]],\n",
      "\n",
      "        [[ 0.0171]],\n",
      "\n",
      "        [[ 0.0388]],\n",
      "\n",
      "        [[ 0.0321]],\n",
      "\n",
      "        [[ 0.0677]],\n",
      "\n",
      "        [[ 0.0770]],\n",
      "\n",
      "        [[ 0.0513]],\n",
      "\n",
      "        [[ 0.1186]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.1233]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0119]],\n",
      "\n",
      "        [[ 0.1087]],\n",
      "\n",
      "        [[ 0.0371]],\n",
      "\n",
      "        [[ 0.0873]],\n",
      "\n",
      "        [[ 0.0649]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0022]],\n",
      "\n",
      "        [[ 0.0266]],\n",
      "\n",
      "        [[ 0.0693]],\n",
      "\n",
      "        [[ 0.0457]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0252]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0037]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0366]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0252]],\n",
      "\n",
      "        [[ 0.0792]],\n",
      "\n",
      "        [[ 0.0319]],\n",
      "\n",
      "        [[ 0.0970]],\n",
      "\n",
      "        [[ 0.0115]],\n",
      "\n",
      "        [[ 0.0104]],\n",
      "\n",
      "        [[ 0.0885]],\n",
      "\n",
      "        [[ 0.0287]],\n",
      "\n",
      "        [[ 0.0166]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0130]],\n",
      "\n",
      "        [[ 0.0221]],\n",
      "\n",
      "        [[ 0.0309]],\n",
      "\n",
      "        [[ 0.0223]],\n",
      "\n",
      "        [[ 0.0118]],\n",
      "\n",
      "        [[ 0.0196]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0204]],\n",
      "\n",
      "        [[ 0.0209]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0180]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0308]],\n",
      "\n",
      "        [[ 0.0194]],\n",
      "\n",
      "        [[ 0.0126]],\n",
      "\n",
      "        [[ 0.0185]],\n",
      "\n",
      "        [[ 0.0446]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0080]],\n",
      "\n",
      "        [[ 0.0099]],\n",
      "\n",
      "        [[ 0.0375]],\n",
      "\n",
      "        [[ 0.0168]],\n",
      "\n",
      "        [[ 0.0115]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0282]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0109]],\n",
      "\n",
      "        [[ 0.0207]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0191]],\n",
      "\n",
      "        [[ 0.0128]],\n",
      "\n",
      "        [[ 0.0210]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0158]],\n",
      "\n",
      "        [[ 0.0108]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.0124]],\n",
      "\n",
      "        [[ 0.0334]],\n",
      "\n",
      "        [[ 0.0144]],\n",
      "\n",
      "        [[ 0.0303]],\n",
      "\n",
      "        [[ 0.0215]],\n",
      "\n",
      "        [[ 0.0181]],\n",
      "\n",
      "        [[ 0.0206]],\n",
      "\n",
      "        [[ 0.0169]],\n",
      "\n",
      "        [[ 0.0313]],\n",
      "\n",
      "        [[ 0.0139]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0191]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0248]],\n",
      "\n",
      "        [[ 0.0288]],\n",
      "\n",
      "        [[ 0.0091]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0133]],\n",
      "\n",
      "        [[ 0.0170]],\n",
      "\n",
      "        [[ 0.0241]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0228]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0177]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0142]],\n",
      "\n",
      "        [[ 0.0267]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.0272]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.0246]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0140]],\n",
      "\n",
      "        [[ 0.0148]],\n",
      "\n",
      "        [[ 0.0073]],\n",
      "\n",
      "        [[ 0.0178]],\n",
      "\n",
      "        [[ 0.0530]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0215]],\n",
      "\n",
      "        [[ 0.0059]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0255]],\n",
      "\n",
      "        [[ 0.0152]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0379]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.0226]],\n",
      "\n",
      "        [[ 0.0152]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0213]],\n",
      "\n",
      "        [[ 0.0387]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0093]],\n",
      "\n",
      "        [[ 0.0222]],\n",
      "\n",
      "        [[ 0.0275]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.0206]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0478]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0491]],\n",
      "\n",
      "        [[ 0.0189]],\n",
      "\n",
      "        [[ 0.0374]],\n",
      "\n",
      "        [[ 0.0268]],\n",
      "\n",
      "        [[ 0.0212]],\n",
      "\n",
      "        [[ 0.0283]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0026]],\n",
      "\n",
      "        [[ 0.0201]],\n",
      "\n",
      "        [[ 0.0129]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0128]],\n",
      "\n",
      "        [[ 0.0238]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0243]],\n",
      "\n",
      "        [[ 0.0216]],\n",
      "\n",
      "        [[ 0.0232]],\n",
      "\n",
      "        [[ 0.0349]],\n",
      "\n",
      "        [[ 0.0290]],\n",
      "\n",
      "        [[ 0.0205]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.0349]],\n",
      "\n",
      "        [[ 0.0234]],\n",
      "\n",
      "        [[ 0.0069]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0149]],\n",
      "\n",
      "        [[ 0.0163]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0249]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0105]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.0474]],\n",
      "\n",
      "        [[ 0.0026]],\n",
      "\n",
      "        [[ 0.0217]],\n",
      "\n",
      "        [[ 0.0128]],\n",
      "\n",
      "        [[ 0.0318]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0388]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0106]],\n",
      "\n",
      "        [[ 0.0156]],\n",
      "\n",
      "        [[ 0.0117]],\n",
      "\n",
      "        [[ 0.0188]],\n",
      "\n",
      "        [[ 0.0221]],\n",
      "\n",
      "        [[ 0.0344]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0013]],\n",
      "\n",
      "        [[ 0.0177]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0227]],\n",
      "\n",
      "        [[ 0.0266]],\n",
      "\n",
      "        [[ 0.0198]],\n",
      "\n",
      "        [[ 0.0344]],\n",
      "\n",
      "        [[ 0.0172]],\n",
      "\n",
      "        [[ 0.0159]],\n",
      "\n",
      "        [[ 0.0290]],\n",
      "\n",
      "        [[ 0.0188]],\n",
      "\n",
      "        [[ 0.0161]],\n",
      "\n",
      "        [[ 0.0206]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0290]],\n",
      "\n",
      "        [[ 0.0797]],\n",
      "\n",
      "        [[ 0.0242]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0266]],\n",
      "\n",
      "        [[ 0.0314]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0324]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0117]],\n",
      "\n",
      "        [[ 0.0148]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0173]],\n",
      "\n",
      "        [[ 0.0357]],\n",
      "\n",
      "        [[ 0.0121]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0221]],\n",
      "\n",
      "        [[ 0.0216]],\n",
      "\n",
      "        [[ 0.0271]],\n",
      "\n",
      "        [[ 0.0305]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0138]],\n",
      "\n",
      "        [[ 0.0243]],\n",
      "\n",
      "        [[ 0.0115]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.0217]],\n",
      "\n",
      "        [[ 0.0246]],\n",
      "\n",
      "        [[ 0.0142]],\n",
      "\n",
      "        [[ 0.0370]],\n",
      "\n",
      "        [[ 0.0124]],\n",
      "\n",
      "        [[ 0.0225]],\n",
      "\n",
      "        [[ 0.0117]],\n",
      "\n",
      "        [[ 0.0413]],\n",
      "\n",
      "        [[ 0.0295]],\n",
      "\n",
      "        [[ 0.0267]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.0188]],\n",
      "\n",
      "        [[ 0.0132]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0226]],\n",
      "\n",
      "        [[ 0.0028]],\n",
      "\n",
      "        [[ 0.0101]],\n",
      "\n",
      "        [[ 0.0163]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0313]],\n",
      "\n",
      "        [[ 0.0142]],\n",
      "\n",
      "        [[ 0.0068]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.0105]],\n",
      "\n",
      "        [[ 0.0100]],\n",
      "\n",
      "        [[ 0.0208]],\n",
      "\n",
      "        [[ 0.0215]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0212]],\n",
      "\n",
      "        [[ 0.0153]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0264]],\n",
      "\n",
      "        [[ 0.0312]],\n",
      "\n",
      "        [[ 0.0172]],\n",
      "\n",
      "        [[ 0.0451]],\n",
      "\n",
      "        [[ 0.0202]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0379]],\n",
      "\n",
      "        [[ 0.0207]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0199]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0404]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0159]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.0216]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0174]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0415]],\n",
      "\n",
      "        [[ 0.0311]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0107]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0120]],\n",
      "\n",
      "        [[ 0.0251]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0167]],\n",
      "\n",
      "        [[ 0.0012]],\n",
      "\n",
      "        [[ 0.0188]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0145]],\n",
      "\n",
      "        [[ 0.0193]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0158]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0118]],\n",
      "\n",
      "        [[ 0.0174]],\n",
      "\n",
      "        [[ 0.0146]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0130]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0198]],\n",
      "\n",
      "        [[ 0.0164]],\n",
      "\n",
      "        [[ 0.0153]],\n",
      "\n",
      "        [[ 0.0261]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0238]],\n",
      "\n",
      "        [[ 0.0146]],\n",
      "\n",
      "        [[ 0.0181]],\n",
      "\n",
      "        [[ 0.0084]],\n",
      "\n",
      "        [[ 0.0065]],\n",
      "\n",
      "        [[ 0.0358]],\n",
      "\n",
      "        [[ 0.0254]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.0211]],\n",
      "\n",
      "        [[ 0.0208]],\n",
      "\n",
      "        [[ 0.0132]],\n",
      "\n",
      "        [[ 0.0247]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0088]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.0138]],\n",
      "\n",
      "        [[ 0.0212]],\n",
      "\n",
      "        [[ 0.0395]],\n",
      "\n",
      "        [[ 0.0380]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0281]],\n",
      "\n",
      "        [[ 0.0046]],\n",
      "\n",
      "        [[ 0.0216]],\n",
      "\n",
      "        [[ 0.0287]],\n",
      "\n",
      "        [[ 0.0369]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0249]],\n",
      "\n",
      "        [[ 0.0462]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0026]],\n",
      "\n",
      "        [[ 0.0189]],\n",
      "\n",
      "        [[ 0.0234]],\n",
      "\n",
      "        [[ 0.0119]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0123]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0059]],\n",
      "\n",
      "        [[ 0.0288]],\n",
      "\n",
      "        [[ 0.0286]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0271]],\n",
      "\n",
      "        [[ 0.0085]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0328]],\n",
      "\n",
      "        [[ 0.0221]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0165]],\n",
      "\n",
      "        [[ 0.0237]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0304]],\n",
      "\n",
      "        [[ 0.0224]],\n",
      "\n",
      "        [[ 0.0026]],\n",
      "\n",
      "        [[ 0.0166]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0159]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0178]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0158]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0115]],\n",
      "\n",
      "        [[ 0.0157]],\n",
      "\n",
      "        [[ 0.0402]],\n",
      "\n",
      "        [[ 0.0062]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0121]],\n",
      "\n",
      "        [[ 0.0130]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0329]],\n",
      "\n",
      "        [[ 0.0201]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.0175]],\n",
      "\n",
      "        [[ 0.0092]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.0163]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0219]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0105]],\n",
      "\n",
      "        [[ 0.0102]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0331]],\n",
      "\n",
      "        [[ 0.0107]],\n",
      "\n",
      "        [[ 0.0327]],\n",
      "\n",
      "        [[ 0.0155]],\n",
      "\n",
      "        [[ 0.0678]],\n",
      "\n",
      "        [[ 0.0203]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0237]],\n",
      "\n",
      "        [[ 0.0235]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0240]],\n",
      "\n",
      "        [[ 0.0103]],\n",
      "\n",
      "        [[ 0.0129]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0164]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0325]],\n",
      "\n",
      "        [[ 0.0250]],\n",
      "\n",
      "        [[ 0.0154]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0121]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0608]],\n",
      "\n",
      "        [[ 0.0340]],\n",
      "\n",
      "        [[ 0.0239]],\n",
      "\n",
      "        [[ 0.0264]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0193]],\n",
      "\n",
      "        [[ 0.0273]],\n",
      "\n",
      "        [[ 0.0295]],\n",
      "\n",
      "        [[ 0.0353]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0364]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0185]],\n",
      "\n",
      "        [[ 0.0230]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0303]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0203]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0182]],\n",
      "\n",
      "        [[ 0.0216]],\n",
      "\n",
      "        [[ 0.0149]],\n",
      "\n",
      "        [[ 0.0284]],\n",
      "\n",
      "        [[ 0.0279]],\n",
      "\n",
      "        [[ 0.0165]],\n",
      "\n",
      "        [[ 0.0226]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0152]],\n",
      "\n",
      "        [[ 0.0177]],\n",
      "\n",
      "        [[ 0.0214]],\n",
      "\n",
      "        [[ 0.0063]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0141]],\n",
      "\n",
      "        [[ 0.0202]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 2.5744]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 0.2290, -2.7547,  2.4719,  ..., -0.9201, -0.5117,  0.9064]],\n",
      "\n",
      "        [[ 0.5547, -2.1172,  1.6698,  ...,  1.2420, -2.7529,  2.6065]],\n",
      "\n",
      "        [[ 0.2275,  2.5690,  1.8535,  ..., -1.4578, -1.4950, -0.2520]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.0899, -2.8127, -2.8877,  ...,  1.4872, -0.2843, -2.3071]],\n",
      "\n",
      "        [[ 1.1523, -0.5276,  0.8963,  ...,  2.0742, -0.8672, -1.2547]],\n",
      "\n",
      "        [[ 1.0811, -1.9558, -0.0809,  ...,  2.9874,  1.1582, -0.5547]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor(1.00000e-03 *\n",
      "       [[[-0.3400, -0.8088, -1.3544,  ..., -0.7177, -0.9188, -0.9546],\n",
      "         [ 0.7149,  1.2016,  0.8240,  ...,  1.1437,  1.4161,  1.0370],\n",
      "         [ 1.1366,  0.6938,  0.2857,  ..., -0.7138, -1.0336, -1.6294],\n",
      "         ...,\n",
      "         [ 0.3208,  0.5094,  1.1445,  ...,  0.7409,  1.0667,  1.0985],\n",
      "         [ 0.7456,  0.5271,  0.3316,  ..., -0.2691,  0.1858, -0.4356],\n",
      "         [-0.6989, -1.2472, -1.2137,  ..., -0.8960, -0.2401,  0.1554]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 55 [0/50000 (0%)]\tLoss: 0.241980, Accuracy: 92.58\n",
      "Train Epoch: 55 [2560/50000 (6%)]\tLoss: 0.203301, Accuracy: 93.55\n",
      "Train Epoch: 55 [5120/50000 (11%)]\tLoss: 0.177597, Accuracy: 94.34\n",
      "Train Epoch: 55 [7680/50000 (17%)]\tLoss: 0.209715, Accuracy: 92.19\n",
      "Train Epoch: 55 [10240/50000 (23%)]\tLoss: 0.238280, Accuracy: 91.80\n",
      "Train Epoch: 55 [12800/50000 (28%)]\tLoss: 0.241791, Accuracy: 91.02\n",
      "Train Epoch: 55 [15360/50000 (34%)]\tLoss: 0.228430, Accuracy: 92.38\n",
      "Train Epoch: 55 [17920/50000 (40%)]\tLoss: 0.220081, Accuracy: 92.97\n",
      "Train Epoch: 55 [20480/50000 (45%)]\tLoss: 0.202208, Accuracy: 94.53\n",
      "Train Epoch: 55 [23040/50000 (51%)]\tLoss: 0.254036, Accuracy: 90.82\n",
      "Train Epoch: 55 [25600/50000 (57%)]\tLoss: 0.268724, Accuracy: 91.41\n",
      "Train Epoch: 55 [28160/50000 (62%)]\tLoss: 0.236895, Accuracy: 92.19\n",
      "Train Epoch: 55 [30720/50000 (68%)]\tLoss: 0.192790, Accuracy: 93.75\n",
      "Train Epoch: 55 [33280/50000 (74%)]\tLoss: 0.223386, Accuracy: 92.77\n",
      "Train Epoch: 55 [35840/50000 (80%)]\tLoss: 0.244328, Accuracy: 91.60\n",
      "Train Epoch: 55 [38400/50000 (85%)]\tLoss: 0.248687, Accuracy: 91.21\n",
      "Train Epoch: 55 [40960/50000 (91%)]\tLoss: 0.213436, Accuracy: 92.58\n",
      "Train Epoch: 55 [43520/50000 (97%)]\tLoss: 0.211593, Accuracy: 92.19\n",
      "\n",
      "Validation set: Average loss: 0.6374, Accuracy: 4016/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[39.36036133766174 s]\n",
      "Train Epoch: 56 [0/50000 (0%)]\tLoss: 0.226882, Accuracy: 91.80\n",
      "Train Epoch: 56 [2560/50000 (6%)]\tLoss: 0.258767, Accuracy: 91.02\n",
      "Train Epoch: 56 [5120/50000 (11%)]\tLoss: 0.198010, Accuracy: 94.14\n",
      "Train Epoch: 56 [7680/50000 (17%)]\tLoss: 0.194095, Accuracy: 92.97\n",
      "Train Epoch: 56 [10240/50000 (23%)]\tLoss: 0.245730, Accuracy: 91.21\n",
      "Train Epoch: 56 [12800/50000 (28%)]\tLoss: 0.231958, Accuracy: 91.99\n",
      "Train Epoch: 56 [15360/50000 (34%)]\tLoss: 0.235072, Accuracy: 91.60\n",
      "Train Epoch: 56 [17920/50000 (40%)]\tLoss: 0.236652, Accuracy: 93.16\n",
      "Train Epoch: 56 [20480/50000 (45%)]\tLoss: 0.232534, Accuracy: 91.80\n",
      "Train Epoch: 56 [23040/50000 (51%)]\tLoss: 0.229375, Accuracy: 91.99\n",
      "Train Epoch: 56 [25600/50000 (57%)]\tLoss: 0.269231, Accuracy: 91.80\n",
      "Train Epoch: 56 [28160/50000 (62%)]\tLoss: 0.310769, Accuracy: 88.67\n",
      "Train Epoch: 56 [30720/50000 (68%)]\tLoss: 0.292014, Accuracy: 91.21\n",
      "Train Epoch: 56 [33280/50000 (74%)]\tLoss: 0.205130, Accuracy: 93.95\n",
      "Train Epoch: 56 [35840/50000 (80%)]\tLoss: 0.258767, Accuracy: 91.02\n",
      "Train Epoch: 56 [38400/50000 (85%)]\tLoss: 0.232308, Accuracy: 92.19\n",
      "Train Epoch: 56 [40960/50000 (91%)]\tLoss: 0.217947, Accuracy: 91.99\n",
      "Train Epoch: 56 [43520/50000 (97%)]\tLoss: 0.226368, Accuracy: 91.60\n",
      "\n",
      "Validation set: Average loss: 0.4670, Accuracy: 4252/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[36.26202058792114 s]\n",
      "\n",
      "Test set: Average loss: 0.5094, Accuracy: 8427/10000 (84.27%)\n",
      "\n",
      "Train Epoch: 57 [0/50000 (0%)]\tLoss: 0.217632, Accuracy: 93.16\n",
      "Train Epoch: 57 [2560/50000 (6%)]\tLoss: 0.201000, Accuracy: 93.16\n",
      "Train Epoch: 57 [5120/50000 (11%)]\tLoss: 0.223204, Accuracy: 91.99\n",
      "Train Epoch: 57 [7680/50000 (17%)]\tLoss: 0.195547, Accuracy: 92.38\n",
      "Train Epoch: 57 [10240/50000 (23%)]\tLoss: 0.236699, Accuracy: 91.60\n",
      "Train Epoch: 57 [12800/50000 (28%)]\tLoss: 0.238162, Accuracy: 90.62\n",
      "Train Epoch: 57 [15360/50000 (34%)]\tLoss: 0.243390, Accuracy: 91.80\n",
      "Train Epoch: 57 [17920/50000 (40%)]\tLoss: 0.203686, Accuracy: 93.95\n",
      "Train Epoch: 57 [20480/50000 (45%)]\tLoss: 0.235092, Accuracy: 91.60\n",
      "Train Epoch: 57 [23040/50000 (51%)]\tLoss: 0.199390, Accuracy: 92.77\n",
      "Train Epoch: 57 [25600/50000 (57%)]\tLoss: 0.227303, Accuracy: 91.60\n",
      "Train Epoch: 57 [28160/50000 (62%)]\tLoss: 0.227960, Accuracy: 92.77\n",
      "Train Epoch: 57 [30720/50000 (68%)]\tLoss: 0.268128, Accuracy: 90.04\n",
      "Train Epoch: 57 [33280/50000 (74%)]\tLoss: 0.265619, Accuracy: 90.43\n",
      "Train Epoch: 57 [35840/50000 (80%)]\tLoss: 0.214930, Accuracy: 92.38\n",
      "Train Epoch: 57 [38400/50000 (85%)]\tLoss: 0.282578, Accuracy: 90.43\n",
      "Train Epoch: 57 [40960/50000 (91%)]\tLoss: 0.256612, Accuracy: 91.41\n",
      "Train Epoch: 57 [43520/50000 (97%)]\tLoss: 0.248228, Accuracy: 91.21\n",
      "\n",
      "Validation set: Average loss: 0.7246, Accuracy: 3975/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[39.32860589027405 s]\n",
      "Train Epoch: 58 [0/50000 (0%)]\tLoss: 0.254531, Accuracy: 92.19\n",
      "Train Epoch: 58 [2560/50000 (6%)]\tLoss: 0.237807, Accuracy: 91.80\n",
      "Train Epoch: 58 [5120/50000 (11%)]\tLoss: 0.204275, Accuracy: 94.34\n",
      "Train Epoch: 58 [7680/50000 (17%)]\tLoss: 0.308944, Accuracy: 89.45\n",
      "Train Epoch: 58 [10240/50000 (23%)]\tLoss: 0.228112, Accuracy: 91.80\n",
      "Train Epoch: 58 [12800/50000 (28%)]\tLoss: 0.200852, Accuracy: 93.36\n",
      "Train Epoch: 58 [15360/50000 (34%)]\tLoss: 0.203683, Accuracy: 92.97\n",
      "Train Epoch: 58 [17920/50000 (40%)]\tLoss: 0.212017, Accuracy: 92.38\n",
      "Train Epoch: 58 [20480/50000 (45%)]\tLoss: 0.262388, Accuracy: 90.62\n",
      "Train Epoch: 58 [23040/50000 (51%)]\tLoss: 0.329657, Accuracy: 88.67\n",
      "Train Epoch: 58 [25600/50000 (57%)]\tLoss: 0.263266, Accuracy: 90.04\n",
      "Train Epoch: 58 [28160/50000 (62%)]\tLoss: 0.239256, Accuracy: 91.21\n",
      "Train Epoch: 58 [30720/50000 (68%)]\tLoss: 0.238934, Accuracy: 91.80\n",
      "Train Epoch: 58 [33280/50000 (74%)]\tLoss: 0.285856, Accuracy: 90.43\n",
      "Train Epoch: 58 [35840/50000 (80%)]\tLoss: 0.211408, Accuracy: 92.97\n",
      "Train Epoch: 58 [38400/50000 (85%)]\tLoss: 0.247309, Accuracy: 91.41\n",
      "Train Epoch: 58 [40960/50000 (91%)]\tLoss: 0.324936, Accuracy: 88.87\n",
      "Train Epoch: 58 [43520/50000 (97%)]\tLoss: 0.212061, Accuracy: 93.55\n",
      "\n",
      "Validation set: Average loss: 0.5413, Accuracy: 4160/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.24424862861633 s]\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 8357/10000 (83.57%)\n",
      "\n",
      "Train Epoch: 59 [0/50000 (0%)]\tLoss: 0.229901, Accuracy: 92.19\n",
      "Train Epoch: 59 [2560/50000 (6%)]\tLoss: 0.208557, Accuracy: 91.80\n",
      "Train Epoch: 59 [5120/50000 (11%)]\tLoss: 0.235740, Accuracy: 91.41\n",
      "Train Epoch: 59 [7680/50000 (17%)]\tLoss: 0.221969, Accuracy: 91.60\n",
      "Train Epoch: 59 [10240/50000 (23%)]\tLoss: 0.207945, Accuracy: 91.80\n",
      "Train Epoch: 59 [12800/50000 (28%)]\tLoss: 0.208559, Accuracy: 93.16\n",
      "Train Epoch: 59 [15360/50000 (34%)]\tLoss: 0.246050, Accuracy: 90.62\n",
      "Train Epoch: 59 [17920/50000 (40%)]\tLoss: 0.230123, Accuracy: 92.58\n",
      "Train Epoch: 59 [20480/50000 (45%)]\tLoss: 0.177997, Accuracy: 94.34\n",
      "Train Epoch: 59 [23040/50000 (51%)]\tLoss: 0.272805, Accuracy: 90.23\n",
      "Train Epoch: 59 [25600/50000 (57%)]\tLoss: 0.179708, Accuracy: 93.55\n",
      "Train Epoch: 59 [28160/50000 (62%)]\tLoss: 0.203991, Accuracy: 93.16\n",
      "Train Epoch: 59 [30720/50000 (68%)]\tLoss: 0.236891, Accuracy: 93.36\n",
      "Train Epoch: 59 [33280/50000 (74%)]\tLoss: 0.192684, Accuracy: 93.16\n",
      "Train Epoch: 59 [35840/50000 (80%)]\tLoss: 0.228383, Accuracy: 91.80\n",
      "Train Epoch: 59 [38400/50000 (85%)]\tLoss: 0.226929, Accuracy: 91.60\n",
      "Train Epoch: 59 [40960/50000 (91%)]\tLoss: 0.256313, Accuracy: 90.82\n",
      "Train Epoch: 59 [43520/50000 (97%)]\tLoss: 0.220394, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.4408, Accuracy: 4287/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[39.37209105491638 s]\n",
      "Train Epoch: 60 [0/50000 (0%)]\tLoss: 0.226175, Accuracy: 92.19\n",
      "Train Epoch: 60 [2560/50000 (6%)]\tLoss: 0.227942, Accuracy: 91.99\n",
      "Train Epoch: 60 [5120/50000 (11%)]\tLoss: 0.225452, Accuracy: 93.55\n",
      "Train Epoch: 60 [7680/50000 (17%)]\tLoss: 0.211269, Accuracy: 91.99\n",
      "Train Epoch: 60 [10240/50000 (23%)]\tLoss: 0.230980, Accuracy: 92.19\n",
      "Train Epoch: 60 [12800/50000 (28%)]\tLoss: 0.311987, Accuracy: 89.26\n",
      "Train Epoch: 60 [15360/50000 (34%)]\tLoss: 0.265268, Accuracy: 90.62\n",
      "Train Epoch: 60 [17920/50000 (40%)]\tLoss: 0.282371, Accuracy: 89.84\n",
      "Train Epoch: 60 [20480/50000 (45%)]\tLoss: 0.283789, Accuracy: 91.21\n",
      "Train Epoch: 60 [23040/50000 (51%)]\tLoss: 0.263890, Accuracy: 91.02\n",
      "Train Epoch: 60 [25600/50000 (57%)]\tLoss: 0.211174, Accuracy: 93.16\n",
      "Train Epoch: 60 [28160/50000 (62%)]\tLoss: 0.237407, Accuracy: 93.55\n",
      "Train Epoch: 60 [30720/50000 (68%)]\tLoss: 0.260103, Accuracy: 92.19\n",
      "Train Epoch: 60 [33280/50000 (74%)]\tLoss: 0.247214, Accuracy: 91.41\n",
      "Train Epoch: 60 [35840/50000 (80%)]\tLoss: 0.204788, Accuracy: 92.58\n",
      "Train Epoch: 60 [38400/50000 (85%)]\tLoss: 0.221793, Accuracy: 92.38\n",
      "Train Epoch: 60 [40960/50000 (91%)]\tLoss: 0.248161, Accuracy: 91.21\n",
      "Train Epoch: 60 [43520/50000 (97%)]\tLoss: 0.212052, Accuracy: 92.77\n",
      "\n",
      "Validation set: Average loss: 0.7199, Accuracy: 3976/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[36.237542390823364 s]\n",
      "\n",
      "Test set: Average loss: 0.7577, Accuracy: 7857/10000 (78.57%)\n",
      "\n",
      "Train Epoch: 61 [0/50000 (0%)]\tLoss: 0.180862, Accuracy: 93.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 61 [2560/50000 (6%)]\tLoss: 0.175836, Accuracy: 93.36\n",
      "Train Epoch: 61 [5120/50000 (11%)]\tLoss: 0.192006, Accuracy: 93.95\n",
      "Train Epoch: 61 [7680/50000 (17%)]\tLoss: 0.220573, Accuracy: 91.41\n",
      "Train Epoch: 61 [10240/50000 (23%)]\tLoss: 0.191846, Accuracy: 92.97\n",
      "Train Epoch: 61 [12800/50000 (28%)]\tLoss: 0.174692, Accuracy: 93.75\n",
      "Train Epoch: 61 [15360/50000 (34%)]\tLoss: 0.225491, Accuracy: 92.58\n",
      "Train Epoch: 61 [17920/50000 (40%)]\tLoss: 0.216419, Accuracy: 92.38\n",
      "Train Epoch: 61 [20480/50000 (45%)]\tLoss: 0.245571, Accuracy: 91.99\n",
      "Train Epoch: 61 [23040/50000 (51%)]\tLoss: 0.197739, Accuracy: 93.55\n",
      "Train Epoch: 61 [25600/50000 (57%)]\tLoss: 0.214446, Accuracy: 92.77\n",
      "Train Epoch: 61 [28160/50000 (62%)]\tLoss: 0.241475, Accuracy: 92.58\n",
      "Train Epoch: 61 [30720/50000 (68%)]\tLoss: 0.261674, Accuracy: 90.82\n",
      "Train Epoch: 61 [33280/50000 (74%)]\tLoss: 0.232503, Accuracy: 90.62\n",
      "Train Epoch: 61 [35840/50000 (80%)]\tLoss: 0.269856, Accuracy: 90.62\n",
      "Train Epoch: 61 [38400/50000 (85%)]\tLoss: 0.226806, Accuracy: 93.16\n",
      "Train Epoch: 61 [40960/50000 (91%)]\tLoss: 0.275415, Accuracy: 89.84\n",
      "Train Epoch: 61 [43520/50000 (97%)]\tLoss: 0.190168, Accuracy: 92.97\n",
      "\n",
      "Validation set: Average loss: 0.7226, Accuracy: 3991/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[39.31329703330994 s]\n",
      "Train Epoch: 62 [0/50000 (0%)]\tLoss: 0.193767, Accuracy: 93.95\n",
      "Train Epoch: 62 [2560/50000 (6%)]\tLoss: 0.252434, Accuracy: 90.62\n",
      "Train Epoch: 62 [5120/50000 (11%)]\tLoss: 0.240209, Accuracy: 91.80\n",
      "Train Epoch: 62 [7680/50000 (17%)]\tLoss: 0.291814, Accuracy: 90.23\n",
      "Train Epoch: 62 [10240/50000 (23%)]\tLoss: 0.202676, Accuracy: 92.38\n",
      "Train Epoch: 62 [12800/50000 (28%)]\tLoss: 0.198869, Accuracy: 93.36\n",
      "Train Epoch: 62 [15360/50000 (34%)]\tLoss: 0.247447, Accuracy: 91.60\n",
      "Train Epoch: 62 [17920/50000 (40%)]\tLoss: 0.257854, Accuracy: 90.62\n",
      "Train Epoch: 62 [20480/50000 (45%)]\tLoss: 0.234523, Accuracy: 92.38\n",
      "Train Epoch: 62 [23040/50000 (51%)]\tLoss: 0.253798, Accuracy: 91.60\n",
      "Train Epoch: 62 [25600/50000 (57%)]\tLoss: 0.240432, Accuracy: 92.19\n",
      "Train Epoch: 62 [28160/50000 (62%)]\tLoss: 0.208482, Accuracy: 93.16\n",
      "Train Epoch: 62 [30720/50000 (68%)]\tLoss: 0.186949, Accuracy: 93.95\n",
      "Train Epoch: 62 [33280/50000 (74%)]\tLoss: 0.230204, Accuracy: 91.99\n",
      "Train Epoch: 62 [35840/50000 (80%)]\tLoss: 0.205381, Accuracy: 93.95\n",
      "Train Epoch: 62 [38400/50000 (85%)]\tLoss: 0.229135, Accuracy: 91.80\n",
      "Train Epoch: 62 [40960/50000 (91%)]\tLoss: 0.255967, Accuracy: 91.99\n",
      "Train Epoch: 62 [43520/50000 (97%)]\tLoss: 0.279275, Accuracy: 91.41\n",
      "\n",
      "Validation set: Average loss: 0.6357, Accuracy: 4057/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.28371262550354 s]\n",
      "\n",
      "Test set: Average loss: 0.6528, Accuracy: 8115/10000 (81.15%)\n",
      "\n",
      "Train Epoch: 63 [0/50000 (0%)]\tLoss: 0.181102, Accuracy: 94.92\n",
      "Train Epoch: 63 [2560/50000 (6%)]\tLoss: 0.233776, Accuracy: 91.41\n",
      "Train Epoch: 63 [5120/50000 (11%)]\tLoss: 0.195189, Accuracy: 93.36\n",
      "Train Epoch: 63 [7680/50000 (17%)]\tLoss: 0.210224, Accuracy: 92.77\n",
      "Train Epoch: 63 [10240/50000 (23%)]\tLoss: 0.165829, Accuracy: 94.34\n",
      "Train Epoch: 63 [12800/50000 (28%)]\tLoss: 0.202094, Accuracy: 93.75\n",
      "Train Epoch: 63 [15360/50000 (34%)]\tLoss: 0.221295, Accuracy: 91.99\n",
      "Train Epoch: 63 [17920/50000 (40%)]\tLoss: 0.246066, Accuracy: 91.60\n",
      "Train Epoch: 63 [20480/50000 (45%)]\tLoss: 0.208473, Accuracy: 92.77\n",
      "Train Epoch: 63 [23040/50000 (51%)]\tLoss: 0.193418, Accuracy: 93.16\n",
      "Train Epoch: 63 [25600/50000 (57%)]\tLoss: 0.239421, Accuracy: 91.80\n",
      "Train Epoch: 63 [28160/50000 (62%)]\tLoss: 0.221978, Accuracy: 92.58\n",
      "Train Epoch: 63 [30720/50000 (68%)]\tLoss: 0.226080, Accuracy: 92.97\n",
      "Train Epoch: 63 [33280/50000 (74%)]\tLoss: 0.263433, Accuracy: 90.82\n",
      "Train Epoch: 63 [35840/50000 (80%)]\tLoss: 0.310884, Accuracy: 88.48\n",
      "Train Epoch: 63 [38400/50000 (85%)]\tLoss: 0.218164, Accuracy: 92.19\n",
      "Train Epoch: 63 [40960/50000 (91%)]\tLoss: 0.283521, Accuracy: 90.43\n",
      "Train Epoch: 63 [43520/50000 (97%)]\tLoss: 0.277271, Accuracy: 91.60\n",
      "\n",
      "Validation set: Average loss: 0.6963, Accuracy: 4014/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[39.358009576797485 s]\n",
      "Train Epoch: 64 [0/50000 (0%)]\tLoss: 0.214326, Accuracy: 93.16\n",
      "Train Epoch: 64 [2560/50000 (6%)]\tLoss: 0.251837, Accuracy: 91.02\n",
      "Train Epoch: 64 [5120/50000 (11%)]\tLoss: 0.215781, Accuracy: 93.16\n",
      "Train Epoch: 64 [7680/50000 (17%)]\tLoss: 0.240210, Accuracy: 91.80\n",
      "Train Epoch: 64 [10240/50000 (23%)]\tLoss: 0.169461, Accuracy: 93.36\n",
      "Train Epoch: 64 [12800/50000 (28%)]\tLoss: 0.188586, Accuracy: 93.36\n",
      "Train Epoch: 64 [15360/50000 (34%)]\tLoss: 0.244796, Accuracy: 91.80\n",
      "Train Epoch: 64 [17920/50000 (40%)]\tLoss: 0.252302, Accuracy: 91.21\n",
      "Train Epoch: 64 [20480/50000 (45%)]\tLoss: 0.209557, Accuracy: 91.99\n",
      "Train Epoch: 64 [23040/50000 (51%)]\tLoss: 0.237136, Accuracy: 91.80\n",
      "Train Epoch: 64 [25600/50000 (57%)]\tLoss: 0.193026, Accuracy: 92.58\n",
      "Train Epoch: 64 [28160/50000 (62%)]\tLoss: 0.226610, Accuracy: 91.99\n",
      "Train Epoch: 64 [30720/50000 (68%)]\tLoss: 0.215585, Accuracy: 92.19\n",
      "Train Epoch: 64 [33280/50000 (74%)]\tLoss: 0.216663, Accuracy: 92.97\n",
      "Train Epoch: 64 [35840/50000 (80%)]\tLoss: 0.234661, Accuracy: 91.21\n",
      "Train Epoch: 64 [38400/50000 (85%)]\tLoss: 0.227476, Accuracy: 91.60\n",
      "Train Epoch: 64 [40960/50000 (91%)]\tLoss: 0.236390, Accuracy: 91.80\n",
      "Train Epoch: 64 [43520/50000 (97%)]\tLoss: 0.273996, Accuracy: 90.23\n",
      "\n",
      "Validation set: Average loss: 0.6700, Accuracy: 4034/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.27685022354126 s]\n",
      "\n",
      "Test set: Average loss: 0.6670, Accuracy: 8054/10000 (80.54%)\n",
      "\n",
      "Train Epoch: 65 [0/50000 (0%)]\tLoss: 0.211564, Accuracy: 93.95\n",
      "Train Epoch: 65 [2560/50000 (6%)]\tLoss: 0.165995, Accuracy: 93.95\n",
      "Train Epoch: 65 [5120/50000 (11%)]\tLoss: 0.174940, Accuracy: 93.95\n",
      "Train Epoch: 65 [7680/50000 (17%)]\tLoss: 0.200992, Accuracy: 93.36\n",
      "Train Epoch: 65 [10240/50000 (23%)]\tLoss: 0.145350, Accuracy: 95.31\n",
      "Train Epoch: 65 [12800/50000 (28%)]\tLoss: 0.206627, Accuracy: 91.80\n",
      "Train Epoch: 65 [15360/50000 (34%)]\tLoss: 0.224727, Accuracy: 92.77\n",
      "Train Epoch: 65 [17920/50000 (40%)]\tLoss: 0.187276, Accuracy: 92.97\n",
      "Train Epoch: 65 [20480/50000 (45%)]\tLoss: 0.236006, Accuracy: 92.38\n",
      "Train Epoch: 65 [23040/50000 (51%)]\tLoss: 0.231015, Accuracy: 92.19\n",
      "Train Epoch: 65 [25600/50000 (57%)]\tLoss: 0.309123, Accuracy: 88.67\n",
      "Train Epoch: 65 [28160/50000 (62%)]\tLoss: 0.350499, Accuracy: 89.84\n",
      "Train Epoch: 65 [30720/50000 (68%)]\tLoss: 0.257012, Accuracy: 90.43\n",
      "Train Epoch: 65 [33280/50000 (74%)]\tLoss: 0.225155, Accuracy: 90.82\n",
      "Train Epoch: 65 [35840/50000 (80%)]\tLoss: 0.271043, Accuracy: 90.82\n",
      "Train Epoch: 65 [38400/50000 (85%)]\tLoss: 0.260700, Accuracy: 91.21\n",
      "Train Epoch: 65 [40960/50000 (91%)]\tLoss: 0.262731, Accuracy: 92.58\n",
      "Train Epoch: 65 [43520/50000 (97%)]\tLoss: 0.243267, Accuracy: 92.58\n",
      "\n",
      "Validation set: Average loss: 0.6696, Accuracy: 3989/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[39.36351299285889 s]\n",
      "Train Epoch: 66 [0/50000 (0%)]\tLoss: 0.233122, Accuracy: 92.97\n",
      "Train Epoch: 66 [2560/50000 (6%)]\tLoss: 0.196957, Accuracy: 92.58\n",
      "Train Epoch: 66 [5120/50000 (11%)]\tLoss: 0.176767, Accuracy: 94.14\n",
      "Train Epoch: 66 [7680/50000 (17%)]\tLoss: 0.187767, Accuracy: 93.55\n",
      "Train Epoch: 66 [10240/50000 (23%)]\tLoss: 0.250384, Accuracy: 91.99\n",
      "Train Epoch: 66 [12800/50000 (28%)]\tLoss: 0.243809, Accuracy: 91.41\n",
      "Train Epoch: 66 [15360/50000 (34%)]\tLoss: 0.175636, Accuracy: 94.92\n",
      "Train Epoch: 66 [17920/50000 (40%)]\tLoss: 0.231685, Accuracy: 91.60\n",
      "Train Epoch: 66 [20480/50000 (45%)]\tLoss: 0.186927, Accuracy: 92.77\n",
      "Train Epoch: 66 [23040/50000 (51%)]\tLoss: 0.243777, Accuracy: 91.80\n",
      "Train Epoch: 66 [25600/50000 (57%)]\tLoss: 0.231996, Accuracy: 92.77\n",
      "Train Epoch: 66 [28160/50000 (62%)]\tLoss: 0.204355, Accuracy: 93.16\n",
      "Train Epoch: 66 [30720/50000 (68%)]\tLoss: 0.215590, Accuracy: 93.75\n",
      "Train Epoch: 66 [33280/50000 (74%)]\tLoss: 0.217139, Accuracy: 91.99\n",
      "Train Epoch: 66 [35840/50000 (80%)]\tLoss: 0.267164, Accuracy: 90.23\n",
      "Train Epoch: 66 [38400/50000 (85%)]\tLoss: 0.266596, Accuracy: 91.02\n",
      "Train Epoch: 66 [40960/50000 (91%)]\tLoss: 0.245114, Accuracy: 91.99\n",
      "Train Epoch: 66 [43520/50000 (97%)]\tLoss: 0.192955, Accuracy: 93.75\n",
      "\n",
      "Validation set: Average loss: 0.5221, Accuracy: 4177/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.231934785842896 s]\n",
      "\n",
      "Test set: Average loss: 0.5475, Accuracy: 8298/10000 (82.98%)\n",
      "\n",
      "Train Epoch: 67 [0/50000 (0%)]\tLoss: 0.166042, Accuracy: 94.53\n",
      "Train Epoch: 67 [2560/50000 (6%)]\tLoss: 0.225137, Accuracy: 93.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 67 [5120/50000 (11%)]\tLoss: 0.185732, Accuracy: 91.99\n",
      "Train Epoch: 67 [7680/50000 (17%)]\tLoss: 0.212107, Accuracy: 93.75\n",
      "Train Epoch: 67 [10240/50000 (23%)]\tLoss: 0.164999, Accuracy: 94.34\n",
      "Train Epoch: 67 [12800/50000 (28%)]\tLoss: 0.149811, Accuracy: 95.12\n",
      "Train Epoch: 67 [15360/50000 (34%)]\tLoss: 0.218602, Accuracy: 92.77\n",
      "Train Epoch: 67 [17920/50000 (40%)]\tLoss: 0.224023, Accuracy: 92.58\n",
      "Train Epoch: 67 [20480/50000 (45%)]\tLoss: 0.246603, Accuracy: 92.58\n",
      "Train Epoch: 67 [23040/50000 (51%)]\tLoss: 0.201924, Accuracy: 92.77\n",
      "Train Epoch: 67 [25600/50000 (57%)]\tLoss: 0.211827, Accuracy: 93.16\n",
      "Train Epoch: 67 [28160/50000 (62%)]\tLoss: 0.217947, Accuracy: 92.97\n",
      "Train Epoch: 67 [30720/50000 (68%)]\tLoss: 0.225606, Accuracy: 91.60\n",
      "Train Epoch: 67 [33280/50000 (74%)]\tLoss: 0.195241, Accuracy: 93.36\n",
      "Train Epoch: 67 [35840/50000 (80%)]\tLoss: 0.264114, Accuracy: 91.41\n",
      "Train Epoch: 67 [38400/50000 (85%)]\tLoss: 0.274581, Accuracy: 89.84\n",
      "Train Epoch: 67 [40960/50000 (91%)]\tLoss: 0.257871, Accuracy: 90.62\n",
      "Train Epoch: 67 [43520/50000 (97%)]\tLoss: 0.284840, Accuracy: 90.82\n",
      "\n",
      "Validation set: Average loss: 0.4920, Accuracy: 4251/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[39.28230047225952 s]\n",
      "Train Epoch: 68 [0/50000 (0%)]\tLoss: 0.207264, Accuracy: 92.77\n",
      "Train Epoch: 68 [2560/50000 (6%)]\tLoss: 0.231959, Accuracy: 92.19\n",
      "Train Epoch: 68 [5120/50000 (11%)]\tLoss: 0.229935, Accuracy: 91.99\n",
      "Train Epoch: 68 [7680/50000 (17%)]\tLoss: 0.217847, Accuracy: 91.80\n",
      "Train Epoch: 68 [10240/50000 (23%)]\tLoss: 0.227163, Accuracy: 93.36\n",
      "Train Epoch: 68 [12800/50000 (28%)]\tLoss: 0.206992, Accuracy: 91.60\n",
      "Train Epoch: 68 [15360/50000 (34%)]\tLoss: 0.202849, Accuracy: 93.55\n",
      "Train Epoch: 68 [17920/50000 (40%)]\tLoss: 0.226021, Accuracy: 91.99\n",
      "Train Epoch: 68 [20480/50000 (45%)]\tLoss: 0.204631, Accuracy: 94.14\n",
      "Train Epoch: 68 [23040/50000 (51%)]\tLoss: 0.270946, Accuracy: 91.21\n",
      "Train Epoch: 68 [25600/50000 (57%)]\tLoss: 0.314352, Accuracy: 89.06\n",
      "Train Epoch: 68 [28160/50000 (62%)]\tLoss: 0.221560, Accuracy: 92.58\n",
      "Train Epoch: 68 [30720/50000 (68%)]\tLoss: 0.290194, Accuracy: 89.65\n",
      "Train Epoch: 68 [33280/50000 (74%)]\tLoss: 0.227580, Accuracy: 92.19\n",
      "Train Epoch: 68 [35840/50000 (80%)]\tLoss: 0.204147, Accuracy: 92.97\n",
      "Train Epoch: 68 [38400/50000 (85%)]\tLoss: 0.256694, Accuracy: 90.82\n",
      "Train Epoch: 68 [40960/50000 (91%)]\tLoss: 0.251423, Accuracy: 92.58\n",
      "Train Epoch: 68 [43520/50000 (97%)]\tLoss: 0.248357, Accuracy: 90.62\n",
      "\n",
      "Validation set: Average loss: 0.6132, Accuracy: 4051/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.23863172531128 s]\n",
      "\n",
      "Test set: Average loss: 0.6422, Accuracy: 8035/10000 (80.35%)\n",
      "\n",
      "Train Epoch: 69 [0/50000 (0%)]\tLoss: 0.187878, Accuracy: 94.53\n",
      "Train Epoch: 69 [2560/50000 (6%)]\tLoss: 0.264664, Accuracy: 91.21\n",
      "Train Epoch: 69 [5120/50000 (11%)]\tLoss: 0.161901, Accuracy: 94.73\n",
      "Train Epoch: 69 [7680/50000 (17%)]\tLoss: 0.171939, Accuracy: 94.53\n",
      "Train Epoch: 69 [10240/50000 (23%)]\tLoss: 0.195572, Accuracy: 92.97\n",
      "Train Epoch: 69 [12800/50000 (28%)]\tLoss: 0.177619, Accuracy: 92.38\n",
      "Train Epoch: 69 [15360/50000 (34%)]\tLoss: 0.198202, Accuracy: 93.16\n",
      "Train Epoch: 69 [17920/50000 (40%)]\tLoss: 0.225319, Accuracy: 91.80\n",
      "Train Epoch: 69 [20480/50000 (45%)]\tLoss: 0.199201, Accuracy: 92.58\n",
      "Train Epoch: 69 [23040/50000 (51%)]\tLoss: 0.216031, Accuracy: 92.58\n",
      "Train Epoch: 69 [25600/50000 (57%)]\tLoss: 0.249914, Accuracy: 90.43\n",
      "Train Epoch: 69 [28160/50000 (62%)]\tLoss: 0.285409, Accuracy: 90.82\n",
      "Train Epoch: 69 [30720/50000 (68%)]\tLoss: 0.212702, Accuracy: 92.77\n",
      "Train Epoch: 69 [33280/50000 (74%)]\tLoss: 0.244611, Accuracy: 90.43\n",
      "Train Epoch: 69 [35840/50000 (80%)]\tLoss: 0.254486, Accuracy: 89.84\n",
      "Train Epoch: 69 [38400/50000 (85%)]\tLoss: 0.252197, Accuracy: 91.41\n",
      "Train Epoch: 69 [40960/50000 (91%)]\tLoss: 0.203577, Accuracy: 92.97\n",
      "Train Epoch: 69 [43520/50000 (97%)]\tLoss: 0.269720, Accuracy: 90.04\n",
      "\n",
      "Validation set: Average loss: 0.4956, Accuracy: 4223/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[39.261682987213135 s]\n",
      "Train Epoch: 70 [0/50000 (0%)]\tLoss: 0.190313, Accuracy: 93.55\n",
      "Train Epoch: 70 [2560/50000 (6%)]\tLoss: 0.180435, Accuracy: 94.34\n",
      "Train Epoch: 70 [5120/50000 (11%)]\tLoss: 0.198168, Accuracy: 92.97\n",
      "Train Epoch: 70 [7680/50000 (17%)]\tLoss: 0.208805, Accuracy: 92.38\n",
      "Train Epoch: 70 [10240/50000 (23%)]\tLoss: 0.235770, Accuracy: 92.38\n",
      "Train Epoch: 70 [12800/50000 (28%)]\tLoss: 0.197317, Accuracy: 94.34\n",
      "Train Epoch: 70 [15360/50000 (34%)]\tLoss: 0.211296, Accuracy: 92.19\n",
      "Train Epoch: 70 [17920/50000 (40%)]\tLoss: 0.212177, Accuracy: 93.36\n",
      "Train Epoch: 70 [20480/50000 (45%)]\tLoss: 0.214553, Accuracy: 92.97\n",
      "Train Epoch: 70 [23040/50000 (51%)]\tLoss: 0.266146, Accuracy: 90.04\n",
      "Train Epoch: 70 [25600/50000 (57%)]\tLoss: 0.179656, Accuracy: 94.14\n",
      "Train Epoch: 70 [28160/50000 (62%)]\tLoss: 0.212982, Accuracy: 90.82\n",
      "Train Epoch: 70 [30720/50000 (68%)]\tLoss: 0.222289, Accuracy: 94.14\n",
      "Train Epoch: 70 [33280/50000 (74%)]\tLoss: 0.177795, Accuracy: 94.34\n",
      "Train Epoch: 70 [35840/50000 (80%)]\tLoss: 0.222034, Accuracy: 91.99\n",
      "Train Epoch: 70 [38400/50000 (85%)]\tLoss: 0.225309, Accuracy: 91.80\n",
      "Train Epoch: 70 [40960/50000 (91%)]\tLoss: 0.206445, Accuracy: 93.16\n",
      "Train Epoch: 70 [43520/50000 (97%)]\tLoss: 0.217772, Accuracy: 92.38\n",
      "\n",
      "Validation set: Average loss: 0.5447, Accuracy: 4174/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.19408321380615 s]\n",
      "\n",
      "Test set: Average loss: 0.5904, Accuracy: 8227/10000 (82.27%)\n",
      "\n",
      "Train Epoch: 71 [0/50000 (0%)]\tLoss: 0.166878, Accuracy: 94.92\n",
      "Train Epoch: 71 [2560/50000 (6%)]\tLoss: 0.177066, Accuracy: 93.95\n",
      "Train Epoch: 71 [5120/50000 (11%)]\tLoss: 0.203983, Accuracy: 93.16\n",
      "Train Epoch: 71 [7680/50000 (17%)]\tLoss: 0.249873, Accuracy: 92.58\n",
      "Train Epoch: 71 [10240/50000 (23%)]\tLoss: 0.217868, Accuracy: 92.58\n",
      "Train Epoch: 71 [12800/50000 (28%)]\tLoss: 0.237799, Accuracy: 92.19\n",
      "Train Epoch: 71 [15360/50000 (34%)]\tLoss: 0.191396, Accuracy: 93.75\n",
      "Train Epoch: 71 [17920/50000 (40%)]\tLoss: 0.264041, Accuracy: 91.21\n",
      "Train Epoch: 71 [20480/50000 (45%)]\tLoss: 0.223945, Accuracy: 92.38\n",
      "Train Epoch: 71 [23040/50000 (51%)]\tLoss: 0.243419, Accuracy: 91.41\n",
      "Train Epoch: 71 [25600/50000 (57%)]\tLoss: 0.259780, Accuracy: 90.62\n",
      "Train Epoch: 71 [28160/50000 (62%)]\tLoss: 0.231715, Accuracy: 92.38\n",
      "Train Epoch: 71 [30720/50000 (68%)]\tLoss: 0.287216, Accuracy: 90.23\n",
      "Train Epoch: 71 [33280/50000 (74%)]\tLoss: 0.247400, Accuracy: 91.21\n",
      "Train Epoch: 71 [35840/50000 (80%)]\tLoss: 0.242930, Accuracy: 90.62\n",
      "Train Epoch: 71 [38400/50000 (85%)]\tLoss: 0.272253, Accuracy: 91.02\n",
      "Train Epoch: 71 [40960/50000 (91%)]\tLoss: 0.250590, Accuracy: 91.60\n",
      "Train Epoch: 71 [43520/50000 (97%)]\tLoss: 0.227822, Accuracy: 92.77\n",
      "\n",
      "Validation set: Average loss: 0.5637, Accuracy: 4183/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.269142389297485 s]\n",
      "Train Epoch: 72 [0/50000 (0%)]\tLoss: 0.256259, Accuracy: 90.82\n",
      "Train Epoch: 72 [2560/50000 (6%)]\tLoss: 0.173030, Accuracy: 94.53\n",
      "Train Epoch: 72 [5120/50000 (11%)]\tLoss: 0.202329, Accuracy: 93.16\n",
      "Train Epoch: 72 [7680/50000 (17%)]\tLoss: 0.176496, Accuracy: 94.92\n",
      "Train Epoch: 72 [10240/50000 (23%)]\tLoss: 0.219306, Accuracy: 92.58\n",
      "Train Epoch: 72 [12800/50000 (28%)]\tLoss: 0.182399, Accuracy: 94.14\n",
      "Train Epoch: 72 [15360/50000 (34%)]\tLoss: 0.192167, Accuracy: 92.58\n",
      "Train Epoch: 72 [17920/50000 (40%)]\tLoss: 0.182993, Accuracy: 94.14\n",
      "Train Epoch: 72 [20480/50000 (45%)]\tLoss: 0.219930, Accuracy: 93.16\n",
      "Train Epoch: 72 [23040/50000 (51%)]\tLoss: 0.278592, Accuracy: 89.84\n",
      "Train Epoch: 72 [25600/50000 (57%)]\tLoss: 0.212448, Accuracy: 91.60\n",
      "Train Epoch: 72 [28160/50000 (62%)]\tLoss: 0.210973, Accuracy: 93.55\n",
      "Train Epoch: 72 [30720/50000 (68%)]\tLoss: 0.217227, Accuracy: 91.60\n",
      "Train Epoch: 72 [33280/50000 (74%)]\tLoss: 0.324312, Accuracy: 89.45\n",
      "Train Epoch: 72 [35840/50000 (80%)]\tLoss: 0.238435, Accuracy: 92.19\n",
      "Train Epoch: 72 [38400/50000 (85%)]\tLoss: 0.227448, Accuracy: 93.36\n",
      "Train Epoch: 72 [40960/50000 (91%)]\tLoss: 0.252654, Accuracy: 90.82\n",
      "Train Epoch: 72 [43520/50000 (97%)]\tLoss: 0.254983, Accuracy: 91.02\n",
      "\n",
      "Validation set: Average loss: 0.4369, Accuracy: 4303/5000 (86.00%)\n",
      "\n",
      "the time of this epoch:[36.21256637573242 s]\n",
      "\n",
      "Test set: Average loss: 0.4738, Accuracy: 8488/10000 (84.88%)\n",
      "\n",
      "Train Epoch: 73 [0/50000 (0%)]\tLoss: 0.187733, Accuracy: 92.77\n",
      "Train Epoch: 73 [2560/50000 (6%)]\tLoss: 0.254669, Accuracy: 90.82\n",
      "Train Epoch: 73 [5120/50000 (11%)]\tLoss: 0.208488, Accuracy: 93.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73 [7680/50000 (17%)]\tLoss: 0.166492, Accuracy: 94.53\n",
      "Train Epoch: 73 [10240/50000 (23%)]\tLoss: 0.224187, Accuracy: 91.99\n",
      "Train Epoch: 73 [12800/50000 (28%)]\tLoss: 0.193976, Accuracy: 92.97\n",
      "Train Epoch: 73 [15360/50000 (34%)]\tLoss: 0.277723, Accuracy: 91.21\n",
      "Train Epoch: 73 [17920/50000 (40%)]\tLoss: 0.204948, Accuracy: 94.14\n",
      "Train Epoch: 73 [20480/50000 (45%)]\tLoss: 0.227744, Accuracy: 91.41\n",
      "Train Epoch: 73 [23040/50000 (51%)]\tLoss: 0.177464, Accuracy: 93.75\n",
      "Train Epoch: 73 [25600/50000 (57%)]\tLoss: 0.211608, Accuracy: 93.36\n",
      "Train Epoch: 73 [28160/50000 (62%)]\tLoss: 0.258112, Accuracy: 91.60\n",
      "Train Epoch: 73 [30720/50000 (68%)]\tLoss: 0.225857, Accuracy: 91.02\n",
      "Train Epoch: 73 [33280/50000 (74%)]\tLoss: 0.225406, Accuracy: 91.60\n",
      "Train Epoch: 73 [35840/50000 (80%)]\tLoss: 0.205068, Accuracy: 92.19\n",
      "Train Epoch: 73 [38400/50000 (85%)]\tLoss: 0.235859, Accuracy: 91.41\n",
      "Train Epoch: 73 [40960/50000 (91%)]\tLoss: 0.227533, Accuracy: 92.97\n",
      "Train Epoch: 73 [43520/50000 (97%)]\tLoss: 0.193090, Accuracy: 92.97\n",
      "\n",
      "Validation set: Average loss: 0.6005, Accuracy: 4126/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.295069456100464 s]\n",
      "Train Epoch: 74 [0/50000 (0%)]\tLoss: 0.167383, Accuracy: 93.55\n",
      "Train Epoch: 74 [2560/50000 (6%)]\tLoss: 0.183146, Accuracy: 93.75\n",
      "Train Epoch: 74 [5120/50000 (11%)]\tLoss: 0.189287, Accuracy: 93.16\n",
      "Train Epoch: 74 [7680/50000 (17%)]\tLoss: 0.194218, Accuracy: 93.16\n",
      "Train Epoch: 74 [10240/50000 (23%)]\tLoss: 0.180423, Accuracy: 93.95\n",
      "Train Epoch: 74 [12800/50000 (28%)]\tLoss: 0.198353, Accuracy: 92.77\n",
      "Train Epoch: 74 [15360/50000 (34%)]\tLoss: 0.249075, Accuracy: 91.02\n",
      "Train Epoch: 74 [17920/50000 (40%)]\tLoss: 0.274861, Accuracy: 90.82\n",
      "Train Epoch: 74 [20480/50000 (45%)]\tLoss: 0.240570, Accuracy: 91.60\n",
      "Train Epoch: 74 [23040/50000 (51%)]\tLoss: 0.240217, Accuracy: 90.82\n",
      "Train Epoch: 74 [25600/50000 (57%)]\tLoss: 0.208090, Accuracy: 92.38\n",
      "Train Epoch: 74 [28160/50000 (62%)]\tLoss: 0.244141, Accuracy: 92.38\n",
      "Train Epoch: 74 [30720/50000 (68%)]\tLoss: 0.225385, Accuracy: 92.77\n",
      "Train Epoch: 74 [33280/50000 (74%)]\tLoss: 0.179308, Accuracy: 93.95\n",
      "Train Epoch: 74 [35840/50000 (80%)]\tLoss: 0.233203, Accuracy: 91.99\n",
      "Train Epoch: 74 [38400/50000 (85%)]\tLoss: 0.216259, Accuracy: 92.77\n",
      "Train Epoch: 74 [40960/50000 (91%)]\tLoss: 0.224199, Accuracy: 92.19\n",
      "Train Epoch: 74 [43520/50000 (97%)]\tLoss: 0.263278, Accuracy: 91.60\n",
      "\n",
      "Validation set: Average loss: 0.5660, Accuracy: 4126/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[36.24414777755737 s]\n",
      "\n",
      "Test set: Average loss: 0.6207, Accuracy: 8139/10000 (81.39%)\n",
      "\n",
      "Train Epoch: 75 [0/50000 (0%)]\tLoss: 0.208392, Accuracy: 93.36\n",
      "Train Epoch: 75 [2560/50000 (6%)]\tLoss: 0.191589, Accuracy: 93.55\n",
      "Train Epoch: 75 [5120/50000 (11%)]\tLoss: 0.196187, Accuracy: 92.97\n",
      "Train Epoch: 75 [7680/50000 (17%)]\tLoss: 0.187329, Accuracy: 92.38\n",
      "Train Epoch: 75 [10240/50000 (23%)]\tLoss: 0.199574, Accuracy: 92.77\n",
      "Train Epoch: 75 [12800/50000 (28%)]\tLoss: 0.205284, Accuracy: 91.60\n",
      "Train Epoch: 75 [15360/50000 (34%)]\tLoss: 0.143768, Accuracy: 95.12\n",
      "Train Epoch: 75 [17920/50000 (40%)]\tLoss: 0.217781, Accuracy: 92.38\n",
      "Train Epoch: 75 [20480/50000 (45%)]\tLoss: 0.270455, Accuracy: 91.21\n",
      "Train Epoch: 75 [23040/50000 (51%)]\tLoss: 0.223367, Accuracy: 91.02\n",
      "Train Epoch: 75 [25600/50000 (57%)]\tLoss: 0.207367, Accuracy: 93.16\n",
      "Train Epoch: 75 [28160/50000 (62%)]\tLoss: 0.228276, Accuracy: 92.19\n",
      "Train Epoch: 75 [30720/50000 (68%)]\tLoss: 0.214205, Accuracy: 91.60\n",
      "Train Epoch: 75 [33280/50000 (74%)]\tLoss: 0.250923, Accuracy: 91.21\n",
      "Train Epoch: 75 [35840/50000 (80%)]\tLoss: 0.255026, Accuracy: 91.60\n",
      "Train Epoch: 75 [38400/50000 (85%)]\tLoss: 0.235374, Accuracy: 92.38\n",
      "Train Epoch: 75 [40960/50000 (91%)]\tLoss: 0.243450, Accuracy: 91.60\n",
      "Train Epoch: 75 [43520/50000 (97%)]\tLoss: 0.171054, Accuracy: 94.73\n",
      "\n",
      "Validation set: Average loss: 0.5675, Accuracy: 4175/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.25939154624939 s]\n",
      "Train Epoch: 76 [0/50000 (0%)]\tLoss: 0.168087, Accuracy: 94.34\n",
      "Train Epoch: 76 [2560/50000 (6%)]\tLoss: 0.138643, Accuracy: 95.70\n",
      "Train Epoch: 76 [5120/50000 (11%)]\tLoss: 0.199755, Accuracy: 93.36\n",
      "Train Epoch: 76 [7680/50000 (17%)]\tLoss: 0.179641, Accuracy: 93.95\n",
      "Train Epoch: 76 [10240/50000 (23%)]\tLoss: 0.165245, Accuracy: 93.75\n",
      "Train Epoch: 76 [12800/50000 (28%)]\tLoss: 0.200393, Accuracy: 92.97\n",
      "Train Epoch: 76 [15360/50000 (34%)]\tLoss: 0.239162, Accuracy: 92.38\n",
      "Train Epoch: 76 [17920/50000 (40%)]\tLoss: 0.198827, Accuracy: 93.55\n",
      "Train Epoch: 76 [20480/50000 (45%)]\tLoss: 0.211719, Accuracy: 93.16\n",
      "Train Epoch: 76 [23040/50000 (51%)]\tLoss: 0.250139, Accuracy: 91.41\n",
      "Train Epoch: 76 [25600/50000 (57%)]\tLoss: 0.229193, Accuracy: 91.41\n",
      "Train Epoch: 76 [28160/50000 (62%)]\tLoss: 0.209589, Accuracy: 92.58\n",
      "Train Epoch: 76 [30720/50000 (68%)]\tLoss: 0.206955, Accuracy: 93.16\n",
      "Train Epoch: 76 [33280/50000 (74%)]\tLoss: 0.235530, Accuracy: 91.99\n",
      "Train Epoch: 76 [35840/50000 (80%)]\tLoss: 0.259742, Accuracy: 91.21\n",
      "Train Epoch: 76 [38400/50000 (85%)]\tLoss: 0.214859, Accuracy: 92.58\n",
      "Train Epoch: 76 [40960/50000 (91%)]\tLoss: 0.210280, Accuracy: 92.97\n",
      "Train Epoch: 76 [43520/50000 (97%)]\tLoss: 0.231796, Accuracy: 90.43\n",
      "\n",
      "Validation set: Average loss: 0.4943, Accuracy: 4277/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[36.18350076675415 s]\n",
      "\n",
      "Test set: Average loss: 0.5118, Accuracy: 8476/10000 (84.76%)\n",
      "\n",
      "Train Epoch: 77 [0/50000 (0%)]\tLoss: 0.188409, Accuracy: 92.97\n",
      "Train Epoch: 77 [2560/50000 (6%)]\tLoss: 0.239084, Accuracy: 92.58\n",
      "Train Epoch: 77 [5120/50000 (11%)]\tLoss: 0.204990, Accuracy: 93.95\n",
      "Train Epoch: 77 [7680/50000 (17%)]\tLoss: 0.186482, Accuracy: 92.58\n",
      "Train Epoch: 77 [10240/50000 (23%)]\tLoss: 0.189378, Accuracy: 93.95\n",
      "Train Epoch: 77 [12800/50000 (28%)]\tLoss: 0.198362, Accuracy: 93.36\n",
      "Train Epoch: 77 [15360/50000 (34%)]\tLoss: 0.224852, Accuracy: 92.19\n",
      "Train Epoch: 77 [17920/50000 (40%)]\tLoss: 0.206992, Accuracy: 93.16\n",
      "Train Epoch: 77 [20480/50000 (45%)]\tLoss: 0.203031, Accuracy: 93.36\n",
      "Train Epoch: 77 [23040/50000 (51%)]\tLoss: 0.174868, Accuracy: 94.14\n",
      "Train Epoch: 77 [25600/50000 (57%)]\tLoss: 0.242865, Accuracy: 92.19\n",
      "Train Epoch: 77 [28160/50000 (62%)]\tLoss: 0.248485, Accuracy: 92.38\n",
      "Train Epoch: 77 [30720/50000 (68%)]\tLoss: 0.250999, Accuracy: 91.80\n",
      "Train Epoch: 77 [33280/50000 (74%)]\tLoss: 0.226324, Accuracy: 91.80\n",
      "Train Epoch: 77 [35840/50000 (80%)]\tLoss: 0.214856, Accuracy: 92.77\n",
      "Train Epoch: 77 [38400/50000 (85%)]\tLoss: 0.192951, Accuracy: 93.36\n",
      "Train Epoch: 77 [40960/50000 (91%)]\tLoss: 0.184743, Accuracy: 93.16\n",
      "Train Epoch: 77 [43520/50000 (97%)]\tLoss: 0.216308, Accuracy: 93.16\n",
      "\n",
      "Validation set: Average loss: 0.5317, Accuracy: 4190/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.27399253845215 s]\n",
      "Train Epoch: 78 [0/50000 (0%)]\tLoss: 0.200796, Accuracy: 92.97\n",
      "Train Epoch: 78 [2560/50000 (6%)]\tLoss: 0.143981, Accuracy: 95.70\n",
      "Train Epoch: 78 [5120/50000 (11%)]\tLoss: 0.193853, Accuracy: 93.16\n",
      "Train Epoch: 78 [7680/50000 (17%)]\tLoss: 0.200500, Accuracy: 92.19\n",
      "Train Epoch: 78 [10240/50000 (23%)]\tLoss: 0.201253, Accuracy: 92.97\n",
      "Train Epoch: 78 [12800/50000 (28%)]\tLoss: 0.246797, Accuracy: 91.99\n",
      "Train Epoch: 78 [15360/50000 (34%)]\tLoss: 0.263438, Accuracy: 90.62\n",
      "Train Epoch: 78 [17920/50000 (40%)]\tLoss: 0.160666, Accuracy: 95.31\n",
      "Train Epoch: 78 [20480/50000 (45%)]\tLoss: 0.196020, Accuracy: 92.19\n",
      "Train Epoch: 78 [23040/50000 (51%)]\tLoss: 0.183182, Accuracy: 94.14\n",
      "Train Epoch: 78 [25600/50000 (57%)]\tLoss: 0.242654, Accuracy: 91.80\n",
      "Train Epoch: 78 [28160/50000 (62%)]\tLoss: 0.175856, Accuracy: 93.75\n",
      "Train Epoch: 78 [30720/50000 (68%)]\tLoss: 0.269554, Accuracy: 90.82\n",
      "Train Epoch: 78 [33280/50000 (74%)]\tLoss: 0.250924, Accuracy: 91.99\n",
      "Train Epoch: 78 [35840/50000 (80%)]\tLoss: 0.221481, Accuracy: 92.77\n",
      "Train Epoch: 78 [38400/50000 (85%)]\tLoss: 0.230408, Accuracy: 92.19\n",
      "Train Epoch: 78 [40960/50000 (91%)]\tLoss: 0.256429, Accuracy: 90.82\n",
      "Train Epoch: 78 [43520/50000 (97%)]\tLoss: 0.251024, Accuracy: 90.23\n",
      "\n",
      "Validation set: Average loss: 0.5961, Accuracy: 4059/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.138283252716064 s]\n",
      "\n",
      "Test set: Average loss: 0.5997, Accuracy: 8188/10000 (81.88%)\n",
      "\n",
      "Train Epoch: 79 [0/50000 (0%)]\tLoss: 0.182485, Accuracy: 94.34\n",
      "Train Epoch: 79 [2560/50000 (6%)]\tLoss: 0.195386, Accuracy: 92.97\n",
      "Train Epoch: 79 [5120/50000 (11%)]\tLoss: 0.214576, Accuracy: 92.19\n",
      "Train Epoch: 79 [7680/50000 (17%)]\tLoss: 0.180008, Accuracy: 92.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [10240/50000 (23%)]\tLoss: 0.217284, Accuracy: 92.77\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0016]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2770]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0333]],\n",
      "\n",
      "        [[ 0.1957]],\n",
      "\n",
      "        [[ 0.0199]],\n",
      "\n",
      "        [[ 0.0201]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.1145]],\n",
      "\n",
      "        [[ 0.0229]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0943]],\n",
      "\n",
      "        [[ 0.0574]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0487]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.1056]],\n",
      "\n",
      "        [[ 0.0714]],\n",
      "\n",
      "        [[ 0.0022]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0280]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0097]],\n",
      "\n",
      "        [[ 0.1354]],\n",
      "\n",
      "        [[ 0.0446]],\n",
      "\n",
      "        [[ 0.1026]],\n",
      "\n",
      "        [[ 0.1069]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.0755]],\n",
      "\n",
      "        [[ 0.1174]],\n",
      "\n",
      "        [[ 0.0379]],\n",
      "\n",
      "        [[ 0.0396]],\n",
      "\n",
      "        [[ 0.0055]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.1251]],\n",
      "\n",
      "        [[ 0.0398]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0028]],\n",
      "\n",
      "        [[ 0.0994]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0631]],\n",
      "\n",
      "        [[ 0.0513]],\n",
      "\n",
      "        [[ 0.1284]],\n",
      "\n",
      "        [[ 0.2374]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0283]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0542]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0656]],\n",
      "\n",
      "        [[ 0.0762]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0037]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.3209]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0358]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0343]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0052]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0249]],\n",
      "\n",
      "        [[ 0.0274]],\n",
      "\n",
      "        [[ 0.0315]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0026]],\n",
      "\n",
      "        [[ 0.0062]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0304]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0284]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0085]],\n",
      "\n",
      "        [[ 0.0065]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.0274]],\n",
      "\n",
      "        [[ 0.0309]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0067]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0066]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0051]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0217]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0201]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0264]],\n",
      "\n",
      "        [[ 0.0221]],\n",
      "\n",
      "        [[ 0.0294]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0192]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0222]],\n",
      "\n",
      "        [[ 0.0314]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0064]],\n",
      "\n",
      "        [[ 0.0073]],\n",
      "\n",
      "        [[ 0.0029]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0322]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.0379]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0298]],\n",
      "\n",
      "        [[ 0.0094]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0155]],\n",
      "\n",
      "        [[ 0.0077]],\n",
      "\n",
      "        [[ 0.0078]],\n",
      "\n",
      "        [[ 0.0062]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0066]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0020]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0240]],\n",
      "\n",
      "        [[ 0.0093]],\n",
      "\n",
      "        [[ 0.0110]],\n",
      "\n",
      "        [[ 0.0226]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0300]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0122]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0168]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0104]],\n",
      "\n",
      "        [[ 0.0177]],\n",
      "\n",
      "        [[ 0.0161]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0252]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0284]],\n",
      "\n",
      "        [[ 0.0252]],\n",
      "\n",
      "        [[ 0.0293]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0181]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0278]],\n",
      "\n",
      "        [[ 0.0046]],\n",
      "\n",
      "        [[ 0.0101]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0242]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0064]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0071]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0060]],\n",
      "\n",
      "        [[ 0.0047]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0061]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0417]],\n",
      "\n",
      "        [[ 0.0081]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0051]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0078]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0112]],\n",
      "\n",
      "        [[ 0.0090]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0161]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0280]],\n",
      "\n",
      "        [[ 0.0068]],\n",
      "\n",
      "        [[ 0.0141]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0013]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0307]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0207]],\n",
      "\n",
      "        [[ 0.0277]],\n",
      "\n",
      "        [[ 0.0088]],\n",
      "\n",
      "        [[ 0.0282]],\n",
      "\n",
      "        [[ 0.0047]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0078]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0260]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0046]],\n",
      "\n",
      "        [[ 0.0344]],\n",
      "\n",
      "        [[ 0.0107]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0402]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0325]],\n",
      "\n",
      "        [[ 0.0012]],\n",
      "\n",
      "        [[ 0.0080]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0047]],\n",
      "\n",
      "        [[ 0.0051]],\n",
      "\n",
      "        [[ 0.0093]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0203]],\n",
      "\n",
      "        [[ 0.0118]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0065]],\n",
      "\n",
      "        [[ 0.0092]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0203]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0051]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0046]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0074]],\n",
      "\n",
      "        [[ 0.0055]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0078]],\n",
      "\n",
      "        [[ 0.0262]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0116]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0061]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0354]],\n",
      "\n",
      "        [[ 0.0235]],\n",
      "\n",
      "        [[ 0.0060]],\n",
      "\n",
      "        [[ 0.0162]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0430]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.0061]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0066]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0048]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0346]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0066]],\n",
      "\n",
      "        [[ 0.0066]],\n",
      "\n",
      "        [[ 0.0012]],\n",
      "\n",
      "        [[ 0.0206]],\n",
      "\n",
      "        [[ 0.0319]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0380]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0013]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0068]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0269]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0029]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0029]],\n",
      "\n",
      "        [[ 0.0068]],\n",
      "\n",
      "        [[ 0.0220]],\n",
      "\n",
      "        [[ 0.0219]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0141]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0104]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0064]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0094]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0089]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0305]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0077]],\n",
      "\n",
      "        [[ 0.0047]],\n",
      "\n",
      "        [[ 0.0058]],\n",
      "\n",
      "        [[ 0.0264]],\n",
      "\n",
      "        [[ 0.0165]],\n",
      "\n",
      "        [[ 0.0298]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0028]],\n",
      "\n",
      "        [[ 0.0126]],\n",
      "\n",
      "        [[ 0.0055]],\n",
      "\n",
      "        [[ 0.0046]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0558]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0086]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0365]],\n",
      "\n",
      "        [[ 0.0128]],\n",
      "\n",
      "        [[ 0.0046]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0047]],\n",
      "\n",
      "        [[ 0.0144]],\n",
      "\n",
      "        [[ 0.0333]],\n",
      "\n",
      "        [[ 0.0121]],\n",
      "\n",
      "        [[ 0.0037]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0012]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0097]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0276]],\n",
      "\n",
      "        [[ 0.0090]],\n",
      "\n",
      "        [[ 0.0203]],\n",
      "\n",
      "        [[ 0.0074]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0140]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0125]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0210]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0085]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0284]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0101]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0066]],\n",
      "\n",
      "        [[ 0.0093]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0279]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0352]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0023]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0065]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0051]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0162]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0101]],\n",
      "\n",
      "        [[ 0.0037]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0053]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0081]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0157]],\n",
      "\n",
      "        [[ 0.0116]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.0020]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0023]],\n",
      "\n",
      "        [[ 0.0073]],\n",
      "\n",
      "        [[ 0.0275]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0110]],\n",
      "\n",
      "        [[ 0.0047]],\n",
      "\n",
      "        [[ 0.0249]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0035]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0181]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0088]],\n",
      "\n",
      "        [[ 0.0126]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0081]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0071]],\n",
      "\n",
      "        [[ 0.0085]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0289]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-04 *\n",
      "       [[[ 7.1695]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 0.2910,  0.1991,  0.8036,  ...,  0.8266, -0.7287,  0.3492]],\n",
      "\n",
      "        [[ 0.6779, -0.4677, -0.4469,  ..., -0.1527, -0.7988,  0.3945]],\n",
      "\n",
      "        [[ 0.0782,  0.8216, -0.4662,  ...,  0.6956,  0.0570,  0.8099]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0262, -0.4374,  0.1578,  ..., -0.1302, -0.9193,  0.7161]],\n",
      "\n",
      "        [[ 0.6389,  0.5057,  0.7029,  ...,  0.2233,  0.6611,  0.4627]],\n",
      "\n",
      "        [[-0.7393,  0.5841,  0.5754,  ...,  0.1717, -0.1481,  0.9912]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor(1.00000e-03 *\n",
      "       [[[ 0.2348,  0.5171,  0.7384,  ...,  0.9781,  0.7503,  0.8496],\n",
      "         [-0.0893, -0.1317, -0.2549,  ..., -0.0855, -0.0434,  0.0675],\n",
      "         [ 0.4219,  0.2369,  0.1081,  ...,  0.1173, -0.0745,  0.1620],\n",
      "         ...,\n",
      "         [-0.2026, -0.1192, -0.0758,  ..., -0.1475, -0.1116,  0.0273],\n",
      "         [ 0.6333,  0.5944,  0.7881,  ...,  0.7938,  0.7323,  0.7407],\n",
      "         [ 0.3987,  0.4527,  0.6114,  ...,  0.3361,  0.2888,  0.0120]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [12800/50000 (28%)]\tLoss: 0.249785, Accuracy: 92.77\n",
      "Train Epoch: 79 [15360/50000 (34%)]\tLoss: 0.233914, Accuracy: 92.58\n",
      "Train Epoch: 79 [17920/50000 (40%)]\tLoss: 0.217989, Accuracy: 91.60\n",
      "Train Epoch: 79 [20480/50000 (45%)]\tLoss: 0.266111, Accuracy: 91.80\n",
      "Train Epoch: 79 [23040/50000 (51%)]\tLoss: 0.198632, Accuracy: 93.55\n",
      "Train Epoch: 79 [25600/50000 (57%)]\tLoss: 0.311374, Accuracy: 89.06\n",
      "Train Epoch: 79 [28160/50000 (62%)]\tLoss: 0.257038, Accuracy: 91.21\n",
      "Train Epoch: 79 [30720/50000 (68%)]\tLoss: 0.252791, Accuracy: 91.21\n",
      "Train Epoch: 79 [33280/50000 (74%)]\tLoss: 0.217173, Accuracy: 92.38\n",
      "Train Epoch: 79 [35840/50000 (80%)]\tLoss: 0.258299, Accuracy: 90.23\n",
      "Train Epoch: 79 [38400/50000 (85%)]\tLoss: 0.243910, Accuracy: 90.23\n",
      "Train Epoch: 79 [40960/50000 (91%)]\tLoss: 0.270120, Accuracy: 91.21\n",
      "Train Epoch: 79 [43520/50000 (97%)]\tLoss: 0.285534, Accuracy: 88.87\n",
      "\n",
      "Validation set: Average loss: 0.5265, Accuracy: 4151/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.409602880477905 s]\n",
      "Train Epoch: 80 [0/50000 (0%)]\tLoss: 0.182258, Accuracy: 94.53\n",
      "Train Epoch: 80 [2560/50000 (6%)]\tLoss: 0.164406, Accuracy: 94.53\n",
      "Train Epoch: 80 [5120/50000 (11%)]\tLoss: 0.173823, Accuracy: 93.55\n",
      "Train Epoch: 80 [7680/50000 (17%)]\tLoss: 0.183252, Accuracy: 93.55\n",
      "Train Epoch: 80 [10240/50000 (23%)]\tLoss: 0.225189, Accuracy: 91.60\n",
      "Train Epoch: 80 [12800/50000 (28%)]\tLoss: 0.210000, Accuracy: 92.77\n",
      "Train Epoch: 80 [15360/50000 (34%)]\tLoss: 0.199870, Accuracy: 92.19\n",
      "Train Epoch: 80 [17920/50000 (40%)]\tLoss: 0.149010, Accuracy: 94.34\n",
      "Train Epoch: 80 [20480/50000 (45%)]\tLoss: 0.210972, Accuracy: 92.19\n",
      "Train Epoch: 80 [23040/50000 (51%)]\tLoss: 0.213219, Accuracy: 92.77\n",
      "Train Epoch: 80 [25600/50000 (57%)]\tLoss: 0.236720, Accuracy: 92.77\n",
      "Train Epoch: 80 [28160/50000 (62%)]\tLoss: 0.189385, Accuracy: 93.55\n",
      "Train Epoch: 80 [30720/50000 (68%)]\tLoss: 0.191014, Accuracy: 93.75\n",
      "Train Epoch: 80 [33280/50000 (74%)]\tLoss: 0.256269, Accuracy: 92.58\n",
      "Train Epoch: 80 [35840/50000 (80%)]\tLoss: 0.208283, Accuracy: 93.16\n",
      "Train Epoch: 80 [38400/50000 (85%)]\tLoss: 0.202550, Accuracy: 92.97\n",
      "Train Epoch: 80 [40960/50000 (91%)]\tLoss: 0.286572, Accuracy: 90.04\n",
      "Train Epoch: 80 [43520/50000 (97%)]\tLoss: 0.255659, Accuracy: 90.62\n",
      "\n",
      "Validation set: Average loss: 0.5736, Accuracy: 4144/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[36.22138261795044 s]\n",
      "\n",
      "Test set: Average loss: 0.6215, Accuracy: 8221/10000 (82.21%)\n",
      "\n",
      "Train Epoch: 81 [0/50000 (0%)]\tLoss: 0.141010, Accuracy: 94.53\n",
      "Train Epoch: 81 [2560/50000 (6%)]\tLoss: 0.227311, Accuracy: 94.34\n",
      "Train Epoch: 81 [5120/50000 (11%)]\tLoss: 0.186915, Accuracy: 93.75\n",
      "Train Epoch: 81 [7680/50000 (17%)]\tLoss: 0.194025, Accuracy: 93.95\n",
      "Train Epoch: 81 [10240/50000 (23%)]\tLoss: 0.173705, Accuracy: 94.14\n",
      "Train Epoch: 81 [12800/50000 (28%)]\tLoss: 0.217741, Accuracy: 92.58\n",
      "Train Epoch: 81 [15360/50000 (34%)]\tLoss: 0.215235, Accuracy: 92.77\n",
      "Train Epoch: 81 [17920/50000 (40%)]\tLoss: 0.221532, Accuracy: 92.38\n",
      "Train Epoch: 81 [20480/50000 (45%)]\tLoss: 0.172147, Accuracy: 93.36\n",
      "Train Epoch: 81 [23040/50000 (51%)]\tLoss: 0.207343, Accuracy: 91.99\n",
      "Train Epoch: 81 [25600/50000 (57%)]\tLoss: 0.227498, Accuracy: 93.55\n",
      "Train Epoch: 81 [28160/50000 (62%)]\tLoss: 0.190652, Accuracy: 92.97\n",
      "Train Epoch: 81 [30720/50000 (68%)]\tLoss: 0.215829, Accuracy: 92.97\n",
      "Train Epoch: 81 [33280/50000 (74%)]\tLoss: 0.233642, Accuracy: 92.38\n",
      "Train Epoch: 81 [35840/50000 (80%)]\tLoss: 0.244421, Accuracy: 92.97\n",
      "Train Epoch: 81 [38400/50000 (85%)]\tLoss: 0.279729, Accuracy: 91.02\n",
      "Train Epoch: 81 [40960/50000 (91%)]\tLoss: 0.268472, Accuracy: 91.02\n",
      "Train Epoch: 81 [43520/50000 (97%)]\tLoss: 0.224571, Accuracy: 92.58\n",
      "\n",
      "Validation set: Average loss: 0.6358, Accuracy: 4088/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[39.27528429031372 s]\n",
      "Train Epoch: 82 [0/50000 (0%)]\tLoss: 0.170224, Accuracy: 93.95\n",
      "Train Epoch: 82 [2560/50000 (6%)]\tLoss: 0.190135, Accuracy: 92.58\n",
      "Train Epoch: 82 [5120/50000 (11%)]\tLoss: 0.210722, Accuracy: 93.75\n",
      "Train Epoch: 82 [7680/50000 (17%)]\tLoss: 0.286082, Accuracy: 89.06\n",
      "Train Epoch: 82 [10240/50000 (23%)]\tLoss: 0.294635, Accuracy: 90.23\n",
      "Train Epoch: 82 [12800/50000 (28%)]\tLoss: 0.240350, Accuracy: 92.58\n",
      "Train Epoch: 82 [15360/50000 (34%)]\tLoss: 0.255217, Accuracy: 91.99\n",
      "Train Epoch: 82 [17920/50000 (40%)]\tLoss: 0.191138, Accuracy: 93.36\n",
      "Train Epoch: 82 [20480/50000 (45%)]\tLoss: 0.270764, Accuracy: 91.02\n",
      "Train Epoch: 82 [23040/50000 (51%)]\tLoss: 0.251511, Accuracy: 92.38\n",
      "Train Epoch: 82 [25600/50000 (57%)]\tLoss: 0.175128, Accuracy: 92.97\n",
      "Train Epoch: 82 [28160/50000 (62%)]\tLoss: 0.263367, Accuracy: 90.82\n",
      "Train Epoch: 82 [30720/50000 (68%)]\tLoss: 0.212898, Accuracy: 94.14\n",
      "Train Epoch: 82 [33280/50000 (74%)]\tLoss: 0.251226, Accuracy: 91.80\n",
      "Train Epoch: 82 [35840/50000 (80%)]\tLoss: 0.204117, Accuracy: 91.99\n",
      "Train Epoch: 82 [38400/50000 (85%)]\tLoss: 0.231157, Accuracy: 92.58\n",
      "Train Epoch: 82 [40960/50000 (91%)]\tLoss: 0.228213, Accuracy: 92.38\n",
      "Train Epoch: 82 [43520/50000 (97%)]\tLoss: 0.247404, Accuracy: 90.43\n",
      "\n",
      "Validation set: Average loss: 0.8732, Accuracy: 3730/5000 (74.00%)\n",
      "\n",
      "the time of this epoch:[36.200047731399536 s]\n",
      "\n",
      "Test set: Average loss: 0.8498, Accuracy: 7507/10000 (75.07%)\n",
      "\n",
      "Train Epoch: 83 [0/50000 (0%)]\tLoss: 0.214667, Accuracy: 92.38\n",
      "Train Epoch: 83 [2560/50000 (6%)]\tLoss: 0.248229, Accuracy: 91.60\n",
      "Train Epoch: 83 [5120/50000 (11%)]\tLoss: 0.198704, Accuracy: 92.58\n",
      "Train Epoch: 83 [7680/50000 (17%)]\tLoss: 0.220708, Accuracy: 92.19\n",
      "Train Epoch: 83 [10240/50000 (23%)]\tLoss: 0.213748, Accuracy: 93.16\n",
      "Train Epoch: 83 [12800/50000 (28%)]\tLoss: 0.199535, Accuracy: 93.75\n",
      "Train Epoch: 83 [15360/50000 (34%)]\tLoss: 0.187743, Accuracy: 93.55\n",
      "Train Epoch: 83 [17920/50000 (40%)]\tLoss: 0.189665, Accuracy: 94.14\n",
      "Train Epoch: 83 [20480/50000 (45%)]\tLoss: 0.194308, Accuracy: 92.58\n",
      "Train Epoch: 83 [23040/50000 (51%)]\tLoss: 0.166124, Accuracy: 93.95\n",
      "Train Epoch: 83 [25600/50000 (57%)]\tLoss: 0.227470, Accuracy: 92.19\n",
      "Train Epoch: 83 [28160/50000 (62%)]\tLoss: 0.178176, Accuracy: 93.95\n",
      "Train Epoch: 83 [30720/50000 (68%)]\tLoss: 0.218020, Accuracy: 91.99\n",
      "Train Epoch: 83 [33280/50000 (74%)]\tLoss: 0.240748, Accuracy: 91.60\n",
      "Train Epoch: 83 [35840/50000 (80%)]\tLoss: 0.199109, Accuracy: 93.55\n",
      "Train Epoch: 83 [38400/50000 (85%)]\tLoss: 0.212862, Accuracy: 91.99\n",
      "Train Epoch: 83 [40960/50000 (91%)]\tLoss: 0.245268, Accuracy: 91.41\n",
      "Train Epoch: 83 [43520/50000 (97%)]\tLoss: 0.197887, Accuracy: 91.80\n",
      "\n",
      "Validation set: Average loss: 0.8430, Accuracy: 3837/5000 (76.00%)\n",
      "\n",
      "the time of this epoch:[39.27651810646057 s]\n",
      "Train Epoch: 84 [0/50000 (0%)]\tLoss: 0.154616, Accuracy: 94.14\n",
      "Train Epoch: 84 [2560/50000 (6%)]\tLoss: 0.213041, Accuracy: 93.36\n",
      "Train Epoch: 84 [5120/50000 (11%)]\tLoss: 0.199378, Accuracy: 93.95\n",
      "Train Epoch: 84 [7680/50000 (17%)]\tLoss: 0.172633, Accuracy: 93.16\n",
      "Train Epoch: 84 [10240/50000 (23%)]\tLoss: 0.167232, Accuracy: 93.55\n",
      "Train Epoch: 84 [12800/50000 (28%)]\tLoss: 0.202878, Accuracy: 93.95\n",
      "Train Epoch: 84 [15360/50000 (34%)]\tLoss: 0.193600, Accuracy: 94.34\n",
      "Train Epoch: 84 [17920/50000 (40%)]\tLoss: 0.183800, Accuracy: 92.97\n",
      "Train Epoch: 84 [20480/50000 (45%)]\tLoss: 0.148290, Accuracy: 95.12\n",
      "Train Epoch: 84 [23040/50000 (51%)]\tLoss: 0.224524, Accuracy: 91.99\n",
      "Train Epoch: 84 [25600/50000 (57%)]\tLoss: 0.221968, Accuracy: 92.97\n",
      "Train Epoch: 84 [28160/50000 (62%)]\tLoss: 0.201550, Accuracy: 93.95\n",
      "Train Epoch: 84 [30720/50000 (68%)]\tLoss: 0.227074, Accuracy: 92.38\n",
      "Train Epoch: 84 [33280/50000 (74%)]\tLoss: 0.240859, Accuracy: 93.36\n",
      "Train Epoch: 84 [35840/50000 (80%)]\tLoss: 0.257454, Accuracy: 92.58\n",
      "Train Epoch: 84 [38400/50000 (85%)]\tLoss: 0.226655, Accuracy: 92.19\n",
      "Train Epoch: 84 [40960/50000 (91%)]\tLoss: 0.188068, Accuracy: 94.14\n",
      "Train Epoch: 84 [43520/50000 (97%)]\tLoss: 0.208380, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.5621, Accuracy: 4132/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[36.167181968688965 s]\n",
      "\n",
      "Test set: Average loss: 0.5968, Accuracy: 8205/10000 (82.05%)\n",
      "\n",
      "Train Epoch: 85 [0/50000 (0%)]\tLoss: 0.186095, Accuracy: 91.99\n",
      "Train Epoch: 85 [2560/50000 (6%)]\tLoss: 0.144465, Accuracy: 95.31\n",
      "Train Epoch: 85 [5120/50000 (11%)]\tLoss: 0.215561, Accuracy: 91.99\n",
      "Train Epoch: 85 [7680/50000 (17%)]\tLoss: 0.138977, Accuracy: 95.12\n",
      "Train Epoch: 85 [10240/50000 (23%)]\tLoss: 0.202735, Accuracy: 91.60\n",
      "Train Epoch: 85 [12800/50000 (28%)]\tLoss: 0.190494, Accuracy: 93.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 85 [15360/50000 (34%)]\tLoss: 0.172215, Accuracy: 93.95\n",
      "Train Epoch: 85 [17920/50000 (40%)]\tLoss: 0.195348, Accuracy: 92.77\n",
      "Train Epoch: 85 [20480/50000 (45%)]\tLoss: 0.200004, Accuracy: 93.36\n",
      "Train Epoch: 85 [23040/50000 (51%)]\tLoss: 0.243598, Accuracy: 90.82\n",
      "Train Epoch: 85 [25600/50000 (57%)]\tLoss: 0.202441, Accuracy: 92.97\n",
      "Train Epoch: 85 [28160/50000 (62%)]\tLoss: 0.194336, Accuracy: 94.34\n",
      "Train Epoch: 85 [30720/50000 (68%)]\tLoss: 0.251846, Accuracy: 91.60\n",
      "Train Epoch: 85 [33280/50000 (74%)]\tLoss: 0.227852, Accuracy: 91.02\n",
      "Train Epoch: 85 [35840/50000 (80%)]\tLoss: 0.157321, Accuracy: 94.34\n",
      "Train Epoch: 85 [38400/50000 (85%)]\tLoss: 0.224733, Accuracy: 91.80\n",
      "Train Epoch: 85 [40960/50000 (91%)]\tLoss: 0.230965, Accuracy: 92.58\n",
      "Train Epoch: 85 [43520/50000 (97%)]\tLoss: 0.301163, Accuracy: 90.04\n",
      "\n",
      "Validation set: Average loss: 0.5870, Accuracy: 4122/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.25863337516785 s]\n",
      "Train Epoch: 86 [0/50000 (0%)]\tLoss: 0.175744, Accuracy: 94.14\n",
      "Train Epoch: 86 [2560/50000 (6%)]\tLoss: 0.176741, Accuracy: 94.34\n",
      "Train Epoch: 86 [5120/50000 (11%)]\tLoss: 0.210521, Accuracy: 93.36\n",
      "Train Epoch: 86 [7680/50000 (17%)]\tLoss: 0.206142, Accuracy: 92.38\n",
      "Train Epoch: 86 [10240/50000 (23%)]\tLoss: 0.210029, Accuracy: 92.38\n",
      "Train Epoch: 86 [12800/50000 (28%)]\tLoss: 0.183121, Accuracy: 94.34\n",
      "Train Epoch: 86 [15360/50000 (34%)]\tLoss: 0.206463, Accuracy: 92.97\n",
      "Train Epoch: 86 [17920/50000 (40%)]\tLoss: 0.265699, Accuracy: 91.21\n",
      "Train Epoch: 86 [20480/50000 (45%)]\tLoss: 0.201182, Accuracy: 92.38\n",
      "Train Epoch: 86 [23040/50000 (51%)]\tLoss: 0.160617, Accuracy: 93.95\n",
      "Train Epoch: 86 [25600/50000 (57%)]\tLoss: 0.224573, Accuracy: 92.77\n",
      "Train Epoch: 86 [28160/50000 (62%)]\tLoss: 0.256067, Accuracy: 91.41\n",
      "Train Epoch: 86 [30720/50000 (68%)]\tLoss: 0.243737, Accuracy: 91.60\n",
      "Train Epoch: 86 [33280/50000 (74%)]\tLoss: 0.210660, Accuracy: 93.75\n",
      "Train Epoch: 86 [35840/50000 (80%)]\tLoss: 0.217986, Accuracy: 93.16\n",
      "Train Epoch: 86 [38400/50000 (85%)]\tLoss: 0.217456, Accuracy: 92.38\n",
      "Train Epoch: 86 [40960/50000 (91%)]\tLoss: 0.270077, Accuracy: 90.23\n",
      "Train Epoch: 86 [43520/50000 (97%)]\tLoss: 0.241399, Accuracy: 92.77\n",
      "\n",
      "Validation set: Average loss: 0.5570, Accuracy: 4156/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.19663166999817 s]\n",
      "\n",
      "Test set: Average loss: 0.5572, Accuracy: 8326/10000 (83.26%)\n",
      "\n",
      "Train Epoch: 87 [0/50000 (0%)]\tLoss: 0.188190, Accuracy: 93.95\n",
      "Train Epoch: 87 [2560/50000 (6%)]\tLoss: 0.176968, Accuracy: 93.36\n",
      "Train Epoch: 87 [5120/50000 (11%)]\tLoss: 0.172714, Accuracy: 94.53\n",
      "Train Epoch: 87 [7680/50000 (17%)]\tLoss: 0.169727, Accuracy: 94.14\n",
      "Train Epoch: 87 [10240/50000 (23%)]\tLoss: 0.247118, Accuracy: 92.19\n",
      "Train Epoch: 87 [12800/50000 (28%)]\tLoss: 0.183285, Accuracy: 93.55\n",
      "Train Epoch: 87 [15360/50000 (34%)]\tLoss: 0.234393, Accuracy: 91.80\n",
      "Train Epoch: 87 [17920/50000 (40%)]\tLoss: 0.161379, Accuracy: 94.73\n",
      "Train Epoch: 87 [20480/50000 (45%)]\tLoss: 0.163064, Accuracy: 94.92\n",
      "Train Epoch: 87 [23040/50000 (51%)]\tLoss: 0.244051, Accuracy: 91.21\n",
      "Train Epoch: 87 [25600/50000 (57%)]\tLoss: 0.187973, Accuracy: 92.77\n",
      "Train Epoch: 87 [28160/50000 (62%)]\tLoss: 0.214703, Accuracy: 91.99\n",
      "Train Epoch: 87 [30720/50000 (68%)]\tLoss: 0.149869, Accuracy: 94.92\n",
      "Train Epoch: 87 [33280/50000 (74%)]\tLoss: 0.269199, Accuracy: 91.41\n",
      "Train Epoch: 87 [35840/50000 (80%)]\tLoss: 0.243778, Accuracy: 92.58\n",
      "Train Epoch: 87 [38400/50000 (85%)]\tLoss: 0.190365, Accuracy: 93.36\n",
      "Train Epoch: 87 [40960/50000 (91%)]\tLoss: 0.289636, Accuracy: 91.41\n",
      "Train Epoch: 87 [43520/50000 (97%)]\tLoss: 0.197640, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.5158, Accuracy: 4215/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[39.21856451034546 s]\n",
      "Train Epoch: 88 [0/50000 (0%)]\tLoss: 0.152894, Accuracy: 95.12\n",
      "Train Epoch: 88 [2560/50000 (6%)]\tLoss: 0.162071, Accuracy: 94.53\n",
      "Train Epoch: 88 [5120/50000 (11%)]\tLoss: 0.166532, Accuracy: 94.14\n",
      "Train Epoch: 88 [7680/50000 (17%)]\tLoss: 0.145748, Accuracy: 95.51\n",
      "Train Epoch: 88 [10240/50000 (23%)]\tLoss: 0.213767, Accuracy: 92.19\n",
      "Train Epoch: 88 [12800/50000 (28%)]\tLoss: 0.160918, Accuracy: 95.51\n",
      "Train Epoch: 88 [15360/50000 (34%)]\tLoss: 0.253179, Accuracy: 92.77\n",
      "Train Epoch: 88 [17920/50000 (40%)]\tLoss: 0.189852, Accuracy: 92.97\n",
      "Train Epoch: 88 [20480/50000 (45%)]\tLoss: 0.232350, Accuracy: 91.80\n",
      "Train Epoch: 88 [23040/50000 (51%)]\tLoss: 0.200444, Accuracy: 92.58\n",
      "Train Epoch: 88 [25600/50000 (57%)]\tLoss: 0.180349, Accuracy: 93.95\n",
      "Train Epoch: 88 [28160/50000 (62%)]\tLoss: 0.191546, Accuracy: 93.36\n",
      "Train Epoch: 88 [30720/50000 (68%)]\tLoss: 0.182425, Accuracy: 92.97\n",
      "Train Epoch: 88 [33280/50000 (74%)]\tLoss: 0.264064, Accuracy: 91.02\n",
      "Train Epoch: 88 [35840/50000 (80%)]\tLoss: 0.249490, Accuracy: 90.82\n",
      "Train Epoch: 88 [38400/50000 (85%)]\tLoss: 0.201325, Accuracy: 92.38\n",
      "Train Epoch: 88 [40960/50000 (91%)]\tLoss: 0.187762, Accuracy: 94.14\n",
      "Train Epoch: 88 [43520/50000 (97%)]\tLoss: 0.274476, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.7104, Accuracy: 3957/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[36.20153737068176 s]\n",
      "\n",
      "Test set: Average loss: 0.7552, Accuracy: 7868/10000 (78.68%)\n",
      "\n",
      "Train Epoch: 89 [0/50000 (0%)]\tLoss: 0.182331, Accuracy: 93.16\n",
      "Train Epoch: 89 [2560/50000 (6%)]\tLoss: 0.184739, Accuracy: 92.97\n",
      "Train Epoch: 89 [5120/50000 (11%)]\tLoss: 0.165275, Accuracy: 93.36\n",
      "Train Epoch: 89 [7680/50000 (17%)]\tLoss: 0.195555, Accuracy: 93.16\n",
      "Train Epoch: 89 [10240/50000 (23%)]\tLoss: 0.212082, Accuracy: 91.80\n",
      "Train Epoch: 89 [12800/50000 (28%)]\tLoss: 0.200408, Accuracy: 93.16\n",
      "Train Epoch: 89 [15360/50000 (34%)]\tLoss: 0.169281, Accuracy: 94.34\n",
      "Train Epoch: 89 [17920/50000 (40%)]\tLoss: 0.279121, Accuracy: 91.80\n",
      "Train Epoch: 89 [20480/50000 (45%)]\tLoss: 0.181036, Accuracy: 93.36\n",
      "Train Epoch: 89 [23040/50000 (51%)]\tLoss: 0.239809, Accuracy: 91.80\n",
      "Train Epoch: 89 [25600/50000 (57%)]\tLoss: 0.211876, Accuracy: 93.16\n",
      "Train Epoch: 89 [28160/50000 (62%)]\tLoss: 0.230415, Accuracy: 91.99\n",
      "Train Epoch: 89 [30720/50000 (68%)]\tLoss: 0.209756, Accuracy: 93.16\n",
      "Train Epoch: 89 [33280/50000 (74%)]\tLoss: 0.242361, Accuracy: 91.21\n",
      "Train Epoch: 89 [35840/50000 (80%)]\tLoss: 0.200867, Accuracy: 92.58\n",
      "Train Epoch: 89 [38400/50000 (85%)]\tLoss: 0.282135, Accuracy: 91.41\n",
      "Train Epoch: 89 [40960/50000 (91%)]\tLoss: 0.194450, Accuracy: 93.95\n",
      "Train Epoch: 89 [43520/50000 (97%)]\tLoss: 0.236258, Accuracy: 91.21\n",
      "\n",
      "Validation set: Average loss: 0.6080, Accuracy: 4116/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.24862861633301 s]\n",
      "Train Epoch: 90 [0/50000 (0%)]\tLoss: 0.173905, Accuracy: 94.53\n",
      "Train Epoch: 90 [2560/50000 (6%)]\tLoss: 0.287370, Accuracy: 88.28\n",
      "Train Epoch: 90 [5120/50000 (11%)]\tLoss: 0.179869, Accuracy: 93.95\n",
      "Train Epoch: 90 [7680/50000 (17%)]\tLoss: 0.150916, Accuracy: 95.70\n",
      "Train Epoch: 90 [10240/50000 (23%)]\tLoss: 0.186545, Accuracy: 94.53\n",
      "Train Epoch: 90 [12800/50000 (28%)]\tLoss: 0.191662, Accuracy: 92.77\n",
      "Train Epoch: 90 [15360/50000 (34%)]\tLoss: 0.226487, Accuracy: 91.60\n",
      "Train Epoch: 90 [17920/50000 (40%)]\tLoss: 0.169029, Accuracy: 93.36\n",
      "Train Epoch: 90 [20480/50000 (45%)]\tLoss: 0.214066, Accuracy: 92.19\n",
      "Train Epoch: 90 [23040/50000 (51%)]\tLoss: 0.180007, Accuracy: 93.36\n",
      "Train Epoch: 90 [25600/50000 (57%)]\tLoss: 0.196110, Accuracy: 93.55\n",
      "Train Epoch: 90 [28160/50000 (62%)]\tLoss: 0.179992, Accuracy: 93.95\n",
      "Train Epoch: 90 [30720/50000 (68%)]\tLoss: 0.217247, Accuracy: 92.77\n",
      "Train Epoch: 90 [33280/50000 (74%)]\tLoss: 0.188720, Accuracy: 93.16\n",
      "Train Epoch: 90 [35840/50000 (80%)]\tLoss: 0.225220, Accuracy: 92.58\n",
      "Train Epoch: 90 [38400/50000 (85%)]\tLoss: 0.220495, Accuracy: 92.77\n",
      "Train Epoch: 90 [40960/50000 (91%)]\tLoss: 0.263017, Accuracy: 91.80\n",
      "Train Epoch: 90 [43520/50000 (97%)]\tLoss: 0.211441, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.7375, Accuracy: 3965/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[36.22264742851257 s]\n",
      "\n",
      "Test set: Average loss: 0.7794, Accuracy: 7881/10000 (78.81%)\n",
      "\n",
      "Train Epoch: 91 [0/50000 (0%)]\tLoss: 0.159569, Accuracy: 94.73\n",
      "Train Epoch: 91 [2560/50000 (6%)]\tLoss: 0.174697, Accuracy: 93.55\n",
      "Train Epoch: 91 [5120/50000 (11%)]\tLoss: 0.170580, Accuracy: 93.95\n",
      "Train Epoch: 91 [7680/50000 (17%)]\tLoss: 0.218028, Accuracy: 93.55\n",
      "Train Epoch: 91 [10240/50000 (23%)]\tLoss: 0.176201, Accuracy: 92.77\n",
      "Train Epoch: 91 [12800/50000 (28%)]\tLoss: 0.222510, Accuracy: 91.80\n",
      "Train Epoch: 91 [15360/50000 (34%)]\tLoss: 0.214927, Accuracy: 92.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 91 [17920/50000 (40%)]\tLoss: 0.192172, Accuracy: 92.97\n",
      "Train Epoch: 91 [20480/50000 (45%)]\tLoss: 0.216427, Accuracy: 91.80\n",
      "Train Epoch: 91 [23040/50000 (51%)]\tLoss: 0.222076, Accuracy: 92.58\n",
      "Train Epoch: 91 [25600/50000 (57%)]\tLoss: 0.164688, Accuracy: 93.55\n",
      "Train Epoch: 91 [28160/50000 (62%)]\tLoss: 0.199360, Accuracy: 92.97\n",
      "Train Epoch: 91 [30720/50000 (68%)]\tLoss: 0.218326, Accuracy: 91.60\n",
      "Train Epoch: 91 [33280/50000 (74%)]\tLoss: 0.214825, Accuracy: 93.16\n",
      "Train Epoch: 91 [35840/50000 (80%)]\tLoss: 0.203211, Accuracy: 93.36\n",
      "Train Epoch: 91 [38400/50000 (85%)]\tLoss: 0.182327, Accuracy: 94.14\n",
      "Train Epoch: 91 [40960/50000 (91%)]\tLoss: 0.206675, Accuracy: 93.75\n",
      "Train Epoch: 91 [43520/50000 (97%)]\tLoss: 0.217964, Accuracy: 91.80\n",
      "\n",
      "Validation set: Average loss: 0.5117, Accuracy: 4206/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[39.4147367477417 s]\n",
      "Train Epoch: 92 [0/50000 (0%)]\tLoss: 0.148963, Accuracy: 94.53\n",
      "Train Epoch: 92 [2560/50000 (6%)]\tLoss: 0.143744, Accuracy: 94.73\n",
      "Train Epoch: 92 [5120/50000 (11%)]\tLoss: 0.217324, Accuracy: 90.62\n",
      "Train Epoch: 92 [7680/50000 (17%)]\tLoss: 0.167446, Accuracy: 94.34\n",
      "Train Epoch: 92 [10240/50000 (23%)]\tLoss: 0.152264, Accuracy: 95.12\n",
      "Train Epoch: 92 [12800/50000 (28%)]\tLoss: 0.132333, Accuracy: 96.29\n",
      "Train Epoch: 92 [15360/50000 (34%)]\tLoss: 0.197479, Accuracy: 93.75\n",
      "Train Epoch: 92 [17920/50000 (40%)]\tLoss: 0.182291, Accuracy: 92.97\n",
      "Train Epoch: 92 [20480/50000 (45%)]\tLoss: 0.173833, Accuracy: 94.53\n",
      "Train Epoch: 92 [23040/50000 (51%)]\tLoss: 0.149209, Accuracy: 95.31\n",
      "Train Epoch: 92 [25600/50000 (57%)]\tLoss: 0.222562, Accuracy: 92.58\n",
      "Train Epoch: 92 [28160/50000 (62%)]\tLoss: 0.192204, Accuracy: 92.58\n",
      "Train Epoch: 92 [30720/50000 (68%)]\tLoss: 0.181444, Accuracy: 94.34\n",
      "Train Epoch: 92 [33280/50000 (74%)]\tLoss: 0.259037, Accuracy: 92.97\n",
      "Train Epoch: 92 [35840/50000 (80%)]\tLoss: 0.231974, Accuracy: 92.38\n",
      "Train Epoch: 92 [38400/50000 (85%)]\tLoss: 0.205484, Accuracy: 92.58\n",
      "Train Epoch: 92 [40960/50000 (91%)]\tLoss: 0.252971, Accuracy: 90.82\n",
      "Train Epoch: 92 [43520/50000 (97%)]\tLoss: 0.184380, Accuracy: 94.14\n",
      "\n",
      "Validation set: Average loss: 0.5884, Accuracy: 4127/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[36.22612166404724 s]\n",
      "\n",
      "Test set: Average loss: 0.6087, Accuracy: 8196/10000 (81.96%)\n",
      "\n",
      "Train Epoch: 93 [0/50000 (0%)]\tLoss: 0.198633, Accuracy: 92.58\n",
      "Train Epoch: 93 [2560/50000 (6%)]\tLoss: 0.231189, Accuracy: 91.60\n",
      "Train Epoch: 93 [5120/50000 (11%)]\tLoss: 0.177647, Accuracy: 93.16\n",
      "Train Epoch: 93 [7680/50000 (17%)]\tLoss: 0.240909, Accuracy: 91.80\n",
      "Train Epoch: 93 [10240/50000 (23%)]\tLoss: 0.176554, Accuracy: 93.95\n",
      "Train Epoch: 93 [12800/50000 (28%)]\tLoss: 0.245566, Accuracy: 90.62\n",
      "torch.Size([1024, 1, 1])\n",
      "tensor([[[ 0.0509]],\n",
      "\n",
      "        [[ 0.0237]],\n",
      "\n",
      "        [[ 0.1859]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0077]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-03 *\n",
      "       [[[ 8.9138]]], device='cuda:0')\n",
      "torch.Size([896, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[-2.4369e-03,  1.4661e-03,  7.4028e-03,  ...,  6.3444e-03,\n",
      "           5.3819e-03,  4.5690e-03]],\n",
      "\n",
      "        [[-1.3280e-02, -1.1909e-02, -3.3829e-03,  ..., -4.0138e-03,\n",
      "          -9.3550e-03, -9.3063e-03]],\n",
      "\n",
      "        [[ 5.3413e-03, -6.6245e-04, -4.6889e-03,  ..., -2.9601e-03,\n",
      "           3.3189e-03, -2.4903e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.0316e-03, -1.4092e-02, -1.2374e-02,  ..., -1.1754e-02,\n",
      "          -9.7236e-03, -7.9303e-03]],\n",
      "\n",
      "        [[-1.4375e-02, -8.0388e-03, -8.7257e-03,  ..., -1.0120e-02,\n",
      "          -6.3730e-03, -7.9967e-03]],\n",
      "\n",
      "        [[-1.2290e-03, -9.2526e-04,  2.6452e-04,  ...,  9.9462e-03,\n",
      "           7.1810e-03,  8.7441e-03]]], device='cuda:0')\n",
      "torch.Size([1, 896, 1024])\n",
      "tensor(1.00000e-03 *\n",
      "       [[[-1.2064, -1.5045, -1.8889,  ..., -1.3225, -0.9923, -0.9965],\n",
      "         [ 2.2423,  2.5971,  2.7735,  ...,  3.1158,  2.9070,  2.2865],\n",
      "         [-0.0359, -0.1454,  0.0986,  ..., -0.1020, -0.2560, -0.1694],\n",
      "         ...,\n",
      "         [ 2.4493,  2.7217,  3.3651,  ...,  3.4968,  2.8857,  2.4281],\n",
      "         [ 2.1452,  2.4904,  2.9434,  ...,  3.3650,  2.8387,  2.5949],\n",
      "         [-2.0968, -2.5788, -2.5934,  ..., -1.6550, -1.1374, -0.6668]]], device='cuda:0')\n",
      "Train Epoch: 93 [15360/50000 (34%)]\tLoss: 0.136972, Accuracy: 95.51\n",
      "Train Epoch: 93 [17920/50000 (40%)]\tLoss: 0.215765, Accuracy: 93.16\n",
      "Train Epoch: 93 [20480/50000 (45%)]\tLoss: 0.241995, Accuracy: 91.80\n",
      "Train Epoch: 93 [23040/50000 (51%)]\tLoss: 0.180505, Accuracy: 93.75\n",
      "Train Epoch: 93 [25600/50000 (57%)]\tLoss: 0.209925, Accuracy: 92.19\n",
      "Train Epoch: 93 [28160/50000 (62%)]\tLoss: 0.213188, Accuracy: 92.58\n",
      "Train Epoch: 93 [30720/50000 (68%)]\tLoss: 0.195086, Accuracy: 93.55\n",
      "Train Epoch: 93 [33280/50000 (74%)]\tLoss: 0.206258, Accuracy: 92.77\n",
      "Train Epoch: 93 [35840/50000 (80%)]\tLoss: 0.210150, Accuracy: 92.38\n",
      "Train Epoch: 93 [38400/50000 (85%)]\tLoss: 0.181444, Accuracy: 93.16\n",
      "Train Epoch: 93 [40960/50000 (91%)]\tLoss: 0.192077, Accuracy: 92.38\n",
      "Train Epoch: 93 [43520/50000 (97%)]\tLoss: 0.191488, Accuracy: 93.36\n",
      "\n",
      "Validation set: Average loss: 0.5797, Accuracy: 4106/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.27778506278992 s]\n",
      "Train Epoch: 94 [0/50000 (0%)]\tLoss: 0.190836, Accuracy: 93.55\n",
      "Train Epoch: 94 [2560/50000 (6%)]\tLoss: 0.219201, Accuracy: 92.38\n",
      "Train Epoch: 94 [5120/50000 (11%)]\tLoss: 0.207909, Accuracy: 92.19\n",
      "Train Epoch: 94 [7680/50000 (17%)]\tLoss: 0.164073, Accuracy: 93.95\n",
      "Train Epoch: 94 [10240/50000 (23%)]\tLoss: 0.217893, Accuracy: 90.62\n",
      "Train Epoch: 94 [12800/50000 (28%)]\tLoss: 0.236765, Accuracy: 90.82\n",
      "Train Epoch: 94 [15360/50000 (34%)]\tLoss: 0.149265, Accuracy: 94.14\n",
      "Train Epoch: 94 [17920/50000 (40%)]\tLoss: 0.123587, Accuracy: 95.90\n",
      "Train Epoch: 94 [20480/50000 (45%)]\tLoss: 0.303968, Accuracy: 90.04\n",
      "Train Epoch: 94 [23040/50000 (51%)]\tLoss: 0.171088, Accuracy: 93.55\n",
      "Train Epoch: 94 [25600/50000 (57%)]\tLoss: 0.251493, Accuracy: 92.19\n",
      "Train Epoch: 94 [28160/50000 (62%)]\tLoss: 0.243630, Accuracy: 91.60\n",
      "Train Epoch: 94 [30720/50000 (68%)]\tLoss: 0.235615, Accuracy: 92.19\n",
      "Train Epoch: 94 [33280/50000 (74%)]\tLoss: 0.228720, Accuracy: 92.38\n",
      "Train Epoch: 94 [35840/50000 (80%)]\tLoss: 0.276000, Accuracy: 91.80\n",
      "Train Epoch: 94 [38400/50000 (85%)]\tLoss: 0.270983, Accuracy: 91.41\n",
      "Train Epoch: 94 [40960/50000 (91%)]\tLoss: 0.204301, Accuracy: 92.58\n",
      "Train Epoch: 94 [43520/50000 (97%)]\tLoss: 0.255280, Accuracy: 91.41\n",
      "\n",
      "Validation set: Average loss: 0.6663, Accuracy: 4033/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.19340658187866 s]\n",
      "\n",
      "Test set: Average loss: 0.6798, Accuracy: 8025/10000 (80.25%)\n",
      "\n",
      "Train Epoch: 95 [0/50000 (0%)]\tLoss: 0.198830, Accuracy: 93.55\n",
      "Train Epoch: 95 [2560/50000 (6%)]\tLoss: 0.189920, Accuracy: 94.14\n",
      "Train Epoch: 95 [5120/50000 (11%)]\tLoss: 0.194701, Accuracy: 92.19\n",
      "Train Epoch: 95 [7680/50000 (17%)]\tLoss: 0.180246, Accuracy: 94.14\n",
      "Train Epoch: 95 [10240/50000 (23%)]\tLoss: 0.185331, Accuracy: 94.53\n",
      "Train Epoch: 95 [12800/50000 (28%)]\tLoss: 0.139215, Accuracy: 94.92\n",
      "Train Epoch: 95 [15360/50000 (34%)]\tLoss: 0.233050, Accuracy: 91.41\n",
      "Train Epoch: 95 [17920/50000 (40%)]\tLoss: 0.157696, Accuracy: 94.53\n",
      "Train Epoch: 95 [20480/50000 (45%)]\tLoss: 0.149469, Accuracy: 93.95\n",
      "Train Epoch: 95 [23040/50000 (51%)]\tLoss: 0.150320, Accuracy: 95.31\n",
      "Train Epoch: 95 [25600/50000 (57%)]\tLoss: 0.162370, Accuracy: 95.12\n",
      "torch.Size([512, 1, 1])\n",
      "tensor([[[ 0.0000]],\n",
      "\n",
      "        [[ 0.2217]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0366]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.1098]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0133]],\n",
      "\n",
      "        [[ 0.0487]],\n",
      "\n",
      "        [[ 0.0170]],\n",
      "\n",
      "        [[ 0.0263]],\n",
      "\n",
      "        [[ 0.1167]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0699]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0130]],\n",
      "\n",
      "        [[ 0.0659]],\n",
      "\n",
      "        [[ 0.1349]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0468]],\n",
      "\n",
      "        [[ 0.0570]],\n",
      "\n",
      "        [[ 0.0703]],\n",
      "\n",
      "        [[ 0.0831]],\n",
      "\n",
      "        [[ 0.0331]],\n",
      "\n",
      "        [[ 0.0525]],\n",
      "\n",
      "        [[ 0.0177]],\n",
      "\n",
      "        [[ 0.0540]],\n",
      "\n",
      "        [[ 0.0147]],\n",
      "\n",
      "        [[ 0.0198]],\n",
      "\n",
      "        [[ 0.0741]],\n",
      "\n",
      "        [[ 0.0133]],\n",
      "\n",
      "        [[ 0.0609]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.1199]],\n",
      "\n",
      "        [[ 0.0221]],\n",
      "\n",
      "        [[ 0.0218]],\n",
      "\n",
      "        [[ 0.0507]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0059]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.2099]],\n",
      "\n",
      "        [[ 0.0383]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.1796]],\n",
      "\n",
      "        [[ 0.0174]],\n",
      "\n",
      "        [[ 0.0238]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0540]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0114]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.0134]],\n",
      "\n",
      "        [[ 0.1178]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0525]],\n",
      "\n",
      "        [[ 0.0716]],\n",
      "\n",
      "        [[ 0.1037]],\n",
      "\n",
      "        [[ 0.0169]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0020]],\n",
      "\n",
      "        [[ 0.0154]],\n",
      "\n",
      "        [[ 0.0123]],\n",
      "\n",
      "        [[ 0.0071]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0132]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0077]],\n",
      "\n",
      "        [[ 0.0020]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0101]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0381]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0452]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0584]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0126]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0037]],\n",
      "\n",
      "        [[ 0.0197]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0082]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0184]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0074]],\n",
      "\n",
      "        [[ 0.0274]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0063]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0188]],\n",
      "\n",
      "        [[ 0.0161]],\n",
      "\n",
      "        [[ 0.0258]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0092]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0076]],\n",
      "\n",
      "        [[ 0.0185]],\n",
      "\n",
      "        [[ 0.0068]],\n",
      "\n",
      "        [[ 0.0099]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0156]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0043]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0138]],\n",
      "\n",
      "        [[ 0.0389]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0254]],\n",
      "\n",
      "        [[ 0.0127]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0063]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0142]],\n",
      "\n",
      "        [[ 0.0535]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0141]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0062]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0413]],\n",
      "\n",
      "        [[ 0.0092]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.0173]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0158]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0126]],\n",
      "\n",
      "        [[ 0.0313]],\n",
      "\n",
      "        [[ 0.0112]],\n",
      "\n",
      "        [[ 0.0060]],\n",
      "\n",
      "        [[ 0.0023]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0466]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0039]],\n",
      "\n",
      "        [[ 0.0382]],\n",
      "\n",
      "        [[ 0.0156]],\n",
      "\n",
      "        [[ 0.0316]],\n",
      "\n",
      "        [[ 0.0074]],\n",
      "\n",
      "        [[ 0.0050]],\n",
      "\n",
      "        [[ 0.0100]],\n",
      "\n",
      "        [[ 0.0205]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0079]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0185]],\n",
      "\n",
      "        [[ 0.0045]],\n",
      "\n",
      "        [[ 0.0371]],\n",
      "\n",
      "        [[ 0.0096]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0569]],\n",
      "\n",
      "        [[ 0.0158]],\n",
      "\n",
      "        [[ 0.0097]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0275]],\n",
      "\n",
      "        [[ 0.0026]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0108]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0064]],\n",
      "\n",
      "        [[ 0.0381]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0049]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0138]],\n",
      "\n",
      "        [[ 0.0331]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0029]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0031]],\n",
      "\n",
      "        [[ 0.0095]],\n",
      "\n",
      "        [[ 0.0245]],\n",
      "\n",
      "        [[ 0.0035]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0062]],\n",
      "\n",
      "        [[ 0.0153]],\n",
      "\n",
      "        [[ 0.0141]],\n",
      "\n",
      "        [[ 0.0052]],\n",
      "\n",
      "        [[ 0.0365]],\n",
      "\n",
      "        [[ 0.0086]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0104]],\n",
      "\n",
      "        [[ 0.0030]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0109]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0600]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0281]],\n",
      "\n",
      "        [[ 0.0152]],\n",
      "\n",
      "        [[ 0.0216]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0333]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0052]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0042]],\n",
      "\n",
      "        [[ 0.0884]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0078]],\n",
      "\n",
      "        [[ 0.0090]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0153]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0078]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0081]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0167]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0234]],\n",
      "\n",
      "        [[ 0.0170]],\n",
      "\n",
      "        [[ 0.0104]],\n",
      "\n",
      "        [[ 0.0116]],\n",
      "\n",
      "        [[ 0.0018]],\n",
      "\n",
      "        [[ 0.0046]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.0209]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0051]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0200]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0139]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0181]],\n",
      "\n",
      "        [[ 0.0907]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0146]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0159]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0109]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0160]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0073]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0166]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0029]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0360]],\n",
      "\n",
      "        [[ 0.0209]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0083]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0129]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0129]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0034]],\n",
      "\n",
      "        [[ 0.0154]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0044]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0024]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0071]],\n",
      "\n",
      "        [[ 0.0142]],\n",
      "\n",
      "        [[ 0.0020]],\n",
      "\n",
      "        [[ 0.0180]],\n",
      "\n",
      "        [[ 0.0097]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.0004]],\n",
      "\n",
      "        [[ 0.0223]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0080]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0116]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0017]],\n",
      "\n",
      "        [[ 0.0148]],\n",
      "\n",
      "        [[ 0.0109]],\n",
      "\n",
      "        [[ 0.0193]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0035]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0097]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0372]],\n",
      "\n",
      "        [[ 0.0572]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0106]],\n",
      "\n",
      "        [[ 0.0115]],\n",
      "\n",
      "        [[ 0.0345]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0028]],\n",
      "\n",
      "        [[ 0.0086]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0193]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0070]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0092]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0137]],\n",
      "\n",
      "        [[ 0.0056]],\n",
      "\n",
      "        [[ 0.0013]],\n",
      "\n",
      "        [[ 0.0147]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0439]],\n",
      "\n",
      "        [[ 0.0014]],\n",
      "\n",
      "        [[ 0.0236]],\n",
      "\n",
      "        [[ 0.0208]],\n",
      "\n",
      "        [[ 0.0147]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0113]],\n",
      "\n",
      "        [[ 0.0138]],\n",
      "\n",
      "        [[ 0.0033]],\n",
      "\n",
      "        [[ 0.0186]],\n",
      "\n",
      "        [[ 0.0162]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0005]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0023]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0135]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0054]],\n",
      "\n",
      "        [[ 0.0055]],\n",
      "\n",
      "        [[ 0.0265]],\n",
      "\n",
      "        [[ 0.0011]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0103]],\n",
      "\n",
      "        [[ 0.0281]],\n",
      "\n",
      "        [[ 0.0059]],\n",
      "\n",
      "        [[ 0.0237]],\n",
      "\n",
      "        [[ 0.0126]],\n",
      "\n",
      "        [[ 0.0016]],\n",
      "\n",
      "        [[ 0.0028]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0021]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0041]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0179]],\n",
      "\n",
      "        [[ 0.0061]],\n",
      "\n",
      "        [[ 0.0022]],\n",
      "\n",
      "        [[ 0.0040]],\n",
      "\n",
      "        [[ 0.0002]],\n",
      "\n",
      "        [[ 0.0312]],\n",
      "\n",
      "        [[ 0.0289]],\n",
      "\n",
      "        [[ 0.0146]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0634]],\n",
      "\n",
      "        [[ 0.0008]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0098]],\n",
      "\n",
      "        [[ 0.0199]],\n",
      "\n",
      "        [[ 0.0025]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.0012]],\n",
      "\n",
      "        [[ 0.0015]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0020]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0122]],\n",
      "\n",
      "        [[ 0.0075]],\n",
      "\n",
      "        [[ 0.0026]],\n",
      "\n",
      "        [[ 0.0007]],\n",
      "\n",
      "        [[ 0.0038]],\n",
      "\n",
      "        [[ 0.0003]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0350]],\n",
      "\n",
      "        [[ 0.0173]],\n",
      "\n",
      "        [[ 0.0209]],\n",
      "\n",
      "        [[ 0.0086]],\n",
      "\n",
      "        [[ 0.0036]],\n",
      "\n",
      "        [[ 0.0057]],\n",
      "\n",
      "        [[ 0.0259]],\n",
      "\n",
      "        [[ 0.0280]],\n",
      "\n",
      "        [[ 0.0438]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0342]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0080]],\n",
      "\n",
      "        [[ 0.0161]],\n",
      "\n",
      "        [[ 0.0072]],\n",
      "\n",
      "        [[ 0.0204]],\n",
      "\n",
      "        [[ 0.0000]],\n",
      "\n",
      "        [[ 0.0027]],\n",
      "\n",
      "        [[ 0.0143]],\n",
      "\n",
      "        [[ 0.0190]],\n",
      "\n",
      "        [[ 0.0156]],\n",
      "\n",
      "        [[ 0.0138]],\n",
      "\n",
      "        [[ 0.0006]],\n",
      "\n",
      "        [[ 0.0150]],\n",
      "\n",
      "        [[ 0.0131]],\n",
      "\n",
      "        [[ 0.0183]],\n",
      "\n",
      "        [[ 0.0095]],\n",
      "\n",
      "        [[ 0.0110]],\n",
      "\n",
      "        [[ 0.0019]],\n",
      "\n",
      "        [[ 0.0009]],\n",
      "\n",
      "        [[ 0.0151]],\n",
      "\n",
      "        [[ 0.0012]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0001]],\n",
      "\n",
      "        [[ 0.0032]],\n",
      "\n",
      "        [[ 0.0077]],\n",
      "\n",
      "        [[ 0.0010]],\n",
      "\n",
      "        [[ 0.0103]]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor(1.00000e-03 *\n",
      "       [[[ 5.5975]]], device='cuda:0')\n",
      "torch.Size([448, 1, 9])\n",
      "Parameter containing:\n",
      "tensor([[[ 1.1156e-03, -3.6856e-03,  4.9918e-03,  ..., -6.2782e-04,\n",
      "           5.4486e-05,  2.4141e-03]],\n",
      "\n",
      "        [[ 9.2250e-04, -3.5180e-03,  2.7759e-03,  ...,  2.0648e-03,\n",
      "          -4.5747e-03,  4.3327e-03]],\n",
      "\n",
      "        [[ 3.7817e-04,  4.2697e-03,  3.0806e-03,  ..., -2.4228e-03,\n",
      "          -2.4847e-03, -4.1886e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.4735e-03, -4.6746e-03, -4.7994e-03,  ...,  2.4717e-03,\n",
      "          -4.7249e-04, -3.8343e-03]],\n",
      "\n",
      "        [[ 1.9051e-03, -8.8687e-04,  1.4792e-03,  ...,  3.4368e-03,\n",
      "          -1.4518e-03, -2.0959e-03]],\n",
      "\n",
      "        [[ 5.5371e-03,  5.3766e-04,  3.7367e-03,  ...,  8.2375e-03,\n",
      "           5.2121e-03,  2.3649e-03]]], device='cuda:0')\n",
      "torch.Size([1, 448, 512])\n",
      "tensor(1.00000e-03 *\n",
      "       [[[-0.1851, -0.3026, -0.4330,  ..., -0.3020, -0.3183, -0.3018],\n",
      "         [ 0.1418,  0.2366,  0.1641,  ...,  0.2261,  0.2800,  0.2051],\n",
      "         [ 0.2254,  0.1370,  0.0568,  ..., -0.1409, -0.2042, -0.3223],\n",
      "         ...,\n",
      "         [ 0.0633,  0.1020,  0.2272,  ...,  0.1463,  0.2109,  0.2174],\n",
      "         [ 0.1496,  0.1066,  0.0681,  ..., -0.0513,  0.0385, -0.0848],\n",
      "         [-0.5663, -0.7582, -0.8550,  ..., -0.8167, -0.6014, -0.4375]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [28160/50000 (62%)]\tLoss: 0.226918, Accuracy: 93.16\n",
      "Train Epoch: 95 [30720/50000 (68%)]\tLoss: 0.246927, Accuracy: 91.21\n",
      "Train Epoch: 95 [33280/50000 (74%)]\tLoss: 0.189312, Accuracy: 93.95\n",
      "Train Epoch: 95 [35840/50000 (80%)]\tLoss: 0.171445, Accuracy: 94.34\n",
      "Train Epoch: 95 [38400/50000 (85%)]\tLoss: 0.152694, Accuracy: 94.92\n",
      "Train Epoch: 95 [40960/50000 (91%)]\tLoss: 0.251295, Accuracy: 91.41\n",
      "Train Epoch: 95 [43520/50000 (97%)]\tLoss: 0.259435, Accuracy: 91.41\n",
      "\n",
      "Validation set: Average loss: 0.7438, Accuracy: 3948/5000 (78.00%)\n",
      "\n",
      "the time of this epoch:[39.275625228881836 s]\n",
      "Train Epoch: 96 [0/50000 (0%)]\tLoss: 0.253222, Accuracy: 91.41\n",
      "Train Epoch: 96 [2560/50000 (6%)]\tLoss: 0.179112, Accuracy: 94.14\n",
      "Train Epoch: 96 [5120/50000 (11%)]\tLoss: 0.200927, Accuracy: 93.36\n",
      "Train Epoch: 96 [7680/50000 (17%)]\tLoss: 0.217532, Accuracy: 92.77\n",
      "Train Epoch: 96 [10240/50000 (23%)]\tLoss: 0.209684, Accuracy: 92.38\n",
      "Train Epoch: 96 [12800/50000 (28%)]\tLoss: 0.187473, Accuracy: 93.55\n",
      "Train Epoch: 96 [15360/50000 (34%)]\tLoss: 0.229063, Accuracy: 92.19\n",
      "Train Epoch: 96 [17920/50000 (40%)]\tLoss: 0.189608, Accuracy: 93.75\n",
      "Train Epoch: 96 [20480/50000 (45%)]\tLoss: 0.195885, Accuracy: 93.16\n",
      "Train Epoch: 96 [23040/50000 (51%)]\tLoss: 0.178069, Accuracy: 93.55\n",
      "Train Epoch: 96 [25600/50000 (57%)]\tLoss: 0.196587, Accuracy: 92.97\n",
      "Train Epoch: 96 [28160/50000 (62%)]\tLoss: 0.254862, Accuracy: 91.41\n",
      "Train Epoch: 96 [30720/50000 (68%)]\tLoss: 0.208718, Accuracy: 92.58\n",
      "Train Epoch: 96 [33280/50000 (74%)]\tLoss: 0.297376, Accuracy: 90.62\n",
      "Train Epoch: 96 [35840/50000 (80%)]\tLoss: 0.219604, Accuracy: 93.55\n",
      "Train Epoch: 96 [38400/50000 (85%)]\tLoss: 0.253654, Accuracy: 90.82\n",
      "Train Epoch: 96 [40960/50000 (91%)]\tLoss: 0.208629, Accuracy: 92.77\n",
      "Train Epoch: 96 [43520/50000 (97%)]\tLoss: 0.204244, Accuracy: 93.36\n",
      "\n",
      "Validation set: Average loss: 0.5954, Accuracy: 4114/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[36.38617396354675 s]\n",
      "\n",
      "Test set: Average loss: 0.6216, Accuracy: 8251/10000 (82.51%)\n",
      "\n",
      "Train Epoch: 97 [0/50000 (0%)]\tLoss: 0.151815, Accuracy: 94.34\n",
      "Train Epoch: 97 [2560/50000 (6%)]\tLoss: 0.198500, Accuracy: 93.95\n",
      "Train Epoch: 97 [5120/50000 (11%)]\tLoss: 0.204834, Accuracy: 92.38\n",
      "Train Epoch: 97 [7680/50000 (17%)]\tLoss: 0.230220, Accuracy: 92.77\n",
      "Train Epoch: 97 [10240/50000 (23%)]\tLoss: 0.134328, Accuracy: 95.90\n",
      "Train Epoch: 97 [12800/50000 (28%)]\tLoss: 0.210409, Accuracy: 92.19\n",
      "Train Epoch: 97 [15360/50000 (34%)]\tLoss: 0.204306, Accuracy: 93.36\n",
      "Train Epoch: 97 [17920/50000 (40%)]\tLoss: 0.148221, Accuracy: 95.31\n",
      "Train Epoch: 97 [20480/50000 (45%)]\tLoss: 0.176453, Accuracy: 94.34\n",
      "Train Epoch: 97 [23040/50000 (51%)]\tLoss: 0.153225, Accuracy: 94.92\n",
      "Train Epoch: 97 [25600/50000 (57%)]\tLoss: 0.224924, Accuracy: 92.58\n",
      "Train Epoch: 97 [28160/50000 (62%)]\tLoss: 0.207875, Accuracy: 93.36\n",
      "Train Epoch: 97 [30720/50000 (68%)]\tLoss: 0.187703, Accuracy: 93.16\n",
      "Train Epoch: 97 [33280/50000 (74%)]\tLoss: 0.228407, Accuracy: 91.80\n",
      "Train Epoch: 97 [35840/50000 (80%)]\tLoss: 0.265603, Accuracy: 91.41\n",
      "Train Epoch: 97 [38400/50000 (85%)]\tLoss: 0.220676, Accuracy: 91.60\n",
      "Train Epoch: 97 [40960/50000 (91%)]\tLoss: 0.238269, Accuracy: 91.99\n",
      "Train Epoch: 97 [43520/50000 (97%)]\tLoss: 0.220955, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.5100, Accuracy: 4197/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.60008192062378 s]\n",
      "Train Epoch: 98 [0/50000 (0%)]\tLoss: 0.200968, Accuracy: 93.16\n",
      "Train Epoch: 98 [2560/50000 (6%)]\tLoss: 0.175221, Accuracy: 94.53\n",
      "Train Epoch: 98 [5120/50000 (11%)]\tLoss: 0.183541, Accuracy: 93.95\n",
      "Train Epoch: 98 [7680/50000 (17%)]\tLoss: 0.255177, Accuracy: 90.82\n",
      "Train Epoch: 98 [10240/50000 (23%)]\tLoss: 0.159728, Accuracy: 94.34\n",
      "Train Epoch: 98 [12800/50000 (28%)]\tLoss: 0.201938, Accuracy: 91.80\n",
      "Train Epoch: 98 [15360/50000 (34%)]\tLoss: 0.183108, Accuracy: 94.34\n",
      "Train Epoch: 98 [17920/50000 (40%)]\tLoss: 0.222168, Accuracy: 91.99\n",
      "Train Epoch: 98 [20480/50000 (45%)]\tLoss: 0.163115, Accuracy: 95.12\n",
      "Train Epoch: 98 [23040/50000 (51%)]\tLoss: 0.143922, Accuracy: 95.31\n",
      "Train Epoch: 98 [25600/50000 (57%)]\tLoss: 0.218355, Accuracy: 92.77\n",
      "Train Epoch: 98 [28160/50000 (62%)]\tLoss: 0.183133, Accuracy: 93.95\n",
      "Train Epoch: 98 [30720/50000 (68%)]\tLoss: 0.183274, Accuracy: 93.55\n",
      "Train Epoch: 98 [33280/50000 (74%)]\tLoss: 0.172943, Accuracy: 93.95\n",
      "Train Epoch: 98 [35840/50000 (80%)]\tLoss: 0.156824, Accuracy: 95.31\n",
      "Train Epoch: 98 [38400/50000 (85%)]\tLoss: 0.218570, Accuracy: 91.41\n",
      "Train Epoch: 98 [40960/50000 (91%)]\tLoss: 0.255281, Accuracy: 91.60\n",
      "Train Epoch: 98 [43520/50000 (97%)]\tLoss: 0.185020, Accuracy: 93.75\n",
      "\n",
      "Validation set: Average loss: 0.5498, Accuracy: 4180/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.193523645401 s]\n",
      "\n",
      "Test set: Average loss: 0.5312, Accuracy: 8391/10000 (83.91%)\n",
      "\n",
      "Train Epoch: 99 [0/50000 (0%)]\tLoss: 0.117936, Accuracy: 96.29\n",
      "Train Epoch: 99 [2560/50000 (6%)]\tLoss: 0.126918, Accuracy: 95.31\n",
      "Train Epoch: 99 [5120/50000 (11%)]\tLoss: 0.225038, Accuracy: 92.19\n",
      "Train Epoch: 99 [7680/50000 (17%)]\tLoss: 0.193296, Accuracy: 93.95\n",
      "Train Epoch: 99 [10240/50000 (23%)]\tLoss: 0.204369, Accuracy: 92.58\n",
      "Train Epoch: 99 [12800/50000 (28%)]\tLoss: 0.244805, Accuracy: 91.60\n",
      "Train Epoch: 99 [15360/50000 (34%)]\tLoss: 0.184693, Accuracy: 93.55\n",
      "Train Epoch: 99 [17920/50000 (40%)]\tLoss: 0.194927, Accuracy: 93.36\n",
      "Train Epoch: 99 [20480/50000 (45%)]\tLoss: 0.208104, Accuracy: 91.21\n",
      "Train Epoch: 99 [23040/50000 (51%)]\tLoss: 0.194320, Accuracy: 91.99\n",
      "Train Epoch: 99 [25600/50000 (57%)]\tLoss: 0.140683, Accuracy: 95.90\n",
      "Train Epoch: 99 [28160/50000 (62%)]\tLoss: 0.187136, Accuracy: 94.34\n",
      "Train Epoch: 99 [30720/50000 (68%)]\tLoss: 0.223868, Accuracy: 92.58\n",
      "Train Epoch: 99 [33280/50000 (74%)]\tLoss: 0.170183, Accuracy: 94.34\n",
      "Train Epoch: 99 [35840/50000 (80%)]\tLoss: 0.202791, Accuracy: 93.36\n",
      "Train Epoch: 99 [38400/50000 (85%)]\tLoss: 0.183917, Accuracy: 93.36\n",
      "Train Epoch: 99 [40960/50000 (91%)]\tLoss: 0.224081, Accuracy: 91.21\n",
      "Train Epoch: 99 [43520/50000 (97%)]\tLoss: 0.212350, Accuracy: 92.77\n",
      "\n",
      "Validation set: Average loss: 0.6114, Accuracy: 4105/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.249194622039795 s]\n",
      "Train Epoch: 100 [0/50000 (0%)]\tLoss: 0.146621, Accuracy: 94.53\n",
      "Train Epoch: 100 [2560/50000 (6%)]\tLoss: 0.135763, Accuracy: 95.31\n",
      "Train Epoch: 100 [5120/50000 (11%)]\tLoss: 0.173016, Accuracy: 92.97\n",
      "Train Epoch: 100 [7680/50000 (17%)]\tLoss: 0.188955, Accuracy: 93.95\n",
      "Train Epoch: 100 [10240/50000 (23%)]\tLoss: 0.205069, Accuracy: 93.16\n",
      "Train Epoch: 100 [12800/50000 (28%)]\tLoss: 0.208615, Accuracy: 92.38\n",
      "Train Epoch: 100 [15360/50000 (34%)]\tLoss: 0.172364, Accuracy: 92.38\n",
      "Train Epoch: 100 [17920/50000 (40%)]\tLoss: 0.182603, Accuracy: 93.16\n",
      "Train Epoch: 100 [20480/50000 (45%)]\tLoss: 0.184461, Accuracy: 93.55\n",
      "Train Epoch: 100 [23040/50000 (51%)]\tLoss: 0.191417, Accuracy: 94.14\n",
      "Train Epoch: 100 [25600/50000 (57%)]\tLoss: 0.207361, Accuracy: 92.97\n",
      "Train Epoch: 100 [28160/50000 (62%)]\tLoss: 0.188872, Accuracy: 93.36\n",
      "Train Epoch: 100 [30720/50000 (68%)]\tLoss: 0.191965, Accuracy: 93.16\n",
      "Train Epoch: 100 [33280/50000 (74%)]\tLoss: 0.217291, Accuracy: 92.19\n",
      "Train Epoch: 100 [35840/50000 (80%)]\tLoss: 0.275994, Accuracy: 90.62\n",
      "Train Epoch: 100 [38400/50000 (85%)]\tLoss: 0.224636, Accuracy: 92.19\n",
      "Train Epoch: 100 [40960/50000 (91%)]\tLoss: 0.193806, Accuracy: 94.14\n",
      "Train Epoch: 100 [43520/50000 (97%)]\tLoss: 0.187643, Accuracy: 92.97\n",
      "\n",
      "Validation set: Average loss: 0.6218, Accuracy: 4076/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.18170619010925 s]\n",
      "\n",
      "Test set: Average loss: 0.6399, Accuracy: 8133/10000 (81.33%)\n",
      "\n",
      "Train Epoch: 101 [0/50000 (0%)]\tLoss: 0.188639, Accuracy: 91.99\n",
      "Train Epoch: 101 [2560/50000 (6%)]\tLoss: 0.158685, Accuracy: 94.73\n",
      "Train Epoch: 101 [5120/50000 (11%)]\tLoss: 0.213582, Accuracy: 92.19\n",
      "Train Epoch: 101 [7680/50000 (17%)]\tLoss: 0.216570, Accuracy: 92.77\n",
      "Train Epoch: 101 [10240/50000 (23%)]\tLoss: 0.137821, Accuracy: 95.70\n",
      "Train Epoch: 101 [12800/50000 (28%)]\tLoss: 0.178144, Accuracy: 93.95\n",
      "Train Epoch: 101 [15360/50000 (34%)]\tLoss: 0.161651, Accuracy: 95.12\n",
      "Train Epoch: 101 [17920/50000 (40%)]\tLoss: 0.170650, Accuracy: 94.34\n",
      "Train Epoch: 101 [20480/50000 (45%)]\tLoss: 0.189784, Accuracy: 93.75\n",
      "Train Epoch: 101 [23040/50000 (51%)]\tLoss: 0.215943, Accuracy: 92.38\n",
      "Train Epoch: 101 [25600/50000 (57%)]\tLoss: 0.197256, Accuracy: 93.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 101 [28160/50000 (62%)]\tLoss: 0.227985, Accuracy: 93.16\n",
      "Train Epoch: 101 [30720/50000 (68%)]\tLoss: 0.189837, Accuracy: 93.55\n",
      "Train Epoch: 101 [33280/50000 (74%)]\tLoss: 0.232291, Accuracy: 92.58\n",
      "Train Epoch: 101 [35840/50000 (80%)]\tLoss: 0.177031, Accuracy: 93.95\n",
      "Train Epoch: 101 [38400/50000 (85%)]\tLoss: 0.175951, Accuracy: 95.31\n",
      "Train Epoch: 101 [40960/50000 (91%)]\tLoss: 0.191157, Accuracy: 94.34\n",
      "Train Epoch: 101 [43520/50000 (97%)]\tLoss: 0.242246, Accuracy: 91.21\n",
      "\n",
      "Validation set: Average loss: 0.5231, Accuracy: 4190/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.33974099159241 s]\n",
      "Train Epoch: 102 [0/50000 (0%)]\tLoss: 0.223106, Accuracy: 92.38\n",
      "Train Epoch: 102 [2560/50000 (6%)]\tLoss: 0.172429, Accuracy: 94.14\n",
      "Train Epoch: 102 [5120/50000 (11%)]\tLoss: 0.173463, Accuracy: 94.14\n",
      "Train Epoch: 102 [7680/50000 (17%)]\tLoss: 0.215994, Accuracy: 92.97\n",
      "Train Epoch: 102 [10240/50000 (23%)]\tLoss: 0.220659, Accuracy: 93.36\n",
      "Train Epoch: 102 [12800/50000 (28%)]\tLoss: 0.169950, Accuracy: 94.53\n",
      "Train Epoch: 102 [15360/50000 (34%)]\tLoss: 0.179924, Accuracy: 93.95\n",
      "Train Epoch: 102 [17920/50000 (40%)]\tLoss: 0.193234, Accuracy: 93.75\n",
      "Train Epoch: 102 [20480/50000 (45%)]\tLoss: 0.173634, Accuracy: 93.55\n",
      "Train Epoch: 102 [23040/50000 (51%)]\tLoss: 0.219496, Accuracy: 92.77\n",
      "Train Epoch: 102 [25600/50000 (57%)]\tLoss: 0.164400, Accuracy: 94.14\n",
      "Train Epoch: 102 [28160/50000 (62%)]\tLoss: 0.217052, Accuracy: 92.77\n",
      "Train Epoch: 102 [30720/50000 (68%)]\tLoss: 0.249460, Accuracy: 91.60\n",
      "Train Epoch: 102 [33280/50000 (74%)]\tLoss: 0.215694, Accuracy: 92.97\n",
      "Train Epoch: 102 [35840/50000 (80%)]\tLoss: 0.204072, Accuracy: 93.16\n",
      "Train Epoch: 102 [38400/50000 (85%)]\tLoss: 0.188907, Accuracy: 92.19\n",
      "Train Epoch: 102 [40960/50000 (91%)]\tLoss: 0.169270, Accuracy: 93.95\n",
      "Train Epoch: 102 [43520/50000 (97%)]\tLoss: 0.221230, Accuracy: 93.55\n",
      "\n",
      "Validation set: Average loss: 0.5463, Accuracy: 4184/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.141704082489014 s]\n",
      "\n",
      "Test set: Average loss: 0.5928, Accuracy: 8305/10000 (83.05%)\n",
      "\n",
      "Train Epoch: 103 [0/50000 (0%)]\tLoss: 0.193331, Accuracy: 93.95\n",
      "Train Epoch: 103 [2560/50000 (6%)]\tLoss: 0.234497, Accuracy: 91.60\n",
      "Train Epoch: 103 [5120/50000 (11%)]\tLoss: 0.223729, Accuracy: 92.38\n",
      "Train Epoch: 103 [7680/50000 (17%)]\tLoss: 0.172928, Accuracy: 94.14\n",
      "Train Epoch: 103 [10240/50000 (23%)]\tLoss: 0.171722, Accuracy: 95.51\n",
      "Train Epoch: 103 [12800/50000 (28%)]\tLoss: 0.212101, Accuracy: 92.19\n",
      "Train Epoch: 103 [15360/50000 (34%)]\tLoss: 0.162158, Accuracy: 93.95\n",
      "Train Epoch: 103 [17920/50000 (40%)]\tLoss: 0.166273, Accuracy: 93.75\n",
      "Train Epoch: 103 [20480/50000 (45%)]\tLoss: 0.195665, Accuracy: 92.38\n",
      "Train Epoch: 103 [23040/50000 (51%)]\tLoss: 0.167076, Accuracy: 93.95\n",
      "Train Epoch: 103 [25600/50000 (57%)]\tLoss: 0.135233, Accuracy: 95.31\n",
      "Train Epoch: 103 [28160/50000 (62%)]\tLoss: 0.221390, Accuracy: 93.36\n",
      "Train Epoch: 103 [30720/50000 (68%)]\tLoss: 0.226160, Accuracy: 91.80\n",
      "Train Epoch: 103 [33280/50000 (74%)]\tLoss: 0.219778, Accuracy: 92.38\n",
      "Train Epoch: 103 [35840/50000 (80%)]\tLoss: 0.222290, Accuracy: 91.80\n",
      "Train Epoch: 103 [38400/50000 (85%)]\tLoss: 0.258524, Accuracy: 90.82\n",
      "Train Epoch: 103 [40960/50000 (91%)]\tLoss: 0.216427, Accuracy: 92.38\n",
      "Train Epoch: 103 [43520/50000 (97%)]\tLoss: 0.197941, Accuracy: 92.77\n",
      "\n",
      "Validation set: Average loss: 0.4580, Accuracy: 4284/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[39.31631946563721 s]\n",
      "Train Epoch: 104 [0/50000 (0%)]\tLoss: 0.153318, Accuracy: 94.53\n",
      "Train Epoch: 104 [2560/50000 (6%)]\tLoss: 0.218107, Accuracy: 92.97\n",
      "Train Epoch: 104 [5120/50000 (11%)]\tLoss: 0.178916, Accuracy: 92.97\n",
      "Train Epoch: 104 [7680/50000 (17%)]\tLoss: 0.189829, Accuracy: 93.55\n",
      "Train Epoch: 104 [10240/50000 (23%)]\tLoss: 0.211493, Accuracy: 92.38\n",
      "Train Epoch: 104 [12800/50000 (28%)]\tLoss: 0.215406, Accuracy: 92.38\n",
      "Train Epoch: 104 [15360/50000 (34%)]\tLoss: 0.231649, Accuracy: 92.77\n",
      "Train Epoch: 104 [17920/50000 (40%)]\tLoss: 0.161671, Accuracy: 94.92\n",
      "Train Epoch: 104 [20480/50000 (45%)]\tLoss: 0.241761, Accuracy: 90.04\n",
      "Train Epoch: 104 [23040/50000 (51%)]\tLoss: 0.171234, Accuracy: 94.34\n",
      "Train Epoch: 104 [25600/50000 (57%)]\tLoss: 0.145363, Accuracy: 95.31\n",
      "Train Epoch: 104 [28160/50000 (62%)]\tLoss: 0.176196, Accuracy: 94.92\n",
      "Train Epoch: 104 [30720/50000 (68%)]\tLoss: 0.244519, Accuracy: 91.21\n",
      "Train Epoch: 104 [33280/50000 (74%)]\tLoss: 0.133742, Accuracy: 94.53\n",
      "Train Epoch: 104 [35840/50000 (80%)]\tLoss: 0.269509, Accuracy: 89.84\n",
      "Train Epoch: 104 [38400/50000 (85%)]\tLoss: 0.195945, Accuracy: 92.97\n",
      "Train Epoch: 104 [40960/50000 (91%)]\tLoss: 0.220305, Accuracy: 91.99\n",
      "Train Epoch: 104 [43520/50000 (97%)]\tLoss: 0.206946, Accuracy: 93.55\n",
      "\n",
      "Validation set: Average loss: 0.5033, Accuracy: 4260/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[36.228604316711426 s]\n",
      "\n",
      "Test set: Average loss: 0.5484, Accuracy: 8425/10000 (84.25%)\n",
      "\n",
      "Train Epoch: 105 [0/50000 (0%)]\tLoss: 0.196211, Accuracy: 92.77\n",
      "Train Epoch: 105 [2560/50000 (6%)]\tLoss: 0.152667, Accuracy: 95.12\n",
      "Train Epoch: 105 [5120/50000 (11%)]\tLoss: 0.179346, Accuracy: 94.73\n",
      "Train Epoch: 105 [7680/50000 (17%)]\tLoss: 0.151858, Accuracy: 93.75\n",
      "Train Epoch: 105 [10240/50000 (23%)]\tLoss: 0.187906, Accuracy: 93.36\n",
      "Train Epoch: 105 [12800/50000 (28%)]\tLoss: 0.167889, Accuracy: 93.55\n",
      "Train Epoch: 105 [15360/50000 (34%)]\tLoss: 0.195718, Accuracy: 92.19\n",
      "Train Epoch: 105 [17920/50000 (40%)]\tLoss: 0.213692, Accuracy: 91.99\n",
      "Train Epoch: 105 [20480/50000 (45%)]\tLoss: 0.174679, Accuracy: 93.95\n",
      "Train Epoch: 105 [23040/50000 (51%)]\tLoss: 0.189099, Accuracy: 92.97\n",
      "Train Epoch: 105 [25600/50000 (57%)]\tLoss: 0.185485, Accuracy: 93.36\n",
      "Train Epoch: 105 [28160/50000 (62%)]\tLoss: 0.179049, Accuracy: 94.34\n",
      "Train Epoch: 105 [30720/50000 (68%)]\tLoss: 0.168483, Accuracy: 94.53\n",
      "Train Epoch: 105 [33280/50000 (74%)]\tLoss: 0.177649, Accuracy: 93.95\n",
      "Train Epoch: 105 [35840/50000 (80%)]\tLoss: 0.207807, Accuracy: 92.58\n",
      "Train Epoch: 105 [38400/50000 (85%)]\tLoss: 0.242028, Accuracy: 91.60\n",
      "Train Epoch: 105 [40960/50000 (91%)]\tLoss: 0.158165, Accuracy: 95.31\n",
      "Train Epoch: 105 [43520/50000 (97%)]\tLoss: 0.241074, Accuracy: 92.19\n",
      "\n",
      "Validation set: Average loss: 0.4476, Accuracy: 4325/5000 (86.00%)\n",
      "\n",
      "the time of this epoch:[39.34146523475647 s]\n",
      "Train Epoch: 106 [0/50000 (0%)]\tLoss: 0.227172, Accuracy: 92.58\n",
      "Train Epoch: 106 [2560/50000 (6%)]\tLoss: 0.232743, Accuracy: 92.77\n",
      "Train Epoch: 106 [5120/50000 (11%)]\tLoss: 0.256695, Accuracy: 91.80\n",
      "Train Epoch: 106 [7680/50000 (17%)]\tLoss: 0.168542, Accuracy: 94.34\n",
      "Train Epoch: 106 [10240/50000 (23%)]\tLoss: 0.194464, Accuracy: 93.16\n",
      "Train Epoch: 106 [12800/50000 (28%)]\tLoss: 0.198155, Accuracy: 92.58\n",
      "Train Epoch: 106 [15360/50000 (34%)]\tLoss: 0.186540, Accuracy: 92.77\n",
      "Train Epoch: 106 [17920/50000 (40%)]\tLoss: 0.214936, Accuracy: 93.55\n",
      "Train Epoch: 106 [20480/50000 (45%)]\tLoss: 0.238524, Accuracy: 91.99\n",
      "Train Epoch: 106 [23040/50000 (51%)]\tLoss: 0.207913, Accuracy: 92.19\n",
      "Train Epoch: 106 [25600/50000 (57%)]\tLoss: 0.163230, Accuracy: 94.73\n",
      "Train Epoch: 106 [28160/50000 (62%)]\tLoss: 0.249220, Accuracy: 91.99\n",
      "Train Epoch: 106 [30720/50000 (68%)]\tLoss: 0.196042, Accuracy: 92.97\n",
      "Train Epoch: 106 [33280/50000 (74%)]\tLoss: 0.254673, Accuracy: 91.21\n",
      "Train Epoch: 106 [35840/50000 (80%)]\tLoss: 0.266025, Accuracy: 91.60\n",
      "Train Epoch: 106 [38400/50000 (85%)]\tLoss: 0.228270, Accuracy: 92.58\n",
      "Train Epoch: 106 [40960/50000 (91%)]\tLoss: 0.205135, Accuracy: 92.38\n",
      "Train Epoch: 106 [43520/50000 (97%)]\tLoss: 0.250133, Accuracy: 90.43\n",
      "\n",
      "Validation set: Average loss: 0.7679, Accuracy: 3927/5000 (78.00%)\n",
      "\n",
      "the time of this epoch:[36.219608783721924 s]\n",
      "\n",
      "Test set: Average loss: 0.7766, Accuracy: 7862/10000 (78.62%)\n",
      "\n",
      "Train Epoch: 107 [0/50000 (0%)]\tLoss: 0.200533, Accuracy: 92.58\n",
      "Train Epoch: 107 [2560/50000 (6%)]\tLoss: 0.171289, Accuracy: 95.31\n",
      "Train Epoch: 107 [5120/50000 (11%)]\tLoss: 0.187712, Accuracy: 93.16\n",
      "Train Epoch: 107 [7680/50000 (17%)]\tLoss: 0.236244, Accuracy: 91.02\n",
      "Train Epoch: 107 [10240/50000 (23%)]\tLoss: 0.205219, Accuracy: 92.58\n",
      "Train Epoch: 107 [12800/50000 (28%)]\tLoss: 0.194504, Accuracy: 93.36\n",
      "Train Epoch: 107 [15360/50000 (34%)]\tLoss: 0.172558, Accuracy: 94.92\n",
      "Train Epoch: 107 [17920/50000 (40%)]\tLoss: 0.211676, Accuracy: 93.36\n",
      "Train Epoch: 107 [20480/50000 (45%)]\tLoss: 0.205031, Accuracy: 92.97\n",
      "Train Epoch: 107 [23040/50000 (51%)]\tLoss: 0.229423, Accuracy: 92.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 107 [25600/50000 (57%)]\tLoss: 0.153970, Accuracy: 93.95\n",
      "Train Epoch: 107 [28160/50000 (62%)]\tLoss: 0.213671, Accuracy: 92.58\n",
      "Train Epoch: 107 [30720/50000 (68%)]\tLoss: 0.194403, Accuracy: 93.16\n",
      "Train Epoch: 107 [33280/50000 (74%)]\tLoss: 0.187327, Accuracy: 93.36\n",
      "Train Epoch: 107 [35840/50000 (80%)]\tLoss: 0.201100, Accuracy: 93.36\n",
      "Train Epoch: 107 [38400/50000 (85%)]\tLoss: 0.197784, Accuracy: 92.38\n",
      "Train Epoch: 107 [40960/50000 (91%)]\tLoss: 0.218048, Accuracy: 92.97\n",
      "Train Epoch: 107 [43520/50000 (97%)]\tLoss: 0.190894, Accuracy: 94.34\n",
      "\n",
      "Validation set: Average loss: 0.5656, Accuracy: 4196/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.50050687789917 s]\n",
      "Train Epoch: 108 [0/50000 (0%)]\tLoss: 0.187974, Accuracy: 92.77\n",
      "Train Epoch: 108 [2560/50000 (6%)]\tLoss: 0.187354, Accuracy: 93.95\n",
      "Train Epoch: 108 [5120/50000 (11%)]\tLoss: 0.190941, Accuracy: 94.14\n",
      "Train Epoch: 108 [7680/50000 (17%)]\tLoss: 0.227245, Accuracy: 93.75\n",
      "Train Epoch: 108 [10240/50000 (23%)]\tLoss: 0.212119, Accuracy: 93.36\n",
      "Train Epoch: 108 [12800/50000 (28%)]\tLoss: 0.156133, Accuracy: 95.12\n",
      "Train Epoch: 108 [15360/50000 (34%)]\tLoss: 0.213053, Accuracy: 91.80\n",
      "Train Epoch: 108 [17920/50000 (40%)]\tLoss: 0.180994, Accuracy: 94.73\n",
      "Train Epoch: 108 [20480/50000 (45%)]\tLoss: 0.259441, Accuracy: 91.99\n",
      "Train Epoch: 108 [23040/50000 (51%)]\tLoss: 0.236553, Accuracy: 91.21\n",
      "Train Epoch: 108 [25600/50000 (57%)]\tLoss: 0.188391, Accuracy: 92.97\n",
      "Train Epoch: 108 [28160/50000 (62%)]\tLoss: 0.217285, Accuracy: 91.99\n",
      "Train Epoch: 108 [30720/50000 (68%)]\tLoss: 0.207585, Accuracy: 91.21\n",
      "Train Epoch: 108 [33280/50000 (74%)]\tLoss: 0.212966, Accuracy: 92.58\n",
      "Train Epoch: 108 [35840/50000 (80%)]\tLoss: 0.226964, Accuracy: 92.58\n",
      "Train Epoch: 108 [38400/50000 (85%)]\tLoss: 0.182069, Accuracy: 93.55\n",
      "Train Epoch: 108 [40960/50000 (91%)]\tLoss: 0.232828, Accuracy: 92.97\n",
      "Train Epoch: 108 [43520/50000 (97%)]\tLoss: 0.226255, Accuracy: 91.80\n",
      "\n",
      "Validation set: Average loss: 0.5783, Accuracy: 4188/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.2529456615448 s]\n",
      "\n",
      "Test set: Average loss: 0.5785, Accuracy: 8354/10000 (83.54%)\n",
      "\n",
      "Train Epoch: 109 [0/50000 (0%)]\tLoss: 0.188054, Accuracy: 92.97\n",
      "Train Epoch: 109 [2560/50000 (6%)]\tLoss: 0.148684, Accuracy: 95.12\n",
      "Train Epoch: 109 [5120/50000 (11%)]\tLoss: 0.178402, Accuracy: 95.31\n",
      "Train Epoch: 109 [7680/50000 (17%)]\tLoss: 0.182156, Accuracy: 92.77\n",
      "Train Epoch: 109 [10240/50000 (23%)]\tLoss: 0.187256, Accuracy: 92.58\n",
      "Train Epoch: 109 [12800/50000 (28%)]\tLoss: 0.215873, Accuracy: 91.60\n",
      "Train Epoch: 109 [15360/50000 (34%)]\tLoss: 0.171190, Accuracy: 94.14\n",
      "Train Epoch: 109 [17920/50000 (40%)]\tLoss: 0.161305, Accuracy: 94.92\n",
      "Train Epoch: 109 [20480/50000 (45%)]\tLoss: 0.183279, Accuracy: 93.36\n",
      "Train Epoch: 109 [23040/50000 (51%)]\tLoss: 0.168748, Accuracy: 94.14\n",
      "Train Epoch: 109 [25600/50000 (57%)]\tLoss: 0.160025, Accuracy: 95.31\n",
      "Train Epoch: 109 [28160/50000 (62%)]\tLoss: 0.191222, Accuracy: 93.75\n",
      "Train Epoch: 109 [30720/50000 (68%)]\tLoss: 0.255118, Accuracy: 92.58\n",
      "Train Epoch: 109 [33280/50000 (74%)]\tLoss: 0.189363, Accuracy: 93.36\n",
      "Train Epoch: 109 [35840/50000 (80%)]\tLoss: 0.254183, Accuracy: 90.62\n",
      "Train Epoch: 109 [38400/50000 (85%)]\tLoss: 0.224006, Accuracy: 93.95\n",
      "Train Epoch: 109 [40960/50000 (91%)]\tLoss: 0.173721, Accuracy: 93.75\n",
      "Train Epoch: 109 [43520/50000 (97%)]\tLoss: 0.264583, Accuracy: 91.60\n",
      "\n",
      "Validation set: Average loss: 0.6232, Accuracy: 4122/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.52318835258484 s]\n",
      "Train Epoch: 110 [0/50000 (0%)]\tLoss: 0.184704, Accuracy: 93.55\n",
      "Train Epoch: 110 [2560/50000 (6%)]\tLoss: 0.247023, Accuracy: 91.60\n",
      "Train Epoch: 110 [5120/50000 (11%)]\tLoss: 0.203249, Accuracy: 93.36\n",
      "Train Epoch: 110 [7680/50000 (17%)]\tLoss: 0.170234, Accuracy: 94.34\n",
      "Train Epoch: 110 [10240/50000 (23%)]\tLoss: 0.208344, Accuracy: 93.16\n",
      "Train Epoch: 110 [12800/50000 (28%)]\tLoss: 0.210957, Accuracy: 92.19\n",
      "Train Epoch: 110 [15360/50000 (34%)]\tLoss: 0.214479, Accuracy: 93.36\n",
      "Train Epoch: 110 [17920/50000 (40%)]\tLoss: 0.181018, Accuracy: 94.73\n",
      "Train Epoch: 110 [20480/50000 (45%)]\tLoss: 0.144947, Accuracy: 94.53\n",
      "Train Epoch: 110 [23040/50000 (51%)]\tLoss: 0.173886, Accuracy: 94.73\n",
      "Train Epoch: 110 [25600/50000 (57%)]\tLoss: 0.156663, Accuracy: 95.31\n",
      "Train Epoch: 110 [28160/50000 (62%)]\tLoss: 0.136306, Accuracy: 95.31\n",
      "Train Epoch: 110 [30720/50000 (68%)]\tLoss: 0.213797, Accuracy: 91.21\n",
      "Train Epoch: 110 [33280/50000 (74%)]\tLoss: 0.165827, Accuracy: 95.31\n",
      "Train Epoch: 110 [35840/50000 (80%)]\tLoss: 0.229211, Accuracy: 91.80\n",
      "Train Epoch: 110 [38400/50000 (85%)]\tLoss: 0.270570, Accuracy: 90.82\n",
      "Train Epoch: 110 [40960/50000 (91%)]\tLoss: 0.234004, Accuracy: 92.58\n",
      "Train Epoch: 110 [43520/50000 (97%)]\tLoss: 0.237079, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.6396, Accuracy: 4087/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.20161271095276 s]\n",
      "\n",
      "Test set: Average loss: 0.6645, Accuracy: 8084/10000 (80.84%)\n",
      "\n",
      "Train Epoch: 111 [0/50000 (0%)]\tLoss: 0.176202, Accuracy: 94.34\n",
      "Train Epoch: 111 [2560/50000 (6%)]\tLoss: 0.177951, Accuracy: 92.77\n",
      "Train Epoch: 111 [5120/50000 (11%)]\tLoss: 0.219517, Accuracy: 92.97\n",
      "Train Epoch: 111 [7680/50000 (17%)]\tLoss: 0.189827, Accuracy: 93.75\n",
      "Train Epoch: 111 [10240/50000 (23%)]\tLoss: 0.172544, Accuracy: 93.75\n",
      "Train Epoch: 111 [12800/50000 (28%)]\tLoss: 0.179165, Accuracy: 93.95\n",
      "Train Epoch: 111 [15360/50000 (34%)]\tLoss: 0.184804, Accuracy: 93.16\n",
      "Train Epoch: 111 [17920/50000 (40%)]\tLoss: 0.189488, Accuracy: 92.19\n",
      "Train Epoch: 111 [20480/50000 (45%)]\tLoss: 0.187728, Accuracy: 93.75\n",
      "Train Epoch: 111 [23040/50000 (51%)]\tLoss: 0.147979, Accuracy: 95.90\n",
      "Train Epoch: 111 [25600/50000 (57%)]\tLoss: 0.214116, Accuracy: 92.38\n",
      "Train Epoch: 111 [28160/50000 (62%)]\tLoss: 0.163760, Accuracy: 93.75\n",
      "Train Epoch: 111 [30720/50000 (68%)]\tLoss: 0.167357, Accuracy: 94.14\n",
      "Train Epoch: 111 [33280/50000 (74%)]\tLoss: 0.185181, Accuracy: 92.38\n",
      "Train Epoch: 111 [35840/50000 (80%)]\tLoss: 0.217399, Accuracy: 92.77\n",
      "Train Epoch: 111 [38400/50000 (85%)]\tLoss: 0.216104, Accuracy: 92.97\n",
      "Train Epoch: 111 [40960/50000 (91%)]\tLoss: 0.185338, Accuracy: 93.36\n",
      "Train Epoch: 111 [43520/50000 (97%)]\tLoss: 0.214171, Accuracy: 93.16\n",
      "\n",
      "Validation set: Average loss: 0.7884, Accuracy: 3854/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[39.50764584541321 s]\n",
      "Train Epoch: 112 [0/50000 (0%)]\tLoss: 0.136805, Accuracy: 94.34\n",
      "Train Epoch: 112 [2560/50000 (6%)]\tLoss: 0.160583, Accuracy: 94.92\n",
      "Train Epoch: 112 [5120/50000 (11%)]\tLoss: 0.200582, Accuracy: 93.95\n",
      "Train Epoch: 112 [7680/50000 (17%)]\tLoss: 0.205784, Accuracy: 94.14\n",
      "Train Epoch: 112 [10240/50000 (23%)]\tLoss: 0.153325, Accuracy: 95.31\n",
      "Train Epoch: 112 [12800/50000 (28%)]\tLoss: 0.186959, Accuracy: 93.16\n",
      "Train Epoch: 112 [15360/50000 (34%)]\tLoss: 0.197542, Accuracy: 92.77\n",
      "Train Epoch: 112 [17920/50000 (40%)]\tLoss: 0.205880, Accuracy: 92.77\n",
      "Train Epoch: 112 [20480/50000 (45%)]\tLoss: 0.202759, Accuracy: 93.36\n",
      "Train Epoch: 112 [23040/50000 (51%)]\tLoss: 0.154669, Accuracy: 94.34\n",
      "Train Epoch: 112 [25600/50000 (57%)]\tLoss: 0.179949, Accuracy: 93.16\n",
      "Train Epoch: 112 [28160/50000 (62%)]\tLoss: 0.165057, Accuracy: 93.55\n",
      "Train Epoch: 112 [30720/50000 (68%)]\tLoss: 0.230675, Accuracy: 91.99\n",
      "Train Epoch: 112 [33280/50000 (74%)]\tLoss: 0.208706, Accuracy: 92.58\n",
      "Train Epoch: 112 [35840/50000 (80%)]\tLoss: 0.184917, Accuracy: 93.16\n",
      "Train Epoch: 112 [38400/50000 (85%)]\tLoss: 0.248738, Accuracy: 92.38\n",
      "Train Epoch: 112 [40960/50000 (91%)]\tLoss: 0.241076, Accuracy: 90.62\n",
      "Train Epoch: 112 [43520/50000 (97%)]\tLoss: 0.150388, Accuracy: 95.12\n",
      "\n",
      "Validation set: Average loss: 0.5225, Accuracy: 4215/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[36.21640181541443 s]\n",
      "\n",
      "Test set: Average loss: 0.5499, Accuracy: 8342/10000 (83.42%)\n",
      "\n",
      "Train Epoch: 113 [0/50000 (0%)]\tLoss: 0.192152, Accuracy: 93.16\n",
      "Train Epoch: 113 [2560/50000 (6%)]\tLoss: 0.191369, Accuracy: 93.16\n",
      "Train Epoch: 113 [5120/50000 (11%)]\tLoss: 0.249908, Accuracy: 90.62\n",
      "Train Epoch: 113 [7680/50000 (17%)]\tLoss: 0.180103, Accuracy: 93.55\n",
      "Train Epoch: 113 [10240/50000 (23%)]\tLoss: 0.200024, Accuracy: 94.14\n",
      "Train Epoch: 113 [12800/50000 (28%)]\tLoss: 0.176913, Accuracy: 93.55\n",
      "Train Epoch: 113 [15360/50000 (34%)]\tLoss: 0.188038, Accuracy: 94.34\n",
      "Train Epoch: 113 [17920/50000 (40%)]\tLoss: 0.180502, Accuracy: 94.34\n",
      "Train Epoch: 113 [20480/50000 (45%)]\tLoss: 0.178673, Accuracy: 94.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 113 [23040/50000 (51%)]\tLoss: 0.140223, Accuracy: 95.12\n",
      "Train Epoch: 113 [25600/50000 (57%)]\tLoss: 0.236938, Accuracy: 92.19\n",
      "Train Epoch: 113 [28160/50000 (62%)]\tLoss: 0.186939, Accuracy: 92.97\n",
      "Train Epoch: 113 [30720/50000 (68%)]\tLoss: 0.161062, Accuracy: 94.92\n",
      "Train Epoch: 113 [33280/50000 (74%)]\tLoss: 0.260994, Accuracy: 90.62\n",
      "Train Epoch: 113 [35840/50000 (80%)]\tLoss: 0.203730, Accuracy: 92.38\n",
      "Train Epoch: 113 [38400/50000 (85%)]\tLoss: 0.235519, Accuracy: 92.19\n",
      "Train Epoch: 113 [40960/50000 (91%)]\tLoss: 0.221795, Accuracy: 93.36\n",
      "Train Epoch: 113 [43520/50000 (97%)]\tLoss: 0.213367, Accuracy: 92.97\n",
      "\n",
      "Validation set: Average loss: 0.5981, Accuracy: 4128/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.274521350860596 s]\n",
      "Train Epoch: 114 [0/50000 (0%)]\tLoss: 0.147767, Accuracy: 94.73\n",
      "Train Epoch: 114 [2560/50000 (6%)]\tLoss: 0.179379, Accuracy: 93.36\n",
      "Train Epoch: 114 [5120/50000 (11%)]\tLoss: 0.188589, Accuracy: 93.36\n",
      "Train Epoch: 114 [7680/50000 (17%)]\tLoss: 0.161959, Accuracy: 93.95\n",
      "Train Epoch: 114 [10240/50000 (23%)]\tLoss: 0.222294, Accuracy: 91.99\n",
      "Train Epoch: 114 [12800/50000 (28%)]\tLoss: 0.219413, Accuracy: 92.77\n",
      "Train Epoch: 114 [15360/50000 (34%)]\tLoss: 0.242035, Accuracy: 91.80\n",
      "Train Epoch: 114 [17920/50000 (40%)]\tLoss: 0.216846, Accuracy: 92.38\n",
      "Train Epoch: 114 [20480/50000 (45%)]\tLoss: 0.208352, Accuracy: 92.97\n",
      "Train Epoch: 114 [23040/50000 (51%)]\tLoss: 0.176709, Accuracy: 93.75\n",
      "Train Epoch: 114 [25600/50000 (57%)]\tLoss: 0.168975, Accuracy: 94.14\n",
      "Train Epoch: 114 [28160/50000 (62%)]\tLoss: 0.204400, Accuracy: 93.75\n",
      "Train Epoch: 114 [30720/50000 (68%)]\tLoss: 0.198567, Accuracy: 93.75\n",
      "Train Epoch: 114 [33280/50000 (74%)]\tLoss: 0.145175, Accuracy: 95.51\n",
      "Train Epoch: 114 [35840/50000 (80%)]\tLoss: 0.184388, Accuracy: 93.55\n",
      "Train Epoch: 114 [38400/50000 (85%)]\tLoss: 0.190565, Accuracy: 92.77\n",
      "Train Epoch: 114 [40960/50000 (91%)]\tLoss: 0.172123, Accuracy: 94.73\n",
      "Train Epoch: 114 [43520/50000 (97%)]\tLoss: 0.203901, Accuracy: 91.21\n",
      "\n",
      "Validation set: Average loss: 0.6496, Accuracy: 4063/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.47983646392822 s]\n",
      "\n",
      "Test set: Average loss: 0.6535, Accuracy: 8176/10000 (81.76%)\n",
      "\n",
      "Train Epoch: 115 [0/50000 (0%)]\tLoss: 0.201796, Accuracy: 93.36\n",
      "Train Epoch: 115 [2560/50000 (6%)]\tLoss: 0.217978, Accuracy: 92.19\n",
      "Train Epoch: 115 [5120/50000 (11%)]\tLoss: 0.187624, Accuracy: 94.14\n",
      "Train Epoch: 115 [7680/50000 (17%)]\tLoss: 0.186892, Accuracy: 93.55\n",
      "Train Epoch: 115 [10240/50000 (23%)]\tLoss: 0.195754, Accuracy: 93.36\n",
      "Train Epoch: 115 [12800/50000 (28%)]\tLoss: 0.198258, Accuracy: 92.58\n",
      "Train Epoch: 115 [15360/50000 (34%)]\tLoss: 0.174603, Accuracy: 93.75\n",
      "Train Epoch: 115 [17920/50000 (40%)]\tLoss: 0.135800, Accuracy: 95.90\n",
      "Train Epoch: 115 [20480/50000 (45%)]\tLoss: 0.159529, Accuracy: 94.92\n",
      "Train Epoch: 115 [23040/50000 (51%)]\tLoss: 0.192707, Accuracy: 93.75\n",
      "Train Epoch: 115 [25600/50000 (57%)]\tLoss: 0.264727, Accuracy: 91.02\n",
      "Train Epoch: 115 [28160/50000 (62%)]\tLoss: 0.224714, Accuracy: 92.19\n",
      "Train Epoch: 115 [30720/50000 (68%)]\tLoss: 0.214425, Accuracy: 92.97\n",
      "Train Epoch: 115 [33280/50000 (74%)]\tLoss: 0.199470, Accuracy: 93.55\n",
      "Train Epoch: 115 [35840/50000 (80%)]\tLoss: 0.169969, Accuracy: 93.36\n",
      "Train Epoch: 115 [38400/50000 (85%)]\tLoss: 0.203076, Accuracy: 93.36\n",
      "Train Epoch: 115 [40960/50000 (91%)]\tLoss: 0.177079, Accuracy: 94.53\n",
      "Train Epoch: 115 [43520/50000 (97%)]\tLoss: 0.175694, Accuracy: 93.95\n",
      "\n",
      "Validation set: Average loss: 0.5866, Accuracy: 4151/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.34722113609314 s]\n",
      "Train Epoch: 116 [0/50000 (0%)]\tLoss: 0.192001, Accuracy: 92.38\n",
      "Train Epoch: 116 [2560/50000 (6%)]\tLoss: 0.125392, Accuracy: 95.51\n",
      "Train Epoch: 116 [5120/50000 (11%)]\tLoss: 0.161502, Accuracy: 94.92\n",
      "Train Epoch: 116 [7680/50000 (17%)]\tLoss: 0.159671, Accuracy: 94.14\n",
      "Train Epoch: 116 [10240/50000 (23%)]\tLoss: 0.240522, Accuracy: 91.80\n",
      "Train Epoch: 116 [12800/50000 (28%)]\tLoss: 0.162682, Accuracy: 94.53\n",
      "Train Epoch: 116 [15360/50000 (34%)]\tLoss: 0.202650, Accuracy: 92.19\n",
      "Train Epoch: 116 [17920/50000 (40%)]\tLoss: 0.218718, Accuracy: 92.97\n",
      "Train Epoch: 116 [20480/50000 (45%)]\tLoss: 0.216920, Accuracy: 92.77\n",
      "Train Epoch: 116 [23040/50000 (51%)]\tLoss: 0.252765, Accuracy: 90.62\n",
      "Train Epoch: 116 [25600/50000 (57%)]\tLoss: 0.175761, Accuracy: 94.14\n",
      "Train Epoch: 116 [28160/50000 (62%)]\tLoss: 0.193964, Accuracy: 93.55\n",
      "Train Epoch: 116 [30720/50000 (68%)]\tLoss: 0.181422, Accuracy: 93.36\n",
      "Train Epoch: 116 [33280/50000 (74%)]\tLoss: 0.205378, Accuracy: 93.16\n",
      "Train Epoch: 116 [35840/50000 (80%)]\tLoss: 0.259086, Accuracy: 91.21\n",
      "Train Epoch: 116 [38400/50000 (85%)]\tLoss: 0.213637, Accuracy: 92.97\n",
      "Train Epoch: 116 [40960/50000 (91%)]\tLoss: 0.190247, Accuracy: 93.36\n",
      "Train Epoch: 116 [43520/50000 (97%)]\tLoss: 0.202199, Accuracy: 92.77\n",
      "\n",
      "Validation set: Average loss: 0.7287, Accuracy: 4014/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.15887427330017 s]\n",
      "\n",
      "Test set: Average loss: 0.7064, Accuracy: 8068/10000 (80.68%)\n",
      "\n",
      "Train Epoch: 117 [0/50000 (0%)]\tLoss: 0.192108, Accuracy: 92.97\n",
      "Train Epoch: 117 [2560/50000 (6%)]\tLoss: 0.225624, Accuracy: 92.97\n",
      "Train Epoch: 117 [5120/50000 (11%)]\tLoss: 0.202383, Accuracy: 91.80\n",
      "Train Epoch: 117 [7680/50000 (17%)]\tLoss: 0.180913, Accuracy: 92.58\n",
      "Train Epoch: 117 [10240/50000 (23%)]\tLoss: 0.197160, Accuracy: 94.14\n",
      "Train Epoch: 117 [12800/50000 (28%)]\tLoss: 0.191933, Accuracy: 93.36\n",
      "Train Epoch: 117 [15360/50000 (34%)]\tLoss: 0.233981, Accuracy: 91.99\n",
      "Train Epoch: 117 [17920/50000 (40%)]\tLoss: 0.173790, Accuracy: 95.12\n",
      "Train Epoch: 117 [20480/50000 (45%)]\tLoss: 0.170080, Accuracy: 94.73\n",
      "Train Epoch: 117 [23040/50000 (51%)]\tLoss: 0.223931, Accuracy: 92.38\n",
      "Train Epoch: 117 [25600/50000 (57%)]\tLoss: 0.231330, Accuracy: 91.80\n",
      "Train Epoch: 117 [28160/50000 (62%)]\tLoss: 0.211756, Accuracy: 92.58\n",
      "Train Epoch: 117 [30720/50000 (68%)]\tLoss: 0.168305, Accuracy: 93.95\n",
      "Train Epoch: 117 [33280/50000 (74%)]\tLoss: 0.187606, Accuracy: 93.75\n",
      "Train Epoch: 117 [35840/50000 (80%)]\tLoss: 0.231026, Accuracy: 92.58\n",
      "Train Epoch: 117 [38400/50000 (85%)]\tLoss: 0.219543, Accuracy: 92.77\n",
      "Train Epoch: 117 [40960/50000 (91%)]\tLoss: 0.200411, Accuracy: 93.16\n",
      "Train Epoch: 117 [43520/50000 (97%)]\tLoss: 0.231132, Accuracy: 92.58\n",
      "\n",
      "Validation set: Average loss: 0.6151, Accuracy: 4102/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.246249198913574 s]\n",
      "Train Epoch: 118 [0/50000 (0%)]\tLoss: 0.186468, Accuracy: 93.36\n",
      "Train Epoch: 118 [2560/50000 (6%)]\tLoss: 0.158604, Accuracy: 94.92\n",
      "Train Epoch: 118 [5120/50000 (11%)]\tLoss: 0.186103, Accuracy: 93.95\n",
      "Train Epoch: 118 [7680/50000 (17%)]\tLoss: 0.181364, Accuracy: 94.92\n",
      "Train Epoch: 118 [10240/50000 (23%)]\tLoss: 0.174126, Accuracy: 93.16\n",
      "Train Epoch: 118 [12800/50000 (28%)]\tLoss: 0.153709, Accuracy: 94.92\n",
      "Train Epoch: 118 [15360/50000 (34%)]\tLoss: 0.151796, Accuracy: 95.12\n",
      "Train Epoch: 118 [17920/50000 (40%)]\tLoss: 0.204655, Accuracy: 92.97\n",
      "Train Epoch: 118 [20480/50000 (45%)]\tLoss: 0.198976, Accuracy: 92.58\n",
      "Train Epoch: 118 [23040/50000 (51%)]\tLoss: 0.183073, Accuracy: 93.55\n",
      "Train Epoch: 118 [25600/50000 (57%)]\tLoss: 0.255851, Accuracy: 91.41\n",
      "Train Epoch: 118 [28160/50000 (62%)]\tLoss: 0.160271, Accuracy: 94.53\n",
      "Train Epoch: 118 [30720/50000 (68%)]\tLoss: 0.195354, Accuracy: 92.97\n",
      "Train Epoch: 118 [33280/50000 (74%)]\tLoss: 0.129045, Accuracy: 95.70\n",
      "Train Epoch: 118 [35840/50000 (80%)]\tLoss: 0.194817, Accuracy: 94.14\n",
      "Train Epoch: 118 [38400/50000 (85%)]\tLoss: 0.199541, Accuracy: 93.95\n",
      "Train Epoch: 118 [40960/50000 (91%)]\tLoss: 0.208922, Accuracy: 92.77\n",
      "Train Epoch: 118 [43520/50000 (97%)]\tLoss: 0.206630, Accuracy: 92.19\n",
      "\n",
      "Validation set: Average loss: 0.7121, Accuracy: 4036/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.12974977493286 s]\n",
      "\n",
      "Test set: Average loss: 0.7284, Accuracy: 8014/10000 (80.14%)\n",
      "\n",
      "Train Epoch: 119 [0/50000 (0%)]\tLoss: 0.247450, Accuracy: 92.58\n",
      "Train Epoch: 119 [2560/50000 (6%)]\tLoss: 0.188805, Accuracy: 93.16\n",
      "Train Epoch: 119 [5120/50000 (11%)]\tLoss: 0.166019, Accuracy: 93.55\n",
      "Train Epoch: 119 [7680/50000 (17%)]\tLoss: 0.190924, Accuracy: 92.97\n",
      "Train Epoch: 119 [10240/50000 (23%)]\tLoss: 0.248190, Accuracy: 91.99\n",
      "Train Epoch: 119 [12800/50000 (28%)]\tLoss: 0.217824, Accuracy: 91.80\n",
      "Train Epoch: 119 [15360/50000 (34%)]\tLoss: 0.164410, Accuracy: 93.75\n",
      "Train Epoch: 119 [17920/50000 (40%)]\tLoss: 0.155257, Accuracy: 93.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 119 [20480/50000 (45%)]\tLoss: 0.152741, Accuracy: 94.92\n",
      "Train Epoch: 119 [23040/50000 (51%)]\tLoss: 0.131730, Accuracy: 96.09\n",
      "Train Epoch: 119 [25600/50000 (57%)]\tLoss: 0.217363, Accuracy: 92.77\n",
      "Train Epoch: 119 [28160/50000 (62%)]\tLoss: 0.186650, Accuracy: 93.95\n",
      "Train Epoch: 119 [30720/50000 (68%)]\tLoss: 0.235040, Accuracy: 91.99\n",
      "Train Epoch: 119 [33280/50000 (74%)]\tLoss: 0.233727, Accuracy: 91.80\n",
      "Train Epoch: 119 [35840/50000 (80%)]\tLoss: 0.190114, Accuracy: 93.75\n",
      "Train Epoch: 119 [38400/50000 (85%)]\tLoss: 0.282595, Accuracy: 91.80\n",
      "Train Epoch: 119 [40960/50000 (91%)]\tLoss: 0.211187, Accuracy: 92.58\n",
      "Train Epoch: 119 [43520/50000 (97%)]\tLoss: 0.184077, Accuracy: 93.55\n",
      "\n",
      "Validation set: Average loss: 0.5926, Accuracy: 4101/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.23335790634155 s]\n",
      "Train Epoch: 120 [0/50000 (0%)]\tLoss: 0.191063, Accuracy: 94.34\n",
      "Train Epoch: 120 [2560/50000 (6%)]\tLoss: 0.147852, Accuracy: 95.31\n",
      "Train Epoch: 120 [5120/50000 (11%)]\tLoss: 0.151667, Accuracy: 95.12\n",
      "Train Epoch: 120 [7680/50000 (17%)]\tLoss: 0.197041, Accuracy: 92.97\n",
      "Train Epoch: 120 [10240/50000 (23%)]\tLoss: 0.217441, Accuracy: 91.99\n",
      "Train Epoch: 120 [12800/50000 (28%)]\tLoss: 0.162985, Accuracy: 94.53\n",
      "Train Epoch: 120 [15360/50000 (34%)]\tLoss: 0.246672, Accuracy: 91.41\n",
      "Train Epoch: 120 [17920/50000 (40%)]\tLoss: 0.246787, Accuracy: 91.60\n",
      "Train Epoch: 120 [20480/50000 (45%)]\tLoss: 0.179462, Accuracy: 93.36\n",
      "Train Epoch: 120 [23040/50000 (51%)]\tLoss: 0.167438, Accuracy: 93.75\n",
      "Train Epoch: 120 [25600/50000 (57%)]\tLoss: 0.253518, Accuracy: 92.19\n",
      "Train Epoch: 120 [28160/50000 (62%)]\tLoss: 0.167578, Accuracy: 93.55\n",
      "Train Epoch: 120 [30720/50000 (68%)]\tLoss: 0.182952, Accuracy: 94.34\n",
      "Train Epoch: 120 [33280/50000 (74%)]\tLoss: 0.208304, Accuracy: 93.16\n",
      "Train Epoch: 120 [35840/50000 (80%)]\tLoss: 0.180283, Accuracy: 94.14\n",
      "Train Epoch: 120 [38400/50000 (85%)]\tLoss: 0.176160, Accuracy: 94.14\n",
      "Train Epoch: 120 [40960/50000 (91%)]\tLoss: 0.196932, Accuracy: 93.75\n",
      "Train Epoch: 120 [43520/50000 (97%)]\tLoss: 0.182798, Accuracy: 92.97\n",
      "\n",
      "Validation set: Average loss: 0.6263, Accuracy: 4057/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.19993257522583 s]\n",
      "\n",
      "Test set: Average loss: 0.6324, Accuracy: 8102/10000 (81.02%)\n",
      "\n",
      "Train Epoch: 121 [0/50000 (0%)]\tLoss: 0.212627, Accuracy: 93.75\n",
      "Train Epoch: 121 [2560/50000 (6%)]\tLoss: 0.168972, Accuracy: 94.73\n",
      "Train Epoch: 121 [5120/50000 (11%)]\tLoss: 0.178169, Accuracy: 93.16\n",
      "Train Epoch: 121 [7680/50000 (17%)]\tLoss: 0.211350, Accuracy: 93.16\n",
      "Train Epoch: 121 [10240/50000 (23%)]\tLoss: 0.124190, Accuracy: 96.09\n",
      "Train Epoch: 121 [12800/50000 (28%)]\tLoss: 0.181147, Accuracy: 93.95\n",
      "Train Epoch: 121 [15360/50000 (34%)]\tLoss: 0.181559, Accuracy: 93.55\n",
      "Train Epoch: 121 [17920/50000 (40%)]\tLoss: 0.178072, Accuracy: 94.14\n",
      "Train Epoch: 121 [20480/50000 (45%)]\tLoss: 0.272891, Accuracy: 90.43\n",
      "Train Epoch: 121 [23040/50000 (51%)]\tLoss: 0.250518, Accuracy: 92.19\n",
      "Train Epoch: 121 [25600/50000 (57%)]\tLoss: 0.207472, Accuracy: 92.38\n",
      "Train Epoch: 121 [28160/50000 (62%)]\tLoss: 0.234273, Accuracy: 91.21\n",
      "Train Epoch: 121 [30720/50000 (68%)]\tLoss: 0.197060, Accuracy: 92.97\n",
      "Train Epoch: 121 [33280/50000 (74%)]\tLoss: 0.174674, Accuracy: 93.16\n",
      "Train Epoch: 121 [35840/50000 (80%)]\tLoss: 0.262788, Accuracy: 91.80\n",
      "Train Epoch: 121 [38400/50000 (85%)]\tLoss: 0.180475, Accuracy: 93.95\n",
      "Train Epoch: 121 [40960/50000 (91%)]\tLoss: 0.261462, Accuracy: 90.04\n",
      "Train Epoch: 121 [43520/50000 (97%)]\tLoss: 0.193658, Accuracy: 94.53\n",
      "\n",
      "Validation set: Average loss: 0.6047, Accuracy: 4074/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[39.294798135757446 s]\n",
      "Train Epoch: 122 [0/50000 (0%)]\tLoss: 0.151235, Accuracy: 94.92\n",
      "Train Epoch: 122 [2560/50000 (6%)]\tLoss: 0.140804, Accuracy: 94.92\n",
      "Train Epoch: 122 [5120/50000 (11%)]\tLoss: 0.163353, Accuracy: 94.34\n",
      "Train Epoch: 122 [7680/50000 (17%)]\tLoss: 0.162736, Accuracy: 93.95\n",
      "Train Epoch: 122 [10240/50000 (23%)]\tLoss: 0.171522, Accuracy: 93.95\n",
      "Train Epoch: 122 [12800/50000 (28%)]\tLoss: 0.149441, Accuracy: 95.70\n",
      "Train Epoch: 122 [15360/50000 (34%)]\tLoss: 0.234529, Accuracy: 91.02\n",
      "Train Epoch: 122 [17920/50000 (40%)]\tLoss: 0.167043, Accuracy: 94.14\n",
      "Train Epoch: 122 [20480/50000 (45%)]\tLoss: 0.188760, Accuracy: 93.55\n",
      "Train Epoch: 122 [23040/50000 (51%)]\tLoss: 0.211792, Accuracy: 92.38\n",
      "Train Epoch: 122 [25600/50000 (57%)]\tLoss: 0.224205, Accuracy: 92.58\n",
      "Train Epoch: 122 [28160/50000 (62%)]\tLoss: 0.252768, Accuracy: 92.77\n",
      "Train Epoch: 122 [30720/50000 (68%)]\tLoss: 0.192546, Accuracy: 93.55\n",
      "Train Epoch: 122 [33280/50000 (74%)]\tLoss: 0.269863, Accuracy: 89.65\n",
      "Train Epoch: 122 [35840/50000 (80%)]\tLoss: 0.198848, Accuracy: 93.55\n",
      "Train Epoch: 122 [38400/50000 (85%)]\tLoss: 0.189942, Accuracy: 92.19\n",
      "Train Epoch: 122 [40960/50000 (91%)]\tLoss: 0.191337, Accuracy: 93.95\n",
      "Train Epoch: 122 [43520/50000 (97%)]\tLoss: 0.211523, Accuracy: 92.58\n",
      "\n",
      "Validation set: Average loss: 0.4870, Accuracy: 4267/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[36.178447008132935 s]\n",
      "\n",
      "Test set: Average loss: 0.4957, Accuracy: 8505/10000 (85.05%)\n",
      "\n",
      "Train Epoch: 123 [0/50000 (0%)]\tLoss: 0.197034, Accuracy: 93.36\n",
      "Train Epoch: 123 [2560/50000 (6%)]\tLoss: 0.138065, Accuracy: 96.09\n",
      "Train Epoch: 123 [5120/50000 (11%)]\tLoss: 0.158902, Accuracy: 94.53\n",
      "Train Epoch: 123 [7680/50000 (17%)]\tLoss: 0.121951, Accuracy: 95.51\n",
      "Train Epoch: 123 [10240/50000 (23%)]\tLoss: 0.177761, Accuracy: 93.36\n",
      "Train Epoch: 123 [12800/50000 (28%)]\tLoss: 0.160773, Accuracy: 93.95\n",
      "Train Epoch: 123 [15360/50000 (34%)]\tLoss: 0.189340, Accuracy: 92.38\n",
      "Train Epoch: 123 [17920/50000 (40%)]\tLoss: 0.250007, Accuracy: 90.43\n",
      "Train Epoch: 123 [20480/50000 (45%)]\tLoss: 0.204631, Accuracy: 92.58\n",
      "Train Epoch: 123 [23040/50000 (51%)]\tLoss: 0.256536, Accuracy: 91.02\n",
      "Train Epoch: 123 [25600/50000 (57%)]\tLoss: 0.188583, Accuracy: 93.55\n",
      "Train Epoch: 123 [28160/50000 (62%)]\tLoss: 0.254006, Accuracy: 92.38\n",
      "Train Epoch: 123 [30720/50000 (68%)]\tLoss: 0.183685, Accuracy: 94.14\n",
      "Train Epoch: 123 [33280/50000 (74%)]\tLoss: 0.252615, Accuracy: 92.19\n",
      "Train Epoch: 123 [35840/50000 (80%)]\tLoss: 0.223780, Accuracy: 91.99\n",
      "Train Epoch: 123 [38400/50000 (85%)]\tLoss: 0.172906, Accuracy: 94.14\n",
      "Train Epoch: 123 [40960/50000 (91%)]\tLoss: 0.202251, Accuracy: 93.36\n",
      "Train Epoch: 123 [43520/50000 (97%)]\tLoss: 0.201017, Accuracy: 92.97\n",
      "\n",
      "Validation set: Average loss: 0.6733, Accuracy: 4067/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[39.28789567947388 s]\n",
      "Train Epoch: 124 [0/50000 (0%)]\tLoss: 0.172243, Accuracy: 93.16\n",
      "Train Epoch: 124 [2560/50000 (6%)]\tLoss: 0.238186, Accuracy: 90.04\n",
      "Train Epoch: 124 [5120/50000 (11%)]\tLoss: 0.190151, Accuracy: 93.55\n",
      "Train Epoch: 124 [7680/50000 (17%)]\tLoss: 0.186824, Accuracy: 93.55\n",
      "Train Epoch: 124 [10240/50000 (23%)]\tLoss: 0.178552, Accuracy: 93.55\n",
      "Train Epoch: 124 [12800/50000 (28%)]\tLoss: 0.172958, Accuracy: 93.75\n",
      "Train Epoch: 124 [15360/50000 (34%)]\tLoss: 0.215929, Accuracy: 92.77\n",
      "Train Epoch: 124 [17920/50000 (40%)]\tLoss: 0.172434, Accuracy: 94.34\n",
      "Train Epoch: 124 [20480/50000 (45%)]\tLoss: 0.142850, Accuracy: 94.73\n",
      "Train Epoch: 124 [23040/50000 (51%)]\tLoss: 0.204197, Accuracy: 93.55\n",
      "Train Epoch: 124 [25600/50000 (57%)]\tLoss: 0.158593, Accuracy: 93.75\n",
      "Train Epoch: 124 [28160/50000 (62%)]\tLoss: 0.180477, Accuracy: 93.55\n",
      "Train Epoch: 124 [30720/50000 (68%)]\tLoss: 0.162472, Accuracy: 95.31\n",
      "Train Epoch: 124 [33280/50000 (74%)]\tLoss: 0.164537, Accuracy: 93.55\n",
      "Train Epoch: 124 [35840/50000 (80%)]\tLoss: 0.219562, Accuracy: 92.77\n",
      "Train Epoch: 124 [38400/50000 (85%)]\tLoss: 0.237791, Accuracy: 91.60\n",
      "Train Epoch: 124 [40960/50000 (91%)]\tLoss: 0.228510, Accuracy: 91.21\n",
      "Train Epoch: 124 [43520/50000 (97%)]\tLoss: 0.228608, Accuracy: 91.80\n",
      "\n",
      "Validation set: Average loss: 0.6407, Accuracy: 4056/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.17479228973389 s]\n",
      "\n",
      "Test set: Average loss: 0.6822, Accuracy: 8042/10000 (80.42%)\n",
      "\n",
      "Train Epoch: 125 [0/50000 (0%)]\tLoss: 0.163779, Accuracy: 94.53\n",
      "Train Epoch: 125 [2560/50000 (6%)]\tLoss: 0.188886, Accuracy: 94.34\n",
      "Train Epoch: 125 [5120/50000 (11%)]\tLoss: 0.185377, Accuracy: 93.36\n",
      "Train Epoch: 125 [7680/50000 (17%)]\tLoss: 0.138640, Accuracy: 95.12\n",
      "Train Epoch: 125 [10240/50000 (23%)]\tLoss: 0.164451, Accuracy: 94.92\n",
      "Train Epoch: 125 [12800/50000 (28%)]\tLoss: 0.185892, Accuracy: 93.55\n",
      "Train Epoch: 125 [15360/50000 (34%)]\tLoss: 0.156557, Accuracy: 93.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 125 [17920/50000 (40%)]\tLoss: 0.196982, Accuracy: 92.97\n",
      "Train Epoch: 125 [20480/50000 (45%)]\tLoss: 0.192735, Accuracy: 92.97\n",
      "Train Epoch: 125 [23040/50000 (51%)]\tLoss: 0.233033, Accuracy: 93.16\n",
      "Train Epoch: 125 [25600/50000 (57%)]\tLoss: 0.193550, Accuracy: 93.36\n",
      "Train Epoch: 125 [28160/50000 (62%)]\tLoss: 0.210344, Accuracy: 93.55\n",
      "Train Epoch: 125 [30720/50000 (68%)]\tLoss: 0.214811, Accuracy: 92.97\n",
      "Train Epoch: 125 [33280/50000 (74%)]\tLoss: 0.220018, Accuracy: 92.38\n",
      "Train Epoch: 125 [35840/50000 (80%)]\tLoss: 0.215081, Accuracy: 93.16\n",
      "Train Epoch: 125 [38400/50000 (85%)]\tLoss: 0.216577, Accuracy: 93.75\n",
      "Train Epoch: 125 [40960/50000 (91%)]\tLoss: 0.195847, Accuracy: 93.36\n",
      "Train Epoch: 125 [43520/50000 (97%)]\tLoss: 0.203846, Accuracy: 91.80\n",
      "\n",
      "Validation set: Average loss: 0.5129, Accuracy: 4219/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[39.284104108810425 s]\n",
      "Train Epoch: 126 [0/50000 (0%)]\tLoss: 0.190542, Accuracy: 93.95\n",
      "Train Epoch: 126 [2560/50000 (6%)]\tLoss: 0.167863, Accuracy: 93.75\n",
      "Train Epoch: 126 [5120/50000 (11%)]\tLoss: 0.175361, Accuracy: 93.36\n",
      "Train Epoch: 126 [7680/50000 (17%)]\tLoss: 0.163301, Accuracy: 94.73\n",
      "Train Epoch: 126 [10240/50000 (23%)]\tLoss: 0.186774, Accuracy: 94.14\n",
      "Train Epoch: 126 [12800/50000 (28%)]\tLoss: 0.182696, Accuracy: 93.75\n",
      "Train Epoch: 126 [15360/50000 (34%)]\tLoss: 0.182802, Accuracy: 93.95\n",
      "Train Epoch: 126 [17920/50000 (40%)]\tLoss: 0.211279, Accuracy: 93.95\n",
      "Train Epoch: 126 [20480/50000 (45%)]\tLoss: 0.245823, Accuracy: 91.41\n",
      "Train Epoch: 126 [23040/50000 (51%)]\tLoss: 0.148275, Accuracy: 94.73\n",
      "Train Epoch: 126 [25600/50000 (57%)]\tLoss: 0.215448, Accuracy: 93.16\n",
      "Train Epoch: 126 [28160/50000 (62%)]\tLoss: 0.192257, Accuracy: 92.97\n",
      "Train Epoch: 126 [30720/50000 (68%)]\tLoss: 0.181587, Accuracy: 93.16\n",
      "Train Epoch: 126 [33280/50000 (74%)]\tLoss: 0.170664, Accuracy: 93.36\n",
      "Train Epoch: 126 [35840/50000 (80%)]\tLoss: 0.158430, Accuracy: 94.73\n",
      "Train Epoch: 126 [38400/50000 (85%)]\tLoss: 0.166279, Accuracy: 95.12\n",
      "Train Epoch: 126 [40960/50000 (91%)]\tLoss: 0.178853, Accuracy: 94.34\n",
      "Train Epoch: 126 [43520/50000 (97%)]\tLoss: 0.279272, Accuracy: 90.23\n",
      "\n",
      "Validation set: Average loss: 0.5158, Accuracy: 4232/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[36.228031158447266 s]\n",
      "\n",
      "Test set: Average loss: 0.5410, Accuracy: 8349/10000 (83.49%)\n",
      "\n",
      "Train Epoch: 127 [0/50000 (0%)]\tLoss: 0.185189, Accuracy: 94.14\n",
      "Train Epoch: 127 [2560/50000 (6%)]\tLoss: 0.240269, Accuracy: 91.41\n",
      "Train Epoch: 127 [5120/50000 (11%)]\tLoss: 0.148628, Accuracy: 94.73\n",
      "Train Epoch: 127 [7680/50000 (17%)]\tLoss: 0.173772, Accuracy: 95.12\n",
      "Train Epoch: 127 [10240/50000 (23%)]\tLoss: 0.217091, Accuracy: 93.95\n",
      "Train Epoch: 127 [12800/50000 (28%)]\tLoss: 0.169829, Accuracy: 94.73\n",
      "Train Epoch: 127 [15360/50000 (34%)]\tLoss: 0.150266, Accuracy: 94.53\n",
      "Train Epoch: 127 [17920/50000 (40%)]\tLoss: 0.209558, Accuracy: 91.60\n",
      "Train Epoch: 127 [20480/50000 (45%)]\tLoss: 0.212245, Accuracy: 92.77\n",
      "Train Epoch: 127 [23040/50000 (51%)]\tLoss: 0.149386, Accuracy: 95.51\n",
      "Train Epoch: 127 [25600/50000 (57%)]\tLoss: 0.178482, Accuracy: 93.36\n",
      "Train Epoch: 127 [28160/50000 (62%)]\tLoss: 0.208599, Accuracy: 93.75\n",
      "Train Epoch: 127 [30720/50000 (68%)]\tLoss: 0.218700, Accuracy: 92.19\n",
      "Train Epoch: 127 [33280/50000 (74%)]\tLoss: 0.170191, Accuracy: 94.14\n",
      "Train Epoch: 127 [35840/50000 (80%)]\tLoss: 0.207770, Accuracy: 92.77\n",
      "Train Epoch: 127 [38400/50000 (85%)]\tLoss: 0.226246, Accuracy: 92.19\n",
      "Train Epoch: 127 [40960/50000 (91%)]\tLoss: 0.208710, Accuracy: 92.97\n",
      "Train Epoch: 127 [43520/50000 (97%)]\tLoss: 0.187776, Accuracy: 93.55\n",
      "\n",
      "Validation set: Average loss: 0.4102, Accuracy: 4362/5000 (87.00%)\n",
      "\n",
      "the time of this epoch:[39.23683834075928 s]\n",
      "Train Epoch: 128 [0/50000 (0%)]\tLoss: 0.165526, Accuracy: 94.34\n",
      "Train Epoch: 128 [2560/50000 (6%)]\tLoss: 0.191366, Accuracy: 92.77\n",
      "Train Epoch: 128 [5120/50000 (11%)]\tLoss: 0.165594, Accuracy: 94.14\n",
      "Train Epoch: 128 [7680/50000 (17%)]\tLoss: 0.192477, Accuracy: 93.36\n",
      "Train Epoch: 128 [10240/50000 (23%)]\tLoss: 0.131039, Accuracy: 95.31\n",
      "Train Epoch: 128 [12800/50000 (28%)]\tLoss: 0.167326, Accuracy: 93.55\n",
      "Train Epoch: 128 [15360/50000 (34%)]\tLoss: 0.179997, Accuracy: 93.36\n",
      "Train Epoch: 128 [17920/50000 (40%)]\tLoss: 0.204240, Accuracy: 92.97\n",
      "Train Epoch: 128 [20480/50000 (45%)]\tLoss: 0.178299, Accuracy: 95.51\n",
      "Train Epoch: 128 [23040/50000 (51%)]\tLoss: 0.250394, Accuracy: 90.62\n",
      "Train Epoch: 128 [25600/50000 (57%)]\tLoss: 0.165343, Accuracy: 94.92\n",
      "Train Epoch: 128 [28160/50000 (62%)]\tLoss: 0.201194, Accuracy: 92.38\n",
      "Train Epoch: 128 [30720/50000 (68%)]\tLoss: 0.302683, Accuracy: 89.06\n",
      "Train Epoch: 128 [33280/50000 (74%)]\tLoss: 0.219111, Accuracy: 91.60\n",
      "Train Epoch: 128 [35840/50000 (80%)]\tLoss: 0.164931, Accuracy: 94.53\n",
      "Train Epoch: 128 [38400/50000 (85%)]\tLoss: 0.256117, Accuracy: 91.21\n",
      "Train Epoch: 128 [40960/50000 (91%)]\tLoss: 0.212695, Accuracy: 92.19\n",
      "Train Epoch: 128 [43520/50000 (97%)]\tLoss: 0.189974, Accuracy: 93.95\n",
      "\n",
      "Validation set: Average loss: 0.5416, Accuracy: 4216/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[36.14698052406311 s]\n",
      "\n",
      "Test set: Average loss: 0.5589, Accuracy: 8338/10000 (83.38%)\n",
      "\n",
      "Train Epoch: 129 [0/50000 (0%)]\tLoss: 0.159197, Accuracy: 93.95\n",
      "Train Epoch: 129 [2560/50000 (6%)]\tLoss: 0.140177, Accuracy: 94.73\n",
      "Train Epoch: 129 [5120/50000 (11%)]\tLoss: 0.171990, Accuracy: 94.92\n",
      "Train Epoch: 129 [7680/50000 (17%)]\tLoss: 0.164764, Accuracy: 95.31\n",
      "Train Epoch: 129 [10240/50000 (23%)]\tLoss: 0.165988, Accuracy: 93.95\n",
      "Train Epoch: 129 [12800/50000 (28%)]\tLoss: 0.147135, Accuracy: 95.12\n",
      "Train Epoch: 129 [15360/50000 (34%)]\tLoss: 0.209268, Accuracy: 91.99\n",
      "Train Epoch: 129 [17920/50000 (40%)]\tLoss: 0.236669, Accuracy: 92.19\n",
      "Train Epoch: 129 [20480/50000 (45%)]\tLoss: 0.202920, Accuracy: 92.97\n",
      "Train Epoch: 129 [23040/50000 (51%)]\tLoss: 0.165451, Accuracy: 93.16\n",
      "Train Epoch: 129 [25600/50000 (57%)]\tLoss: 0.190605, Accuracy: 93.55\n",
      "Train Epoch: 129 [28160/50000 (62%)]\tLoss: 0.174336, Accuracy: 94.14\n",
      "Train Epoch: 129 [30720/50000 (68%)]\tLoss: 0.159463, Accuracy: 93.95\n",
      "Train Epoch: 129 [33280/50000 (74%)]\tLoss: 0.224580, Accuracy: 92.77\n",
      "Train Epoch: 129 [35840/50000 (80%)]\tLoss: 0.161153, Accuracy: 93.75\n",
      "Train Epoch: 129 [38400/50000 (85%)]\tLoss: 0.223299, Accuracy: 91.80\n",
      "Train Epoch: 129 [40960/50000 (91%)]\tLoss: 0.161150, Accuracy: 94.73\n",
      "Train Epoch: 129 [43520/50000 (97%)]\tLoss: 0.206274, Accuracy: 93.16\n",
      "\n",
      "Validation set: Average loss: 0.6414, Accuracy: 4062/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[39.26424765586853 s]\n",
      "Train Epoch: 130 [0/50000 (0%)]\tLoss: 0.144756, Accuracy: 95.70\n",
      "Train Epoch: 130 [2560/50000 (6%)]\tLoss: 0.174743, Accuracy: 94.34\n",
      "Train Epoch: 130 [5120/50000 (11%)]\tLoss: 0.140294, Accuracy: 94.73\n",
      "Train Epoch: 130 [7680/50000 (17%)]\tLoss: 0.156718, Accuracy: 94.92\n",
      "Train Epoch: 130 [10240/50000 (23%)]\tLoss: 0.218169, Accuracy: 92.97\n",
      "Train Epoch: 130 [12800/50000 (28%)]\tLoss: 0.207482, Accuracy: 91.80\n",
      "Train Epoch: 130 [15360/50000 (34%)]\tLoss: 0.191517, Accuracy: 93.55\n",
      "Train Epoch: 130 [17920/50000 (40%)]\tLoss: 0.171761, Accuracy: 94.14\n",
      "Train Epoch: 130 [20480/50000 (45%)]\tLoss: 0.203381, Accuracy: 91.60\n",
      "Train Epoch: 130 [23040/50000 (51%)]\tLoss: 0.198773, Accuracy: 94.14\n",
      "Train Epoch: 130 [25600/50000 (57%)]\tLoss: 0.202348, Accuracy: 93.16\n",
      "Train Epoch: 130 [28160/50000 (62%)]\tLoss: 0.185915, Accuracy: 93.75\n",
      "Train Epoch: 130 [30720/50000 (68%)]\tLoss: 0.235488, Accuracy: 90.62\n",
      "Train Epoch: 130 [33280/50000 (74%)]\tLoss: 0.192022, Accuracy: 94.34\n",
      "Train Epoch: 130 [35840/50000 (80%)]\tLoss: 0.227933, Accuracy: 92.97\n",
      "Train Epoch: 130 [38400/50000 (85%)]\tLoss: 0.221766, Accuracy: 91.60\n",
      "Train Epoch: 130 [40960/50000 (91%)]\tLoss: 0.241699, Accuracy: 91.21\n",
      "Train Epoch: 130 [43520/50000 (97%)]\tLoss: 0.164880, Accuracy: 94.53\n",
      "\n",
      "Validation set: Average loss: 0.8313, Accuracy: 3904/5000 (78.00%)\n",
      "\n",
      "the time of this epoch:[36.158918380737305 s]\n",
      "\n",
      "Test set: Average loss: 0.8519, Accuracy: 7793/10000 (77.93%)\n",
      "\n",
      "Train Epoch: 131 [0/50000 (0%)]\tLoss: 0.154513, Accuracy: 94.34\n",
      "Train Epoch: 131 [2560/50000 (6%)]\tLoss: 0.242836, Accuracy: 91.21\n",
      "Train Epoch: 131 [5120/50000 (11%)]\tLoss: 0.154864, Accuracy: 94.92\n",
      "Train Epoch: 131 [7680/50000 (17%)]\tLoss: 0.164251, Accuracy: 94.34\n",
      "Train Epoch: 131 [10240/50000 (23%)]\tLoss: 0.156369, Accuracy: 93.55\n",
      "Train Epoch: 131 [12800/50000 (28%)]\tLoss: 0.200003, Accuracy: 93.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 131 [15360/50000 (34%)]\tLoss: 0.208307, Accuracy: 93.16\n",
      "Train Epoch: 131 [17920/50000 (40%)]\tLoss: 0.242298, Accuracy: 92.19\n",
      "Train Epoch: 131 [20480/50000 (45%)]\tLoss: 0.217606, Accuracy: 91.60\n",
      "Train Epoch: 131 [23040/50000 (51%)]\tLoss: 0.235437, Accuracy: 91.60\n",
      "Train Epoch: 131 [25600/50000 (57%)]\tLoss: 0.216913, Accuracy: 91.99\n",
      "Train Epoch: 131 [28160/50000 (62%)]\tLoss: 0.231940, Accuracy: 91.99\n",
      "Train Epoch: 131 [30720/50000 (68%)]\tLoss: 0.188230, Accuracy: 92.77\n",
      "Train Epoch: 131 [33280/50000 (74%)]\tLoss: 0.181366, Accuracy: 93.36\n",
      "Train Epoch: 131 [35840/50000 (80%)]\tLoss: 0.183613, Accuracy: 93.75\n",
      "Train Epoch: 131 [38400/50000 (85%)]\tLoss: 0.186274, Accuracy: 93.55\n",
      "Train Epoch: 131 [40960/50000 (91%)]\tLoss: 0.220143, Accuracy: 91.99\n",
      "Train Epoch: 131 [43520/50000 (97%)]\tLoss: 0.198395, Accuracy: 93.95\n",
      "\n",
      "Validation set: Average loss: 0.6337, Accuracy: 4041/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[39.29997658729553 s]\n",
      "Train Epoch: 132 [0/50000 (0%)]\tLoss: 0.185130, Accuracy: 93.75\n",
      "Train Epoch: 132 [2560/50000 (6%)]\tLoss: 0.197229, Accuracy: 92.97\n",
      "Train Epoch: 132 [5120/50000 (11%)]\tLoss: 0.221358, Accuracy: 91.60\n",
      "Train Epoch: 132 [7680/50000 (17%)]\tLoss: 0.153980, Accuracy: 94.53\n",
      "Train Epoch: 132 [10240/50000 (23%)]\tLoss: 0.208755, Accuracy: 92.19\n",
      "Train Epoch: 132 [12800/50000 (28%)]\tLoss: 0.178872, Accuracy: 93.36\n",
      "Train Epoch: 132 [15360/50000 (34%)]\tLoss: 0.179644, Accuracy: 93.55\n",
      "Train Epoch: 132 [17920/50000 (40%)]\tLoss: 0.233891, Accuracy: 91.21\n",
      "Train Epoch: 132 [20480/50000 (45%)]\tLoss: 0.186698, Accuracy: 93.75\n",
      "Train Epoch: 132 [23040/50000 (51%)]\tLoss: 0.161802, Accuracy: 93.36\n",
      "Train Epoch: 132 [25600/50000 (57%)]\tLoss: 0.195474, Accuracy: 92.19\n",
      "Train Epoch: 132 [28160/50000 (62%)]\tLoss: 0.239858, Accuracy: 90.04\n",
      "Train Epoch: 132 [30720/50000 (68%)]\tLoss: 0.228485, Accuracy: 93.55\n",
      "Train Epoch: 132 [33280/50000 (74%)]\tLoss: 0.228576, Accuracy: 92.19\n",
      "Train Epoch: 132 [35840/50000 (80%)]\tLoss: 0.186347, Accuracy: 92.97\n",
      "Train Epoch: 132 [38400/50000 (85%)]\tLoss: 0.193875, Accuracy: 92.77\n",
      "Train Epoch: 132 [40960/50000 (91%)]\tLoss: 0.211665, Accuracy: 92.77\n",
      "Train Epoch: 132 [43520/50000 (97%)]\tLoss: 0.210322, Accuracy: 91.80\n",
      "\n",
      "Validation set: Average loss: 0.5001, Accuracy: 4247/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[36.181713581085205 s]\n",
      "\n",
      "Test set: Average loss: 0.5084, Accuracy: 8457/10000 (84.57%)\n",
      "\n",
      "Train Epoch: 133 [0/50000 (0%)]\tLoss: 0.119669, Accuracy: 95.31\n",
      "Train Epoch: 133 [2560/50000 (6%)]\tLoss: 0.142734, Accuracy: 96.29\n",
      "Train Epoch: 133 [5120/50000 (11%)]\tLoss: 0.148162, Accuracy: 93.75\n",
      "Train Epoch: 133 [7680/50000 (17%)]\tLoss: 0.159669, Accuracy: 95.12\n",
      "Train Epoch: 133 [10240/50000 (23%)]\tLoss: 0.152095, Accuracy: 95.12\n",
      "Train Epoch: 133 [12800/50000 (28%)]\tLoss: 0.161674, Accuracy: 94.34\n",
      "Train Epoch: 133 [15360/50000 (34%)]\tLoss: 0.240994, Accuracy: 92.97\n",
      "Train Epoch: 133 [17920/50000 (40%)]\tLoss: 0.254813, Accuracy: 91.99\n",
      "Train Epoch: 133 [20480/50000 (45%)]\tLoss: 0.242835, Accuracy: 91.60\n",
      "Train Epoch: 133 [23040/50000 (51%)]\tLoss: 0.230306, Accuracy: 91.02\n",
      "Train Epoch: 133 [25600/50000 (57%)]\tLoss: 0.248132, Accuracy: 91.02\n",
      "Train Epoch: 133 [28160/50000 (62%)]\tLoss: 0.240371, Accuracy: 92.77\n",
      "Train Epoch: 133 [30720/50000 (68%)]\tLoss: 0.238418, Accuracy: 91.60\n",
      "Train Epoch: 133 [33280/50000 (74%)]\tLoss: 0.189577, Accuracy: 94.73\n",
      "Train Epoch: 133 [35840/50000 (80%)]\tLoss: 0.180150, Accuracy: 94.92\n",
      "Train Epoch: 133 [38400/50000 (85%)]\tLoss: 0.218438, Accuracy: 91.41\n",
      "Train Epoch: 133 [40960/50000 (91%)]\tLoss: 0.177016, Accuracy: 94.14\n",
      "Train Epoch: 133 [43520/50000 (97%)]\tLoss: 0.200978, Accuracy: 93.75\n",
      "\n",
      "Validation set: Average loss: 0.5422, Accuracy: 4197/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.17751479148865 s]\n",
      "Train Epoch: 134 [0/50000 (0%)]\tLoss: 0.155593, Accuracy: 94.73\n",
      "Train Epoch: 134 [2560/50000 (6%)]\tLoss: 0.159762, Accuracy: 94.53\n",
      "Train Epoch: 134 [5120/50000 (11%)]\tLoss: 0.173298, Accuracy: 93.95\n",
      "Train Epoch: 134 [7680/50000 (17%)]\tLoss: 0.175188, Accuracy: 93.75\n",
      "Train Epoch: 134 [10240/50000 (23%)]\tLoss: 0.195203, Accuracy: 93.55\n",
      "Train Epoch: 134 [12800/50000 (28%)]\tLoss: 0.184045, Accuracy: 94.34\n",
      "Train Epoch: 134 [15360/50000 (34%)]\tLoss: 0.181773, Accuracy: 93.95\n",
      "Train Epoch: 134 [17920/50000 (40%)]\tLoss: 0.149470, Accuracy: 94.53\n",
      "Train Epoch: 134 [20480/50000 (45%)]\tLoss: 0.206447, Accuracy: 92.38\n",
      "Train Epoch: 134 [23040/50000 (51%)]\tLoss: 0.121875, Accuracy: 94.92\n",
      "Train Epoch: 134 [25600/50000 (57%)]\tLoss: 0.230424, Accuracy: 91.60\n",
      "Train Epoch: 134 [28160/50000 (62%)]\tLoss: 0.190537, Accuracy: 94.14\n",
      "Train Epoch: 134 [30720/50000 (68%)]\tLoss: 0.222940, Accuracy: 92.77\n",
      "Train Epoch: 134 [33280/50000 (74%)]\tLoss: 0.180451, Accuracy: 93.55\n",
      "Train Epoch: 134 [35840/50000 (80%)]\tLoss: 0.188205, Accuracy: 93.36\n",
      "Train Epoch: 134 [38400/50000 (85%)]\tLoss: 0.245159, Accuracy: 90.43\n",
      "Train Epoch: 134 [40960/50000 (91%)]\tLoss: 0.204858, Accuracy: 92.38\n",
      "Train Epoch: 134 [43520/50000 (97%)]\tLoss: 0.196096, Accuracy: 93.75\n",
      "\n",
      "Validation set: Average loss: 0.7739, Accuracy: 3869/5000 (77.00%)\n",
      "\n",
      "the time of this epoch:[36.129549980163574 s]\n",
      "\n",
      "Test set: Average loss: 0.7675, Accuracy: 7816/10000 (78.16%)\n",
      "\n",
      "Train Epoch: 135 [0/50000 (0%)]\tLoss: 0.219041, Accuracy: 92.77\n",
      "Train Epoch: 135 [2560/50000 (6%)]\tLoss: 0.171351, Accuracy: 94.14\n",
      "Train Epoch: 135 [5120/50000 (11%)]\tLoss: 0.153052, Accuracy: 94.92\n",
      "Train Epoch: 135 [7680/50000 (17%)]\tLoss: 0.208143, Accuracy: 93.36\n",
      "Train Epoch: 135 [10240/50000 (23%)]\tLoss: 0.202599, Accuracy: 93.55\n",
      "Train Epoch: 135 [12800/50000 (28%)]\tLoss: 0.210028, Accuracy: 93.36\n",
      "Train Epoch: 135 [15360/50000 (34%)]\tLoss: 0.160842, Accuracy: 94.92\n",
      "Train Epoch: 135 [17920/50000 (40%)]\tLoss: 0.136588, Accuracy: 94.53\n",
      "Train Epoch: 135 [20480/50000 (45%)]\tLoss: 0.174217, Accuracy: 94.34\n",
      "Train Epoch: 135 [23040/50000 (51%)]\tLoss: 0.247783, Accuracy: 92.58\n",
      "Train Epoch: 135 [25600/50000 (57%)]\tLoss: 0.181330, Accuracy: 93.55\n",
      "Train Epoch: 135 [28160/50000 (62%)]\tLoss: 0.200757, Accuracy: 92.38\n",
      "Train Epoch: 135 [30720/50000 (68%)]\tLoss: 0.183824, Accuracy: 93.75\n",
      "Train Epoch: 135 [33280/50000 (74%)]\tLoss: 0.251628, Accuracy: 91.21\n",
      "Train Epoch: 135 [35840/50000 (80%)]\tLoss: 0.200034, Accuracy: 92.58\n",
      "Train Epoch: 135 [38400/50000 (85%)]\tLoss: 0.223606, Accuracy: 92.19\n",
      "Train Epoch: 135 [40960/50000 (91%)]\tLoss: 0.158620, Accuracy: 94.73\n",
      "Train Epoch: 135 [43520/50000 (97%)]\tLoss: 0.237250, Accuracy: 91.80\n",
      "\n",
      "Validation set: Average loss: 0.5654, Accuracy: 4146/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.24606990814209 s]\n",
      "Train Epoch: 136 [0/50000 (0%)]\tLoss: 0.159216, Accuracy: 94.92\n",
      "Train Epoch: 136 [2560/50000 (6%)]\tLoss: 0.193604, Accuracy: 92.58\n",
      "Train Epoch: 136 [5120/50000 (11%)]\tLoss: 0.167777, Accuracy: 94.14\n",
      "Train Epoch: 136 [7680/50000 (17%)]\tLoss: 0.158223, Accuracy: 93.55\n",
      "Train Epoch: 136 [10240/50000 (23%)]\tLoss: 0.192450, Accuracy: 93.55\n",
      "Train Epoch: 136 [12800/50000 (28%)]\tLoss: 0.156027, Accuracy: 95.12\n",
      "Train Epoch: 136 [15360/50000 (34%)]\tLoss: 0.187810, Accuracy: 94.14\n",
      "Train Epoch: 136 [17920/50000 (40%)]\tLoss: 0.161173, Accuracy: 94.92\n",
      "Train Epoch: 136 [20480/50000 (45%)]\tLoss: 0.177596, Accuracy: 93.95\n",
      "Train Epoch: 136 [23040/50000 (51%)]\tLoss: 0.202226, Accuracy: 93.16\n",
      "Train Epoch: 136 [25600/50000 (57%)]\tLoss: 0.242698, Accuracy: 92.38\n",
      "Train Epoch: 136 [28160/50000 (62%)]\tLoss: 0.183284, Accuracy: 92.77\n",
      "Train Epoch: 136 [30720/50000 (68%)]\tLoss: 0.201333, Accuracy: 93.75\n",
      "Train Epoch: 136 [33280/50000 (74%)]\tLoss: 0.184297, Accuracy: 93.36\n",
      "Train Epoch: 136 [35840/50000 (80%)]\tLoss: 0.216992, Accuracy: 92.19\n",
      "Train Epoch: 136 [38400/50000 (85%)]\tLoss: 0.267381, Accuracy: 92.19\n",
      "Train Epoch: 136 [40960/50000 (91%)]\tLoss: 0.195588, Accuracy: 93.75\n",
      "Train Epoch: 136 [43520/50000 (97%)]\tLoss: 0.203065, Accuracy: 93.36\n",
      "\n",
      "Validation set: Average loss: 0.6750, Accuracy: 4092/5000 (81.00%)\n",
      "\n",
      "the time of this epoch:[36.13723158836365 s]\n",
      "\n",
      "Test set: Average loss: 0.6888, Accuracy: 8144/10000 (81.44%)\n",
      "\n",
      "Train Epoch: 137 [0/50000 (0%)]\tLoss: 0.180038, Accuracy: 93.75\n",
      "Train Epoch: 137 [2560/50000 (6%)]\tLoss: 0.225301, Accuracy: 91.80\n",
      "Train Epoch: 137 [5120/50000 (11%)]\tLoss: 0.205601, Accuracy: 92.38\n",
      "Train Epoch: 137 [7680/50000 (17%)]\tLoss: 0.205277, Accuracy: 93.36\n",
      "Train Epoch: 137 [10240/50000 (23%)]\tLoss: 0.173470, Accuracy: 93.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 137 [12800/50000 (28%)]\tLoss: 0.198122, Accuracy: 93.75\n",
      "Train Epoch: 137 [15360/50000 (34%)]\tLoss: 0.226511, Accuracy: 93.36\n",
      "Train Epoch: 137 [17920/50000 (40%)]\tLoss: 0.182432, Accuracy: 94.34\n",
      "Train Epoch: 137 [20480/50000 (45%)]\tLoss: 0.206091, Accuracy: 92.58\n",
      "Train Epoch: 137 [23040/50000 (51%)]\tLoss: 0.164117, Accuracy: 94.34\n",
      "Train Epoch: 137 [25600/50000 (57%)]\tLoss: 0.149458, Accuracy: 95.12\n",
      "Train Epoch: 137 [28160/50000 (62%)]\tLoss: 0.257934, Accuracy: 91.99\n",
      "Train Epoch: 137 [30720/50000 (68%)]\tLoss: 0.275321, Accuracy: 91.21\n",
      "Train Epoch: 137 [33280/50000 (74%)]\tLoss: 0.213481, Accuracy: 92.97\n",
      "Train Epoch: 137 [35840/50000 (80%)]\tLoss: 0.175282, Accuracy: 95.12\n",
      "Train Epoch: 137 [38400/50000 (85%)]\tLoss: 0.167471, Accuracy: 94.73\n",
      "Train Epoch: 137 [40960/50000 (91%)]\tLoss: 0.211352, Accuracy: 92.38\n",
      "Train Epoch: 137 [43520/50000 (97%)]\tLoss: 0.211117, Accuracy: 93.16\n",
      "\n",
      "Validation set: Average loss: 0.7852, Accuracy: 3988/5000 (79.00%)\n",
      "\n",
      "the time of this epoch:[39.196898221969604 s]\n",
      "Train Epoch: 138 [0/50000 (0%)]\tLoss: 0.184210, Accuracy: 93.16\n",
      "Train Epoch: 138 [2560/50000 (6%)]\tLoss: 0.188894, Accuracy: 93.95\n",
      "Train Epoch: 138 [5120/50000 (11%)]\tLoss: 0.228851, Accuracy: 91.60\n",
      "Train Epoch: 138 [7680/50000 (17%)]\tLoss: 0.140536, Accuracy: 96.09\n",
      "Train Epoch: 138 [10240/50000 (23%)]\tLoss: 0.146138, Accuracy: 94.92\n",
      "Train Epoch: 138 [12800/50000 (28%)]\tLoss: 0.231535, Accuracy: 90.43\n",
      "Train Epoch: 138 [15360/50000 (34%)]\tLoss: 0.190483, Accuracy: 92.97\n",
      "Train Epoch: 138 [17920/50000 (40%)]\tLoss: 0.172605, Accuracy: 93.16\n",
      "Train Epoch: 138 [20480/50000 (45%)]\tLoss: 0.161923, Accuracy: 94.73\n",
      "Train Epoch: 138 [23040/50000 (51%)]\tLoss: 0.177400, Accuracy: 93.16\n",
      "Train Epoch: 138 [25600/50000 (57%)]\tLoss: 0.206383, Accuracy: 91.80\n",
      "Train Epoch: 138 [28160/50000 (62%)]\tLoss: 0.177856, Accuracy: 93.55\n",
      "Train Epoch: 138 [30720/50000 (68%)]\tLoss: 0.217300, Accuracy: 91.80\n",
      "Train Epoch: 138 [33280/50000 (74%)]\tLoss: 0.230712, Accuracy: 91.41\n",
      "Train Epoch: 138 [35840/50000 (80%)]\tLoss: 0.259591, Accuracy: 92.19\n",
      "Train Epoch: 138 [38400/50000 (85%)]\tLoss: 0.223069, Accuracy: 91.80\n",
      "Train Epoch: 138 [40960/50000 (91%)]\tLoss: 0.161992, Accuracy: 94.53\n",
      "Train Epoch: 138 [43520/50000 (97%)]\tLoss: 0.225600, Accuracy: 92.77\n",
      "\n",
      "Validation set: Average loss: 1.0151, Accuracy: 3700/5000 (74.00%)\n",
      "\n",
      "the time of this epoch:[36.15894961357117 s]\n",
      "\n",
      "Test set: Average loss: 1.0409, Accuracy: 7329/10000 (73.29%)\n",
      "\n",
      "Train Epoch: 139 [0/50000 (0%)]\tLoss: 0.152958, Accuracy: 94.14\n",
      "Train Epoch: 139 [2560/50000 (6%)]\tLoss: 0.229389, Accuracy: 91.60\n",
      "Train Epoch: 139 [5120/50000 (11%)]\tLoss: 0.192855, Accuracy: 93.55\n",
      "Train Epoch: 139 [7680/50000 (17%)]\tLoss: 0.187423, Accuracy: 93.95\n",
      "Train Epoch: 139 [10240/50000 (23%)]\tLoss: 0.135028, Accuracy: 95.51\n",
      "Train Epoch: 139 [12800/50000 (28%)]\tLoss: 0.150810, Accuracy: 95.31\n",
      "Train Epoch: 139 [15360/50000 (34%)]\tLoss: 0.208361, Accuracy: 93.16\n",
      "Train Epoch: 139 [17920/50000 (40%)]\tLoss: 0.181264, Accuracy: 94.73\n",
      "Train Epoch: 139 [20480/50000 (45%)]\tLoss: 0.216058, Accuracy: 91.99\n",
      "Train Epoch: 139 [23040/50000 (51%)]\tLoss: 0.203202, Accuracy: 92.77\n",
      "Train Epoch: 139 [25600/50000 (57%)]\tLoss: 0.210703, Accuracy: 92.58\n",
      "Train Epoch: 139 [28160/50000 (62%)]\tLoss: 0.188469, Accuracy: 93.36\n",
      "Train Epoch: 139 [30720/50000 (68%)]\tLoss: 0.223173, Accuracy: 92.97\n",
      "Train Epoch: 139 [33280/50000 (74%)]\tLoss: 0.238128, Accuracy: 92.19\n",
      "Train Epoch: 139 [35840/50000 (80%)]\tLoss: 0.176040, Accuracy: 93.95\n",
      "Train Epoch: 139 [38400/50000 (85%)]\tLoss: 0.206445, Accuracy: 93.16\n",
      "Train Epoch: 139 [40960/50000 (91%)]\tLoss: 0.155585, Accuracy: 94.92\n",
      "Train Epoch: 139 [43520/50000 (97%)]\tLoss: 0.220224, Accuracy: 92.77\n",
      "\n",
      "Validation set: Average loss: 0.9303, Accuracy: 3770/5000 (75.00%)\n",
      "\n",
      "the time of this epoch:[39.18569302558899 s]\n",
      "Train Epoch: 140 [0/50000 (0%)]\tLoss: 0.166585, Accuracy: 95.12\n",
      "Train Epoch: 140 [2560/50000 (6%)]\tLoss: 0.183227, Accuracy: 93.16\n",
      "Train Epoch: 140 [5120/50000 (11%)]\tLoss: 0.196114, Accuracy: 92.97\n",
      "Train Epoch: 140 [7680/50000 (17%)]\tLoss: 0.199585, Accuracy: 93.36\n",
      "Train Epoch: 140 [10240/50000 (23%)]\tLoss: 0.147464, Accuracy: 95.90\n",
      "Train Epoch: 140 [12800/50000 (28%)]\tLoss: 0.173415, Accuracy: 94.73\n",
      "Train Epoch: 140 [15360/50000 (34%)]\tLoss: 0.166901, Accuracy: 96.09\n",
      "Train Epoch: 140 [17920/50000 (40%)]\tLoss: 0.177201, Accuracy: 93.55\n",
      "Train Epoch: 140 [20480/50000 (45%)]\tLoss: 0.138049, Accuracy: 95.70\n",
      "Train Epoch: 140 [23040/50000 (51%)]\tLoss: 0.211471, Accuracy: 91.60\n",
      "Train Epoch: 140 [25600/50000 (57%)]\tLoss: 0.157088, Accuracy: 95.12\n",
      "Train Epoch: 140 [28160/50000 (62%)]\tLoss: 0.225026, Accuracy: 91.02\n",
      "Train Epoch: 140 [30720/50000 (68%)]\tLoss: 0.238288, Accuracy: 90.43\n",
      "Train Epoch: 140 [33280/50000 (74%)]\tLoss: 0.222991, Accuracy: 91.41\n",
      "Train Epoch: 140 [35840/50000 (80%)]\tLoss: 0.253276, Accuracy: 91.41\n",
      "Train Epoch: 140 [38400/50000 (85%)]\tLoss: 0.140266, Accuracy: 95.12\n",
      "Train Epoch: 140 [40960/50000 (91%)]\tLoss: 0.202992, Accuracy: 92.38\n",
      "Train Epoch: 140 [43520/50000 (97%)]\tLoss: 0.200335, Accuracy: 93.16\n",
      "\n",
      "Validation set: Average loss: 0.5321, Accuracy: 4236/5000 (84.00%)\n",
      "\n",
      "the time of this epoch:[36.12030839920044 s]\n",
      "\n",
      "Test set: Average loss: 0.5506, Accuracy: 8438/10000 (84.38%)\n",
      "\n",
      "Train Epoch: 141 [0/50000 (0%)]\tLoss: 0.184670, Accuracy: 93.16\n",
      "Train Epoch: 141 [2560/50000 (6%)]\tLoss: 0.171609, Accuracy: 94.92\n",
      "Train Epoch: 141 [5120/50000 (11%)]\tLoss: 0.148493, Accuracy: 94.73\n",
      "Train Epoch: 141 [7680/50000 (17%)]\tLoss: 0.197982, Accuracy: 93.16\n",
      "Train Epoch: 141 [10240/50000 (23%)]\tLoss: 0.189898, Accuracy: 93.16\n",
      "Train Epoch: 141 [12800/50000 (28%)]\tLoss: 0.162178, Accuracy: 94.53\n",
      "Train Epoch: 141 [15360/50000 (34%)]\tLoss: 0.157708, Accuracy: 94.73\n",
      "Train Epoch: 141 [17920/50000 (40%)]\tLoss: 0.245023, Accuracy: 91.80\n",
      "Train Epoch: 141 [20480/50000 (45%)]\tLoss: 0.169797, Accuracy: 94.92\n",
      "Train Epoch: 141 [23040/50000 (51%)]\tLoss: 0.144771, Accuracy: 95.12\n",
      "Train Epoch: 141 [25600/50000 (57%)]\tLoss: 0.199785, Accuracy: 93.55\n",
      "Train Epoch: 141 [28160/50000 (62%)]\tLoss: 0.200775, Accuracy: 91.80\n",
      "Train Epoch: 141 [30720/50000 (68%)]\tLoss: 0.184892, Accuracy: 93.36\n",
      "Train Epoch: 141 [33280/50000 (74%)]\tLoss: 0.189711, Accuracy: 92.77\n",
      "Train Epoch: 141 [35840/50000 (80%)]\tLoss: 0.253800, Accuracy: 90.82\n",
      "Train Epoch: 141 [38400/50000 (85%)]\tLoss: 0.182507, Accuracy: 93.55\n",
      "Train Epoch: 141 [40960/50000 (91%)]\tLoss: 0.253463, Accuracy: 92.19\n",
      "Train Epoch: 141 [43520/50000 (97%)]\tLoss: 0.226729, Accuracy: 91.02\n",
      "\n",
      "Validation set: Average loss: 0.4288, Accuracy: 4299/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[39.255865812301636 s]\n",
      "Train Epoch: 142 [0/50000 (0%)]\tLoss: 0.187925, Accuracy: 93.75\n",
      "Train Epoch: 142 [2560/50000 (6%)]\tLoss: 0.197061, Accuracy: 93.16\n",
      "Train Epoch: 142 [5120/50000 (11%)]\tLoss: 0.166128, Accuracy: 95.51\n",
      "Train Epoch: 142 [7680/50000 (17%)]\tLoss: 0.171005, Accuracy: 94.14\n",
      "Train Epoch: 142 [10240/50000 (23%)]\tLoss: 0.190761, Accuracy: 93.55\n",
      "Train Epoch: 142 [12800/50000 (28%)]\tLoss: 0.216210, Accuracy: 92.97\n",
      "Train Epoch: 142 [15360/50000 (34%)]\tLoss: 0.198413, Accuracy: 93.16\n",
      "Train Epoch: 142 [17920/50000 (40%)]\tLoss: 0.149549, Accuracy: 95.31\n",
      "Train Epoch: 142 [20480/50000 (45%)]\tLoss: 0.200267, Accuracy: 93.16\n",
      "Train Epoch: 142 [23040/50000 (51%)]\tLoss: 0.186451, Accuracy: 93.16\n",
      "Train Epoch: 142 [25600/50000 (57%)]\tLoss: 0.203022, Accuracy: 93.75\n",
      "Train Epoch: 142 [28160/50000 (62%)]\tLoss: 0.186295, Accuracy: 93.95\n",
      "Train Epoch: 142 [30720/50000 (68%)]\tLoss: 0.176654, Accuracy: 93.75\n",
      "Train Epoch: 142 [33280/50000 (74%)]\tLoss: 0.206837, Accuracy: 92.19\n",
      "Train Epoch: 142 [35840/50000 (80%)]\tLoss: 0.216798, Accuracy: 91.80\n",
      "Train Epoch: 142 [38400/50000 (85%)]\tLoss: 0.206784, Accuracy: 91.80\n",
      "Train Epoch: 142 [40960/50000 (91%)]\tLoss: 0.196560, Accuracy: 93.55\n",
      "Train Epoch: 142 [43520/50000 (97%)]\tLoss: 0.192994, Accuracy: 93.36\n",
      "\n",
      "Validation set: Average loss: 0.5846, Accuracy: 4195/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[36.25394654273987 s]\n",
      "\n",
      "Test set: Average loss: 0.5954, Accuracy: 8321/10000 (83.21%)\n",
      "\n",
      "Train Epoch: 143 [0/50000 (0%)]\tLoss: 0.196059, Accuracy: 92.77\n",
      "Train Epoch: 143 [2560/50000 (6%)]\tLoss: 0.189657, Accuracy: 94.53\n",
      "Train Epoch: 143 [5120/50000 (11%)]\tLoss: 0.185241, Accuracy: 94.34\n",
      "Train Epoch: 143 [7680/50000 (17%)]\tLoss: 0.157173, Accuracy: 93.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 143 [10240/50000 (23%)]\tLoss: 0.147468, Accuracy: 94.34\n",
      "Train Epoch: 143 [12800/50000 (28%)]\tLoss: 0.246987, Accuracy: 92.19\n",
      "Train Epoch: 143 [15360/50000 (34%)]\tLoss: 0.141411, Accuracy: 94.14\n",
      "Train Epoch: 143 [17920/50000 (40%)]\tLoss: 0.173873, Accuracy: 94.14\n",
      "Train Epoch: 143 [20480/50000 (45%)]\tLoss: 0.214698, Accuracy: 91.99\n",
      "Train Epoch: 143 [23040/50000 (51%)]\tLoss: 0.293690, Accuracy: 91.21\n",
      "Train Epoch: 143 [25600/50000 (57%)]\tLoss: 0.227709, Accuracy: 92.38\n",
      "Train Epoch: 143 [28160/50000 (62%)]\tLoss: 0.156757, Accuracy: 94.92\n",
      "Train Epoch: 143 [30720/50000 (68%)]\tLoss: 0.207817, Accuracy: 94.14\n",
      "Train Epoch: 143 [33280/50000 (74%)]\tLoss: 0.184394, Accuracy: 94.14\n",
      "Train Epoch: 143 [35840/50000 (80%)]\tLoss: 0.196001, Accuracy: 94.34\n",
      "Train Epoch: 143 [38400/50000 (85%)]\tLoss: 0.199730, Accuracy: 93.75\n",
      "Train Epoch: 143 [40960/50000 (91%)]\tLoss: 0.257728, Accuracy: 92.38\n",
      "Train Epoch: 143 [43520/50000 (97%)]\tLoss: 0.220501, Accuracy: 93.16\n",
      "\n",
      "Validation set: Average loss: 0.5729, Accuracy: 4171/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.32147979736328 s]\n",
      "Train Epoch: 144 [0/50000 (0%)]\tLoss: 0.182374, Accuracy: 93.16\n",
      "Train Epoch: 144 [2560/50000 (6%)]\tLoss: 0.172056, Accuracy: 94.73\n",
      "Train Epoch: 144 [5120/50000 (11%)]\tLoss: 0.170171, Accuracy: 95.12\n",
      "Train Epoch: 144 [7680/50000 (17%)]\tLoss: 0.185800, Accuracy: 91.80\n",
      "Train Epoch: 144 [10240/50000 (23%)]\tLoss: 0.145260, Accuracy: 95.12\n",
      "Train Epoch: 144 [12800/50000 (28%)]\tLoss: 0.192066, Accuracy: 92.38\n",
      "Train Epoch: 144 [15360/50000 (34%)]\tLoss: 0.227742, Accuracy: 91.60\n",
      "Train Epoch: 144 [17920/50000 (40%)]\tLoss: 0.171025, Accuracy: 93.36\n",
      "Train Epoch: 144 [20480/50000 (45%)]\tLoss: 0.147498, Accuracy: 95.12\n",
      "Train Epoch: 144 [23040/50000 (51%)]\tLoss: 0.159875, Accuracy: 95.51\n",
      "Train Epoch: 144 [25600/50000 (57%)]\tLoss: 0.192798, Accuracy: 93.75\n",
      "Train Epoch: 144 [28160/50000 (62%)]\tLoss: 0.152816, Accuracy: 95.12\n",
      "Train Epoch: 144 [30720/50000 (68%)]\tLoss: 0.194728, Accuracy: 94.34\n",
      "Train Epoch: 144 [33280/50000 (74%)]\tLoss: 0.204102, Accuracy: 92.77\n",
      "Train Epoch: 144 [35840/50000 (80%)]\tLoss: 0.171399, Accuracy: 93.75\n",
      "Train Epoch: 144 [38400/50000 (85%)]\tLoss: 0.200078, Accuracy: 92.38\n",
      "Train Epoch: 144 [40960/50000 (91%)]\tLoss: 0.230836, Accuracy: 91.21\n",
      "Train Epoch: 144 [43520/50000 (97%)]\tLoss: 0.176273, Accuracy: 93.95\n",
      "\n",
      "Validation set: Average loss: 0.4405, Accuracy: 4302/5000 (86.00%)\n",
      "\n",
      "the time of this epoch:[36.23747539520264 s]\n",
      "\n",
      "Test set: Average loss: 0.4526, Accuracy: 8578/10000 (85.78%)\n",
      "\n",
      "Train Epoch: 145 [0/50000 (0%)]\tLoss: 0.117112, Accuracy: 96.68\n",
      "Train Epoch: 145 [2560/50000 (6%)]\tLoss: 0.157474, Accuracy: 94.34\n",
      "Train Epoch: 145 [5120/50000 (11%)]\tLoss: 0.174862, Accuracy: 94.34\n",
      "Train Epoch: 145 [7680/50000 (17%)]\tLoss: 0.171203, Accuracy: 94.14\n",
      "Train Epoch: 145 [10240/50000 (23%)]\tLoss: 0.161612, Accuracy: 94.14\n",
      "Train Epoch: 145 [12800/50000 (28%)]\tLoss: 0.168133, Accuracy: 95.12\n",
      "Train Epoch: 145 [15360/50000 (34%)]\tLoss: 0.214792, Accuracy: 91.41\n",
      "Train Epoch: 145 [17920/50000 (40%)]\tLoss: 0.212021, Accuracy: 92.19\n",
      "Train Epoch: 145 [20480/50000 (45%)]\tLoss: 0.128138, Accuracy: 95.90\n",
      "Train Epoch: 145 [23040/50000 (51%)]\tLoss: 0.211054, Accuracy: 93.16\n",
      "Train Epoch: 145 [25600/50000 (57%)]\tLoss: 0.193083, Accuracy: 94.34\n",
      "Train Epoch: 145 [28160/50000 (62%)]\tLoss: 0.192420, Accuracy: 93.16\n",
      "Train Epoch: 145 [30720/50000 (68%)]\tLoss: 0.195109, Accuracy: 92.97\n",
      "Train Epoch: 145 [33280/50000 (74%)]\tLoss: 0.175945, Accuracy: 93.95\n",
      "Train Epoch: 145 [35840/50000 (80%)]\tLoss: 0.188721, Accuracy: 93.95\n",
      "Train Epoch: 145 [38400/50000 (85%)]\tLoss: 0.211607, Accuracy: 93.55\n",
      "Train Epoch: 145 [40960/50000 (91%)]\tLoss: 0.201613, Accuracy: 92.97\n",
      "Train Epoch: 145 [43520/50000 (97%)]\tLoss: 0.228410, Accuracy: 91.80\n",
      "\n",
      "Validation set: Average loss: 0.5628, Accuracy: 4143/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.202675580978394 s]\n",
      "Train Epoch: 146 [0/50000 (0%)]\tLoss: 0.199028, Accuracy: 94.14\n",
      "Train Epoch: 146 [2560/50000 (6%)]\tLoss: 0.170717, Accuracy: 93.55\n",
      "Train Epoch: 146 [5120/50000 (11%)]\tLoss: 0.181626, Accuracy: 94.34\n",
      "Train Epoch: 146 [7680/50000 (17%)]\tLoss: 0.166738, Accuracy: 94.34\n",
      "Train Epoch: 146 [10240/50000 (23%)]\tLoss: 0.151839, Accuracy: 94.92\n",
      "Train Epoch: 146 [12800/50000 (28%)]\tLoss: 0.189490, Accuracy: 94.73\n",
      "Train Epoch: 146 [15360/50000 (34%)]\tLoss: 0.160335, Accuracy: 95.12\n",
      "Train Epoch: 146 [17920/50000 (40%)]\tLoss: 0.182863, Accuracy: 92.38\n",
      "Train Epoch: 146 [20480/50000 (45%)]\tLoss: 0.185581, Accuracy: 93.55\n",
      "Train Epoch: 146 [23040/50000 (51%)]\tLoss: 0.203359, Accuracy: 93.16\n",
      "Train Epoch: 146 [25600/50000 (57%)]\tLoss: 0.176332, Accuracy: 93.95\n",
      "Train Epoch: 146 [28160/50000 (62%)]\tLoss: 0.179078, Accuracy: 94.14\n",
      "Train Epoch: 146 [30720/50000 (68%)]\tLoss: 0.166899, Accuracy: 95.12\n",
      "Train Epoch: 146 [33280/50000 (74%)]\tLoss: 0.161661, Accuracy: 94.53\n",
      "Train Epoch: 146 [35840/50000 (80%)]\tLoss: 0.202034, Accuracy: 93.16\n",
      "Train Epoch: 146 [38400/50000 (85%)]\tLoss: 0.163194, Accuracy: 95.90\n",
      "Train Epoch: 146 [40960/50000 (91%)]\tLoss: 0.239120, Accuracy: 92.19\n",
      "Train Epoch: 146 [43520/50000 (97%)]\tLoss: 0.187775, Accuracy: 92.97\n",
      "\n",
      "Validation set: Average loss: 0.4796, Accuracy: 4265/5000 (85.00%)\n",
      "\n",
      "the time of this epoch:[36.16309714317322 s]\n",
      "\n",
      "Test set: Average loss: 0.4962, Accuracy: 8500/10000 (85.00%)\n",
      "\n",
      "Train Epoch: 147 [0/50000 (0%)]\tLoss: 0.173983, Accuracy: 93.95\n",
      "Train Epoch: 147 [2560/50000 (6%)]\tLoss: 0.222062, Accuracy: 92.58\n",
      "Train Epoch: 147 [5120/50000 (11%)]\tLoss: 0.196494, Accuracy: 92.58\n",
      "Train Epoch: 147 [7680/50000 (17%)]\tLoss: 0.184288, Accuracy: 93.16\n",
      "Train Epoch: 147 [10240/50000 (23%)]\tLoss: 0.134414, Accuracy: 95.31\n",
      "Train Epoch: 147 [12800/50000 (28%)]\tLoss: 0.203655, Accuracy: 93.36\n",
      "Train Epoch: 147 [15360/50000 (34%)]\tLoss: 0.127271, Accuracy: 95.51\n",
      "Train Epoch: 147 [17920/50000 (40%)]\tLoss: 0.225236, Accuracy: 92.97\n",
      "Train Epoch: 147 [20480/50000 (45%)]\tLoss: 0.203267, Accuracy: 93.36\n",
      "Train Epoch: 147 [23040/50000 (51%)]\tLoss: 0.160489, Accuracy: 93.95\n",
      "Train Epoch: 147 [25600/50000 (57%)]\tLoss: 0.230875, Accuracy: 92.19\n",
      "Train Epoch: 147 [28160/50000 (62%)]\tLoss: 0.211926, Accuracy: 92.38\n",
      "Train Epoch: 147 [30720/50000 (68%)]\tLoss: 0.213959, Accuracy: 92.77\n",
      "Train Epoch: 147 [33280/50000 (74%)]\tLoss: 0.162826, Accuracy: 93.55\n",
      "Train Epoch: 147 [35840/50000 (80%)]\tLoss: 0.200911, Accuracy: 93.16\n",
      "Train Epoch: 147 [38400/50000 (85%)]\tLoss: 0.259996, Accuracy: 91.99\n",
      "Train Epoch: 147 [40960/50000 (91%)]\tLoss: 0.222159, Accuracy: 92.38\n",
      "Train Epoch: 147 [43520/50000 (97%)]\tLoss: 0.217296, Accuracy: 93.36\n",
      "\n",
      "Validation set: Average loss: 0.5624, Accuracy: 4157/5000 (83.00%)\n",
      "\n",
      "the time of this epoch:[39.216947078704834 s]\n",
      "Train Epoch: 148 [0/50000 (0%)]\tLoss: 0.167329, Accuracy: 95.70\n",
      "Train Epoch: 148 [2560/50000 (6%)]\tLoss: 0.201162, Accuracy: 92.77\n",
      "Train Epoch: 148 [5120/50000 (11%)]\tLoss: 0.187163, Accuracy: 93.55\n",
      "Train Epoch: 148 [7680/50000 (17%)]\tLoss: 0.136418, Accuracy: 95.90\n",
      "Train Epoch: 148 [10240/50000 (23%)]\tLoss: 0.166040, Accuracy: 94.53\n",
      "Train Epoch: 148 [12800/50000 (28%)]\tLoss: 0.148514, Accuracy: 95.12\n",
      "Train Epoch: 148 [15360/50000 (34%)]\tLoss: 0.183348, Accuracy: 92.77\n",
      "Train Epoch: 148 [17920/50000 (40%)]\tLoss: 0.122417, Accuracy: 96.29\n",
      "Train Epoch: 148 [20480/50000 (45%)]\tLoss: 0.192784, Accuracy: 93.16\n",
      "Train Epoch: 148 [23040/50000 (51%)]\tLoss: 0.213152, Accuracy: 92.97\n",
      "Train Epoch: 148 [25600/50000 (57%)]\tLoss: 0.188137, Accuracy: 93.16\n",
      "Train Epoch: 148 [28160/50000 (62%)]\tLoss: 0.156756, Accuracy: 94.34\n",
      "Train Epoch: 148 [30720/50000 (68%)]\tLoss: 0.206565, Accuracy: 93.55\n",
      "Train Epoch: 148 [33280/50000 (74%)]\tLoss: 0.235949, Accuracy: 92.38\n",
      "Train Epoch: 148 [35840/50000 (80%)]\tLoss: 0.206534, Accuracy: 93.55\n",
      "Train Epoch: 148 [38400/50000 (85%)]\tLoss: 0.188203, Accuracy: 93.75\n",
      "Train Epoch: 148 [40960/50000 (91%)]\tLoss: 0.231579, Accuracy: 91.99\n",
      "Train Epoch: 148 [43520/50000 (97%)]\tLoss: 0.196834, Accuracy: 91.99\n",
      "\n",
      "Validation set: Average loss: 0.6778, Accuracy: 4044/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.133187770843506 s]\n",
      "\n",
      "Test set: Average loss: 0.7311, Accuracy: 7924/10000 (79.24%)\n",
      "\n",
      "Train Epoch: 149 [0/50000 (0%)]\tLoss: 0.192474, Accuracy: 94.34\n",
      "Train Epoch: 149 [2560/50000 (6%)]\tLoss: 0.158202, Accuracy: 94.53\n",
      "Train Epoch: 149 [5120/50000 (11%)]\tLoss: 0.198147, Accuracy: 92.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 149 [7680/50000 (17%)]\tLoss: 0.219312, Accuracy: 92.19\n",
      "Train Epoch: 149 [10240/50000 (23%)]\tLoss: 0.201585, Accuracy: 93.95\n",
      "Train Epoch: 149 [12800/50000 (28%)]\tLoss: 0.125358, Accuracy: 95.51\n",
      "Train Epoch: 149 [15360/50000 (34%)]\tLoss: 0.178025, Accuracy: 94.53\n",
      "Train Epoch: 149 [17920/50000 (40%)]\tLoss: 0.170160, Accuracy: 94.14\n",
      "Train Epoch: 149 [20480/50000 (45%)]\tLoss: 0.202900, Accuracy: 91.80\n",
      "Train Epoch: 149 [23040/50000 (51%)]\tLoss: 0.207208, Accuracy: 92.19\n",
      "Train Epoch: 149 [25600/50000 (57%)]\tLoss: 0.182015, Accuracy: 93.95\n",
      "Train Epoch: 149 [28160/50000 (62%)]\tLoss: 0.229282, Accuracy: 91.99\n",
      "Train Epoch: 149 [30720/50000 (68%)]\tLoss: 0.209583, Accuracy: 91.99\n",
      "Train Epoch: 149 [33280/50000 (74%)]\tLoss: 0.225877, Accuracy: 91.60\n",
      "Train Epoch: 149 [35840/50000 (80%)]\tLoss: 0.221914, Accuracy: 93.16\n",
      "Train Epoch: 149 [38400/50000 (85%)]\tLoss: 0.138311, Accuracy: 95.70\n",
      "Train Epoch: 149 [40960/50000 (91%)]\tLoss: 0.214349, Accuracy: 92.38\n",
      "Train Epoch: 149 [43520/50000 (97%)]\tLoss: 0.157275, Accuracy: 94.34\n",
      "\n",
      "Validation set: Average loss: 0.5566, Accuracy: 4149/5000 (82.00%)\n",
      "\n",
      "the time of this epoch:[39.208112716674805 s]\n",
      "Train Epoch: 150 [0/50000 (0%)]\tLoss: 0.173462, Accuracy: 94.34\n",
      "Train Epoch: 150 [2560/50000 (6%)]\tLoss: 0.137242, Accuracy: 95.90\n",
      "Train Epoch: 150 [5120/50000 (11%)]\tLoss: 0.148089, Accuracy: 95.12\n",
      "Train Epoch: 150 [7680/50000 (17%)]\tLoss: 0.183068, Accuracy: 92.38\n",
      "Train Epoch: 150 [10240/50000 (23%)]\tLoss: 0.151185, Accuracy: 94.73\n",
      "Train Epoch: 150 [12800/50000 (28%)]\tLoss: 0.196642, Accuracy: 92.97\n",
      "Train Epoch: 150 [15360/50000 (34%)]\tLoss: 0.197012, Accuracy: 94.14\n",
      "Train Epoch: 150 [17920/50000 (40%)]\tLoss: 0.195632, Accuracy: 92.77\n",
      "Train Epoch: 150 [20480/50000 (45%)]\tLoss: 0.148596, Accuracy: 95.70\n",
      "Train Epoch: 150 [23040/50000 (51%)]\tLoss: 0.266172, Accuracy: 91.80\n",
      "Train Epoch: 150 [25600/50000 (57%)]\tLoss: 0.191670, Accuracy: 92.97\n",
      "Train Epoch: 150 [28160/50000 (62%)]\tLoss: 0.208191, Accuracy: 92.19\n",
      "Train Epoch: 150 [30720/50000 (68%)]\tLoss: 0.209793, Accuracy: 93.16\n",
      "Train Epoch: 150 [33280/50000 (74%)]\tLoss: 0.232672, Accuracy: 92.58\n",
      "Train Epoch: 150 [35840/50000 (80%)]\tLoss: 0.230136, Accuracy: 92.97\n",
      "Train Epoch: 150 [38400/50000 (85%)]\tLoss: 0.203958, Accuracy: 93.16\n",
      "Train Epoch: 150 [40960/50000 (91%)]\tLoss: 0.223897, Accuracy: 91.80\n",
      "Train Epoch: 150 [43520/50000 (97%)]\tLoss: 0.241092, Accuracy: 91.21\n",
      "\n",
      "Validation set: Average loss: 0.6854, Accuracy: 4040/5000 (80.00%)\n",
      "\n",
      "the time of this epoch:[36.090378761291504 s]\n",
      "\n",
      "Test set: Average loss: 0.6814, Accuracy: 8062/10000 (80.62%)\n",
      "\n",
      "Train Epoch: 151 [0/50000 (0%)]\tLoss: 0.183252, Accuracy: 93.75\n",
      "Train Epoch: 151 [2560/50000 (6%)]\tLoss: 0.171849, Accuracy: 94.14\n",
      "Train Epoch: 151 [5120/50000 (11%)]\tLoss: 0.151063, Accuracy: 95.12\n",
      "Train Epoch: 151 [7680/50000 (17%)]\tLoss: 0.162302, Accuracy: 94.14\n",
      "Train Epoch: 151 [10240/50000 (23%)]\tLoss: 0.165602, Accuracy: 94.92\n",
      "Train Epoch: 151 [12800/50000 (28%)]\tLoss: 0.115338, Accuracy: 97.27\n",
      "Train Epoch: 151 [15360/50000 (34%)]\tLoss: 0.116149, Accuracy: 96.29\n",
      "Train Epoch: 151 [17920/50000 (40%)]\tLoss: 0.125307, Accuracy: 95.31\n",
      "Train Epoch: 151 [20480/50000 (45%)]\tLoss: 0.121052, Accuracy: 96.09\n",
      "Train Epoch: 151 [23040/50000 (51%)]\tLoss: 0.092183, Accuracy: 97.07\n",
      "Train Epoch: 151 [25600/50000 (57%)]\tLoss: 0.102837, Accuracy: 97.27\n",
      "Train Epoch: 151 [28160/50000 (62%)]\tLoss: 0.089020, Accuracy: 97.27\n",
      "Train Epoch: 151 [30720/50000 (68%)]\tLoss: 0.111824, Accuracy: 96.68\n",
      "Train Epoch: 151 [33280/50000 (74%)]\tLoss: 0.105827, Accuracy: 96.88\n",
      "Train Epoch: 151 [35840/50000 (80%)]\tLoss: 0.080452, Accuracy: 97.27\n",
      "Train Epoch: 151 [38400/50000 (85%)]\tLoss: 0.078836, Accuracy: 97.07\n",
      "Train Epoch: 151 [40960/50000 (91%)]\tLoss: 0.094621, Accuracy: 96.68\n",
      "Train Epoch: 151 [43520/50000 (97%)]\tLoss: 0.093881, Accuracy: 96.68\n",
      "\n",
      "Validation set: Average loss: 0.2549, Accuracy: 4597/5000 (91.00%)\n",
      "\n",
      "the time of this epoch:[39.184579372406006 s]\n",
      "Train Epoch: 152 [0/50000 (0%)]\tLoss: 0.068287, Accuracy: 98.05\n",
      "Train Epoch: 152 [2560/50000 (6%)]\tLoss: 0.090687, Accuracy: 96.88\n",
      "Train Epoch: 152 [5120/50000 (11%)]\tLoss: 0.064426, Accuracy: 97.85\n",
      "Train Epoch: 152 [7680/50000 (17%)]\tLoss: 0.070572, Accuracy: 98.05\n",
      "Train Epoch: 152 [10240/50000 (23%)]\tLoss: 0.042240, Accuracy: 98.83\n",
      "Train Epoch: 152 [12800/50000 (28%)]\tLoss: 0.079596, Accuracy: 97.66\n",
      "Train Epoch: 152 [15360/50000 (34%)]\tLoss: 0.091794, Accuracy: 96.68\n",
      "Train Epoch: 152 [17920/50000 (40%)]\tLoss: 0.070799, Accuracy: 97.27\n",
      "Train Epoch: 152 [20480/50000 (45%)]\tLoss: 0.086067, Accuracy: 96.68\n",
      "Train Epoch: 152 [23040/50000 (51%)]\tLoss: 0.080456, Accuracy: 97.85\n",
      "Train Epoch: 152 [25600/50000 (57%)]\tLoss: 0.061216, Accuracy: 98.44\n",
      "Train Epoch: 152 [28160/50000 (62%)]\tLoss: 0.057442, Accuracy: 98.05\n",
      "Train Epoch: 152 [30720/50000 (68%)]\tLoss: 0.076763, Accuracy: 97.66\n",
      "Train Epoch: 152 [33280/50000 (74%)]\tLoss: 0.073488, Accuracy: 97.85\n",
      "Train Epoch: 152 [35840/50000 (80%)]\tLoss: 0.068508, Accuracy: 97.66\n",
      "Train Epoch: 152 [38400/50000 (85%)]\tLoss: 0.068014, Accuracy: 97.85\n",
      "Train Epoch: 152 [40960/50000 (91%)]\tLoss: 0.044963, Accuracy: 99.02\n",
      "Train Epoch: 152 [43520/50000 (97%)]\tLoss: 0.077860, Accuracy: 97.85\n",
      "\n",
      "Validation set: Average loss: 0.2525, Accuracy: 4614/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[36.23654007911682 s]\n",
      "\n",
      "Test set: Average loss: 0.2797, Accuracy: 9160/10000 (91.60%)\n",
      "\n",
      "Train Epoch: 153 [0/50000 (0%)]\tLoss: 0.045271, Accuracy: 98.63\n",
      "Train Epoch: 153 [2560/50000 (6%)]\tLoss: 0.060715, Accuracy: 98.44\n",
      "Train Epoch: 153 [5120/50000 (11%)]\tLoss: 0.060277, Accuracy: 98.24\n",
      "Train Epoch: 153 [7680/50000 (17%)]\tLoss: 0.058018, Accuracy: 98.24\n",
      "Train Epoch: 153 [10240/50000 (23%)]\tLoss: 0.053165, Accuracy: 98.83\n",
      "Train Epoch: 153 [12800/50000 (28%)]\tLoss: 0.056116, Accuracy: 98.44\n",
      "Train Epoch: 153 [15360/50000 (34%)]\tLoss: 0.066830, Accuracy: 98.05\n",
      "Train Epoch: 153 [17920/50000 (40%)]\tLoss: 0.058309, Accuracy: 98.24\n",
      "Train Epoch: 153 [20480/50000 (45%)]\tLoss: 0.060281, Accuracy: 98.05\n",
      "Train Epoch: 153 [23040/50000 (51%)]\tLoss: 0.053523, Accuracy: 98.83\n",
      "Train Epoch: 153 [25600/50000 (57%)]\tLoss: 0.067276, Accuracy: 98.24\n",
      "Train Epoch: 153 [28160/50000 (62%)]\tLoss: 0.039927, Accuracy: 99.02\n",
      "Train Epoch: 153 [30720/50000 (68%)]\tLoss: 0.079408, Accuracy: 97.66\n",
      "Train Epoch: 153 [33280/50000 (74%)]\tLoss: 0.056572, Accuracy: 98.44\n",
      "Train Epoch: 153 [35840/50000 (80%)]\tLoss: 0.053612, Accuracy: 98.24\n",
      "Train Epoch: 153 [38400/50000 (85%)]\tLoss: 0.070687, Accuracy: 97.85\n",
      "Train Epoch: 153 [40960/50000 (91%)]\tLoss: 0.065772, Accuracy: 98.24\n",
      "Train Epoch: 153 [43520/50000 (97%)]\tLoss: 0.057045, Accuracy: 97.66\n",
      "\n",
      "Validation set: Average loss: 0.2591, Accuracy: 4610/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[39.319931507110596 s]\n",
      "Train Epoch: 154 [0/50000 (0%)]\tLoss: 0.050076, Accuracy: 98.24\n",
      "Train Epoch: 154 [2560/50000 (6%)]\tLoss: 0.041383, Accuracy: 98.63\n",
      "Train Epoch: 154 [5120/50000 (11%)]\tLoss: 0.054518, Accuracy: 98.24\n",
      "Train Epoch: 154 [7680/50000 (17%)]\tLoss: 0.031634, Accuracy: 99.41\n",
      "Train Epoch: 154 [10240/50000 (23%)]\tLoss: 0.058733, Accuracy: 98.24\n",
      "Train Epoch: 154 [12800/50000 (28%)]\tLoss: 0.048447, Accuracy: 98.24\n",
      "Train Epoch: 154 [15360/50000 (34%)]\tLoss: 0.047982, Accuracy: 98.83\n",
      "Train Epoch: 154 [17920/50000 (40%)]\tLoss: 0.039979, Accuracy: 99.41\n",
      "Train Epoch: 154 [20480/50000 (45%)]\tLoss: 0.038624, Accuracy: 99.02\n",
      "Train Epoch: 154 [23040/50000 (51%)]\tLoss: 0.030633, Accuracy: 99.41\n",
      "Train Epoch: 154 [25600/50000 (57%)]\tLoss: 0.057409, Accuracy: 98.05\n",
      "Train Epoch: 154 [28160/50000 (62%)]\tLoss: 0.034266, Accuracy: 98.63\n",
      "Train Epoch: 154 [30720/50000 (68%)]\tLoss: 0.029269, Accuracy: 99.22\n",
      "Train Epoch: 154 [33280/50000 (74%)]\tLoss: 0.067140, Accuracy: 97.85\n",
      "Train Epoch: 154 [35840/50000 (80%)]\tLoss: 0.051954, Accuracy: 98.63\n",
      "Train Epoch: 154 [38400/50000 (85%)]\tLoss: 0.027332, Accuracy: 99.22\n",
      "Train Epoch: 154 [40960/50000 (91%)]\tLoss: 0.049190, Accuracy: 98.44\n",
      "Train Epoch: 154 [43520/50000 (97%)]\tLoss: 0.041481, Accuracy: 98.44\n",
      "\n",
      "Validation set: Average loss: 0.2635, Accuracy: 4607/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[36.21461725234985 s]\n",
      "\n",
      "Test set: Average loss: 0.2862, Accuracy: 9187/10000 (91.87%)\n",
      "\n",
      "Train Epoch: 155 [0/50000 (0%)]\tLoss: 0.045141, Accuracy: 98.63\n",
      "Train Epoch: 155 [2560/50000 (6%)]\tLoss: 0.033610, Accuracy: 99.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 155 [5120/50000 (11%)]\tLoss: 0.041380, Accuracy: 98.44\n",
      "Train Epoch: 155 [7680/50000 (17%)]\tLoss: 0.054637, Accuracy: 97.27\n",
      "Train Epoch: 155 [10240/50000 (23%)]\tLoss: 0.060318, Accuracy: 97.66\n",
      "Train Epoch: 155 [12800/50000 (28%)]\tLoss: 0.048296, Accuracy: 98.44\n",
      "Train Epoch: 155 [15360/50000 (34%)]\tLoss: 0.046927, Accuracy: 98.44\n",
      "Train Epoch: 155 [17920/50000 (40%)]\tLoss: 0.049802, Accuracy: 98.63\n",
      "Train Epoch: 155 [20480/50000 (45%)]\tLoss: 0.061289, Accuracy: 97.85\n",
      "Train Epoch: 155 [23040/50000 (51%)]\tLoss: 0.041847, Accuracy: 99.02\n",
      "Train Epoch: 155 [25600/50000 (57%)]\tLoss: 0.048116, Accuracy: 98.24\n",
      "Train Epoch: 155 [28160/50000 (62%)]\tLoss: 0.055633, Accuracy: 98.63\n",
      "Train Epoch: 155 [30720/50000 (68%)]\tLoss: 0.036135, Accuracy: 99.02\n",
      "Train Epoch: 155 [33280/50000 (74%)]\tLoss: 0.031302, Accuracy: 99.22\n",
      "Train Epoch: 155 [35840/50000 (80%)]\tLoss: 0.026508, Accuracy: 99.61\n",
      "Train Epoch: 155 [38400/50000 (85%)]\tLoss: 0.021060, Accuracy: 99.61\n",
      "Train Epoch: 155 [40960/50000 (91%)]\tLoss: 0.033461, Accuracy: 98.83\n",
      "Train Epoch: 155 [43520/50000 (97%)]\tLoss: 0.039683, Accuracy: 98.63\n",
      "\n",
      "Validation set: Average loss: 0.2654, Accuracy: 4621/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[39.31929874420166 s]\n",
      "Train Epoch: 156 [0/50000 (0%)]\tLoss: 0.053033, Accuracy: 98.24\n",
      "Train Epoch: 156 [2560/50000 (6%)]\tLoss: 0.026500, Accuracy: 99.02\n",
      "Train Epoch: 156 [5120/50000 (11%)]\tLoss: 0.047111, Accuracy: 98.63\n",
      "Train Epoch: 156 [7680/50000 (17%)]\tLoss: 0.024615, Accuracy: 99.61\n",
      "Train Epoch: 156 [10240/50000 (23%)]\tLoss: 0.030089, Accuracy: 99.22\n",
      "Train Epoch: 156 [12800/50000 (28%)]\tLoss: 0.053536, Accuracy: 98.24\n",
      "Train Epoch: 156 [15360/50000 (34%)]\tLoss: 0.030266, Accuracy: 99.22\n",
      "Train Epoch: 156 [17920/50000 (40%)]\tLoss: 0.039159, Accuracy: 98.44\n",
      "Train Epoch: 156 [20480/50000 (45%)]\tLoss: 0.026339, Accuracy: 99.22\n",
      "Train Epoch: 156 [23040/50000 (51%)]\tLoss: 0.054427, Accuracy: 98.63\n",
      "Train Epoch: 156 [25600/50000 (57%)]\tLoss: 0.046935, Accuracy: 98.05\n",
      "Train Epoch: 156 [28160/50000 (62%)]\tLoss: 0.027010, Accuracy: 99.22\n",
      "Train Epoch: 156 [30720/50000 (68%)]\tLoss: 0.023567, Accuracy: 99.41\n",
      "Train Epoch: 156 [33280/50000 (74%)]\tLoss: 0.037641, Accuracy: 98.83\n",
      "Train Epoch: 156 [35840/50000 (80%)]\tLoss: 0.061374, Accuracy: 97.27\n",
      "Train Epoch: 156 [38400/50000 (85%)]\tLoss: 0.045830, Accuracy: 98.24\n",
      "Train Epoch: 156 [40960/50000 (91%)]\tLoss: 0.019248, Accuracy: 99.61\n",
      "Train Epoch: 156 [43520/50000 (97%)]\tLoss: 0.046668, Accuracy: 98.83\n",
      "\n",
      "Validation set: Average loss: 0.2721, Accuracy: 4610/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[36.17984056472778 s]\n",
      "\n",
      "Test set: Average loss: 0.2944, Accuracy: 9175/10000 (91.75%)\n",
      "\n",
      "Train Epoch: 157 [0/50000 (0%)]\tLoss: 0.040807, Accuracy: 98.83\n",
      "Train Epoch: 157 [2560/50000 (6%)]\tLoss: 0.017257, Accuracy: 99.80\n",
      "Train Epoch: 157 [5120/50000 (11%)]\tLoss: 0.023670, Accuracy: 99.41\n",
      "Train Epoch: 157 [7680/50000 (17%)]\tLoss: 0.033975, Accuracy: 98.83\n",
      "Train Epoch: 157 [10240/50000 (23%)]\tLoss: 0.017441, Accuracy: 99.61\n",
      "Train Epoch: 157 [12800/50000 (28%)]\tLoss: 0.021226, Accuracy: 99.41\n",
      "Train Epoch: 157 [15360/50000 (34%)]\tLoss: 0.029918, Accuracy: 99.02\n",
      "Train Epoch: 157 [17920/50000 (40%)]\tLoss: 0.034065, Accuracy: 98.44\n",
      "Train Epoch: 157 [20480/50000 (45%)]\tLoss: 0.021112, Accuracy: 99.22\n",
      "Train Epoch: 157 [23040/50000 (51%)]\tLoss: 0.035135, Accuracy: 98.83\n",
      "Train Epoch: 157 [25600/50000 (57%)]\tLoss: 0.022933, Accuracy: 99.41\n",
      "Train Epoch: 157 [28160/50000 (62%)]\tLoss: 0.024327, Accuracy: 99.22\n",
      "Train Epoch: 157 [30720/50000 (68%)]\tLoss: 0.050035, Accuracy: 99.02\n",
      "Train Epoch: 157 [33280/50000 (74%)]\tLoss: 0.026042, Accuracy: 98.83\n",
      "Train Epoch: 157 [35840/50000 (80%)]\tLoss: 0.028946, Accuracy: 99.22\n",
      "Train Epoch: 157 [38400/50000 (85%)]\tLoss: 0.055267, Accuracy: 98.63\n",
      "Train Epoch: 157 [40960/50000 (91%)]\tLoss: 0.047961, Accuracy: 98.24\n",
      "Train Epoch: 157 [43520/50000 (97%)]\tLoss: 0.059227, Accuracy: 98.24\n",
      "\n",
      "Validation set: Average loss: 0.2844, Accuracy: 4612/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[39.25170350074768 s]\n",
      "Train Epoch: 158 [0/50000 (0%)]\tLoss: 0.027069, Accuracy: 99.02\n",
      "Train Epoch: 158 [2560/50000 (6%)]\tLoss: 0.024516, Accuracy: 99.41\n",
      "Train Epoch: 158 [5120/50000 (11%)]\tLoss: 0.018666, Accuracy: 99.41\n",
      "Train Epoch: 158 [7680/50000 (17%)]\tLoss: 0.035867, Accuracy: 99.02\n",
      "Train Epoch: 158 [10240/50000 (23%)]\tLoss: 0.021046, Accuracy: 99.41\n",
      "Train Epoch: 158 [12800/50000 (28%)]\tLoss: 0.036132, Accuracy: 98.83\n",
      "Train Epoch: 158 [15360/50000 (34%)]\tLoss: 0.018453, Accuracy: 99.61\n",
      "Train Epoch: 158 [17920/50000 (40%)]\tLoss: 0.034208, Accuracy: 98.44\n",
      "Train Epoch: 158 [20480/50000 (45%)]\tLoss: 0.028350, Accuracy: 98.83\n",
      "Train Epoch: 158 [23040/50000 (51%)]\tLoss: 0.025323, Accuracy: 99.22\n",
      "Train Epoch: 158 [25600/50000 (57%)]\tLoss: 0.035972, Accuracy: 98.24\n",
      "Train Epoch: 158 [28160/50000 (62%)]\tLoss: 0.028477, Accuracy: 99.22\n",
      "Train Epoch: 158 [30720/50000 (68%)]\tLoss: 0.027629, Accuracy: 99.61\n",
      "Train Epoch: 158 [33280/50000 (74%)]\tLoss: 0.016601, Accuracy: 99.41\n",
      "Train Epoch: 158 [35840/50000 (80%)]\tLoss: 0.029198, Accuracy: 99.22\n",
      "Train Epoch: 158 [38400/50000 (85%)]\tLoss: 0.023748, Accuracy: 99.61\n",
      "Train Epoch: 158 [40960/50000 (91%)]\tLoss: 0.056952, Accuracy: 97.85\n",
      "Train Epoch: 158 [43520/50000 (97%)]\tLoss: 0.022334, Accuracy: 99.22\n",
      "\n",
      "Validation set: Average loss: 0.2830, Accuracy: 4619/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[36.23114347457886 s]\n",
      "\n",
      "Test set: Average loss: 0.3084, Accuracy: 9174/10000 (91.74%)\n",
      "\n",
      "Train Epoch: 159 [0/50000 (0%)]\tLoss: 0.009081, Accuracy: 100.00\n",
      "Train Epoch: 159 [2560/50000 (6%)]\tLoss: 0.028600, Accuracy: 99.22\n",
      "Train Epoch: 159 [5120/50000 (11%)]\tLoss: 0.021966, Accuracy: 99.22\n",
      "Train Epoch: 159 [7680/50000 (17%)]\tLoss: 0.037258, Accuracy: 99.02\n",
      "Train Epoch: 159 [10240/50000 (23%)]\tLoss: 0.016697, Accuracy: 99.80\n",
      "Train Epoch: 159 [12800/50000 (28%)]\tLoss: 0.044303, Accuracy: 98.24\n",
      "Train Epoch: 159 [15360/50000 (34%)]\tLoss: 0.018944, Accuracy: 99.22\n",
      "Train Epoch: 159 [17920/50000 (40%)]\tLoss: 0.019291, Accuracy: 99.61\n",
      "Train Epoch: 159 [20480/50000 (45%)]\tLoss: 0.024131, Accuracy: 99.61\n",
      "Train Epoch: 159 [23040/50000 (51%)]\tLoss: 0.023260, Accuracy: 99.22\n",
      "Train Epoch: 159 [25600/50000 (57%)]\tLoss: 0.038045, Accuracy: 98.83\n",
      "Train Epoch: 159 [28160/50000 (62%)]\tLoss: 0.034642, Accuracy: 99.22\n",
      "Train Epoch: 159 [30720/50000 (68%)]\tLoss: 0.017862, Accuracy: 99.80\n",
      "Train Epoch: 159 [33280/50000 (74%)]\tLoss: 0.031873, Accuracy: 99.22\n",
      "Train Epoch: 159 [35840/50000 (80%)]\tLoss: 0.021888, Accuracy: 99.22\n",
      "Train Epoch: 159 [38400/50000 (85%)]\tLoss: 0.025608, Accuracy: 99.02\n",
      "Train Epoch: 159 [40960/50000 (91%)]\tLoss: 0.012344, Accuracy: 100.00\n",
      "Train Epoch: 159 [43520/50000 (97%)]\tLoss: 0.025362, Accuracy: 99.41\n",
      "\n",
      "Validation set: Average loss: 0.2949, Accuracy: 4607/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[39.258442640304565 s]\n",
      "Train Epoch: 160 [0/50000 (0%)]\tLoss: 0.014803, Accuracy: 99.80\n",
      "Train Epoch: 160 [2560/50000 (6%)]\tLoss: 0.023030, Accuracy: 99.02\n",
      "Train Epoch: 160 [5120/50000 (11%)]\tLoss: 0.024498, Accuracy: 99.41\n",
      "Train Epoch: 160 [7680/50000 (17%)]\tLoss: 0.031710, Accuracy: 99.41\n",
      "Train Epoch: 160 [10240/50000 (23%)]\tLoss: 0.026320, Accuracy: 99.61\n",
      "Train Epoch: 160 [12800/50000 (28%)]\tLoss: 0.030689, Accuracy: 99.22\n",
      "Train Epoch: 160 [15360/50000 (34%)]\tLoss: 0.017301, Accuracy: 99.80\n",
      "Train Epoch: 160 [17920/50000 (40%)]\tLoss: 0.015552, Accuracy: 99.61\n",
      "Train Epoch: 160 [20480/50000 (45%)]\tLoss: 0.039998, Accuracy: 98.83\n",
      "Train Epoch: 160 [23040/50000 (51%)]\tLoss: 0.020623, Accuracy: 99.22\n",
      "Train Epoch: 160 [25600/50000 (57%)]\tLoss: 0.021312, Accuracy: 99.41\n",
      "Train Epoch: 160 [28160/50000 (62%)]\tLoss: 0.015372, Accuracy: 99.80\n",
      "Train Epoch: 160 [30720/50000 (68%)]\tLoss: 0.036254, Accuracy: 98.24\n",
      "Train Epoch: 160 [33280/50000 (74%)]\tLoss: 0.034825, Accuracy: 99.22\n",
      "Train Epoch: 160 [35840/50000 (80%)]\tLoss: 0.028779, Accuracy: 99.41\n",
      "Train Epoch: 160 [38400/50000 (85%)]\tLoss: 0.024135, Accuracy: 98.83\n",
      "Train Epoch: 160 [40960/50000 (91%)]\tLoss: 0.017157, Accuracy: 99.61\n",
      "Train Epoch: 160 [43520/50000 (97%)]\tLoss: 0.038877, Accuracy: 98.44\n",
      "\n",
      "Validation set: Average loss: 0.2854, Accuracy: 4619/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[36.19459366798401 s]\n",
      "\n",
      "Test set: Average loss: 0.3183, Accuracy: 9171/10000 (91.71%)\n",
      "\n",
      "Train Epoch: 161 [0/50000 (0%)]\tLoss: 0.019586, Accuracy: 99.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 161 [2560/50000 (6%)]\tLoss: 0.015201, Accuracy: 99.41\n",
      "Train Epoch: 161 [5120/50000 (11%)]\tLoss: 0.022125, Accuracy: 98.83\n",
      "Train Epoch: 161 [7680/50000 (17%)]\tLoss: 0.016137, Accuracy: 99.41\n",
      "Train Epoch: 161 [10240/50000 (23%)]\tLoss: 0.013779, Accuracy: 99.61\n",
      "Train Epoch: 161 [12800/50000 (28%)]\tLoss: 0.015713, Accuracy: 99.41\n",
      "Train Epoch: 161 [15360/50000 (34%)]\tLoss: 0.013873, Accuracy: 99.61\n",
      "Train Epoch: 161 [17920/50000 (40%)]\tLoss: 0.014321, Accuracy: 99.80\n",
      "Train Epoch: 161 [20480/50000 (45%)]\tLoss: 0.016813, Accuracy: 99.41\n",
      "Train Epoch: 161 [23040/50000 (51%)]\tLoss: 0.012940, Accuracy: 99.61\n",
      "Train Epoch: 161 [25600/50000 (57%)]\tLoss: 0.025214, Accuracy: 99.41\n",
      "Train Epoch: 161 [28160/50000 (62%)]\tLoss: 0.021207, Accuracy: 99.22\n",
      "Train Epoch: 161 [30720/50000 (68%)]\tLoss: 0.030483, Accuracy: 99.22\n",
      "Train Epoch: 161 [33280/50000 (74%)]\tLoss: 0.033126, Accuracy: 98.44\n",
      "Train Epoch: 161 [35840/50000 (80%)]\tLoss: 0.014429, Accuracy: 99.41\n",
      "Train Epoch: 161 [38400/50000 (85%)]\tLoss: 0.011141, Accuracy: 100.00\n",
      "Train Epoch: 161 [40960/50000 (91%)]\tLoss: 0.018299, Accuracy: 99.41\n",
      "Train Epoch: 161 [43520/50000 (97%)]\tLoss: 0.014897, Accuracy: 99.61\n",
      "\n",
      "Validation set: Average loss: 0.2963, Accuracy: 4627/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[39.25565266609192 s]\n",
      "Train Epoch: 162 [0/50000 (0%)]\tLoss: 0.035507, Accuracy: 98.24\n",
      "Train Epoch: 162 [2560/50000 (6%)]\tLoss: 0.025098, Accuracy: 99.02\n",
      "Train Epoch: 162 [5120/50000 (11%)]\tLoss: 0.019767, Accuracy: 99.61\n",
      "Train Epoch: 162 [7680/50000 (17%)]\tLoss: 0.011123, Accuracy: 99.80\n",
      "Train Epoch: 162 [10240/50000 (23%)]\tLoss: 0.023497, Accuracy: 98.83\n",
      "Train Epoch: 162 [12800/50000 (28%)]\tLoss: 0.021698, Accuracy: 99.02\n",
      "Train Epoch: 162 [15360/50000 (34%)]\tLoss: 0.014218, Accuracy: 99.41\n",
      "Train Epoch: 162 [17920/50000 (40%)]\tLoss: 0.010756, Accuracy: 99.80\n",
      "Train Epoch: 162 [20480/50000 (45%)]\tLoss: 0.020959, Accuracy: 99.41\n",
      "Train Epoch: 162 [23040/50000 (51%)]\tLoss: 0.028140, Accuracy: 99.22\n",
      "Train Epoch: 162 [25600/50000 (57%)]\tLoss: 0.015873, Accuracy: 99.61\n",
      "Train Epoch: 162 [28160/50000 (62%)]\tLoss: 0.015556, Accuracy: 99.61\n",
      "Train Epoch: 162 [30720/50000 (68%)]\tLoss: 0.017877, Accuracy: 99.41\n",
      "Train Epoch: 162 [33280/50000 (74%)]\tLoss: 0.036006, Accuracy: 99.02\n",
      "Train Epoch: 162 [35840/50000 (80%)]\tLoss: 0.021113, Accuracy: 99.41\n",
      "Train Epoch: 162 [38400/50000 (85%)]\tLoss: 0.026422, Accuracy: 99.02\n",
      "Train Epoch: 162 [40960/50000 (91%)]\tLoss: 0.027185, Accuracy: 99.22\n",
      "Train Epoch: 162 [43520/50000 (97%)]\tLoss: 0.024384, Accuracy: 99.22\n",
      "\n",
      "Validation set: Average loss: 0.3047, Accuracy: 4617/5000 (92.00%)\n",
      "\n",
      "the time of this epoch:[36.204427003860474 s]\n",
      "\n",
      "Test set: Average loss: 0.3269, Accuracy: 9165/10000 (91.65%)\n",
      "\n",
      "Train Epoch: 163 [0/50000 (0%)]\tLoss: 0.020362, Accuracy: 99.61\n",
      "Train Epoch: 163 [2560/50000 (6%)]\tLoss: 0.012520, Accuracy: 99.61\n",
      "Train Epoch: 163 [5120/50000 (11%)]\tLoss: 0.015923, Accuracy: 99.61\n",
      "Train Epoch: 163 [7680/50000 (17%)]\tLoss: 0.010495, Accuracy: 100.00\n",
      "Train Epoch: 163 [10240/50000 (23%)]\tLoss: 0.014409, Accuracy: 99.61\n"
     ]
    }
   ],
   "source": [
    "# Fantastic logger for tensorboard and pytorch, \n",
    "# run tensorboard by opening a new terminal and run \"tensorboard --logdir runs\"\n",
    "# open tensorboard at http://localhost:6006/\n",
    "from tensorboardX import SummaryWriter\n",
    "best_loss = None\n",
    "best_acc = None\n",
    "\n",
    "import time \n",
    "SINCE=time.time()\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train(epoch)\n",
    "    \n",
    "    loss, accuracy = validate(epoch)\n",
    "    best_loss, best_acc = save_best(loss, accuracy, best_loss, best_acc)\n",
    "    \n",
    "    NOW=time.time() \n",
    "    DURINGS=NOW-SINCE\n",
    "    SINCE=NOW\n",
    "    print(\"the time of this epoch:[{} s]\".format(DURINGS))\n",
    "    \n",
    "    if epoch>=10 and (epoch-10)%2==0:\n",
    "        test(epoch)\n",
    "    \n",
    "# writer = SummaryWriter() \n",
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "\n",
    "# writer.close()\n",
    "\n",
    "#---------------------------- Test ------------------------------\n",
    "test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6902, Accuracy: 8877/10000 (88.77%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一次 scale 位于[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](http://op4a94iq8.bkt.clouddn.com/18-7-14/70206949.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x9BnCF17x72RAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhLQQijshhLQQijshhLQQijshhLSQ/w9isHt8TVxEXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查看训练过程的信息\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "def parse(in_file,flag):\n",
    "    num=-1\n",
    "    ys=list()\n",
    "    xs=list()\n",
    "    losses=list()\n",
    "    with open(in_file,\"r\") as reader:\n",
    "        for aLine in reader:\n",
    "            #print(aLine)\n",
    "\n",
    "            res=[e for e in aLine.strip('\\n').split(\" \")]\n",
    "            if res[0]==\"Train\" and flag==\"Train\":\n",
    "                num=num+1\n",
    "                ys.append(float(res[-1]))\n",
    "                xs.append(int(num))\n",
    "                losses.append(float(res[-3].split(',')[0]))\n",
    "            if res[0]==\"Validation\" and flag==\"Validation\":\n",
    "                num=num+1\n",
    "                xs.append(int(num))\n",
    "                tmp=[float(e) for e in res[-2].split('/')]\n",
    "                ys.append(100*float(tmp[0]/tmp[1]))\n",
    "                losses.append(float(res[-4].split(',')[0]))\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(xs,ys,'ro')\n",
    "\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(xs, losses, 'ro')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    in_file=\"D://INFO.txt\"\n",
    "    # 显示训练阶段的正确率和Loss信息\n",
    "    parse(in_file,\"Train\") # \"Validation\"\n",
    "    # 显示验证阶段的正确率和Loss信息\n",
    "    #parse(in_file,\"Validation\") # \"Validation\"\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naRBY0BmGYRqE/w/U/zxNDJVCvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngFDcCSGkAaG4E0JIA0JxJ4SQBuT/AHC9SchdaANwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDuAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wPtS+GMghk59AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查看训练过程的信息\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "def parse(in_file,flag):\n",
    "    num=-1\n",
    "    ys=list()\n",
    "    xs=list()\n",
    "    losses=list()\n",
    "    with open(in_file,\"r\") as reader:\n",
    "        for aLine in reader:\n",
    "            #print(aLine)\n",
    "\n",
    "            res=[e for e in aLine.strip('\\n').split(\" \")]\n",
    "            if res[0]==\"Train\" and flag==\"Train\":\n",
    "                num=num+1\n",
    "                ys.append(float(res[-1]))\n",
    "                xs.append(int(num))\n",
    "                losses.append(float(res[-3].split(',')[0]))\n",
    "            if res[0]==\"Validation\" and flag==\"Validation\":\n",
    "                num=num+1\n",
    "                xs.append(int(num))\n",
    "                tmp=[float(e) for e in res[-2].split('/')]\n",
    "                ys.append(100*float(tmp[0]/tmp[1]))\n",
    "                losses.append(float(res[-4].split(',')[0]))\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(xs,ys,'r-')\n",
    "\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(xs, losses, 'r-')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    in_file=\"D://INFO.txt\"\n",
    "    # 显示训练阶段的正确率和Loss信息\n",
    "    parse(in_file,\"Train\") # \"Validation\"\n",
    "    # 显示验证阶段的正确率和Loss信息\n",
    "    parse(in_file,\"Validation\") # \"Validation\"\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
